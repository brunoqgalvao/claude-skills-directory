{
  "id": "llama-cpp",
  "name": "llama-cpp",
  "version": "1.0.0",
  "summary": "Runs LLM inference on CPU, Apple Silicon, and consumer GPUs without NVIDIA hardware. Use for edge deployment, M1/M2/M3 Macs, AMD/Intel GPUs, or when CUDA is unavailable. Support...",
  "verticals": [
    "engineering",
    "support"
  ],
  "tags": [],
  "author": {
    "name": "zechenzhangAGI",
    "github": "zechenzhangAGI"
  },
  "status": "ready",
  "verified": false,
  "visibility": "public",
  "license": "MIT",
  "links": {
    "repo": "https://github.com/zechenzhangAGI/AI-research-SKILLs",
    "skill_md": "https://raw.githubusercontent.com/zechenzhangAGI/AI-research-SKILLs/main/12-inference-serving/llama-cpp/SKILL.md"
  },
  "stats": {
    "stars": 0,
    "forks": 0,
    "installs": 0
  },
  "last_updated": "2025-11-20T22:51:38.000Z",
  "created_at": "2025-12-22T18:02:53.172Z",
  "meta": {
    "source": "skillsdirectory.org",
    "sourceUrl": "https://www.skillsdirectory.org/skill/zechenzhangAGI%2FAI-research-SKILLs/12-inference-serving%2Fllama-cpp%2FSKILL.md",
    "harvestedAt": "2026-01-07T16:59:54.461Z",
    "uniqueId": "sk_13eb326d9ebd"
  }
}