{
  "id": "llm-inference-batching-scheduler",
  "name": "llm-inference-batching-scheduler",
  "version": "1.0.0",
  "summary": "Guidance for optimizing LLM inference request batching and scheduling problems. This skill applies when designing batch schedulers that minimize cost while meeting latency and p...",
  "verticals": [
    "design"
  ],
  "tags": [],
  "author": {
    "name": "letta-ai",
    "github": "letta-ai"
  },
  "status": "ready",
  "verified": false,
  "visibility": "public",
  "license": "MIT",
  "links": {
    "repo": "https://github.com/letta-ai/skills",
    "skill_md": "https://raw.githubusercontent.com/letta-ai/skills/main/ai/benchmarks/letta/terminal-bench-2/trajectory-feedback/llm-inference-batching-scheduler/SKILL.md"
  },
  "stats": {
    "stars": 0,
    "forks": 0,
    "installs": 0
  },
  "last_updated": "2026-01-07T16:59:54.795Z",
  "created_at": "2025-12-22T18:17:12.072Z",
  "meta": {
    "source": "skillsdirectory.org",
    "sourceUrl": "https://www.skillsdirectory.org/skill/letta-ai%2Fskills/ai%2Fbenchmarks%2Fletta%2Fterminal-bench-2%2Ftrajectory-feedback%2Fllm-inference-batching-scheduler%2FSKILL.md",
    "harvestedAt": "2026-01-07T16:59:54.795Z",
    "uniqueId": "sk_4662c4bebd91"
  }
}