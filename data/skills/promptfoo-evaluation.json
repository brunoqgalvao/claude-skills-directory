{
  "id": "promptfoo-evaluation",
  "name": "promptfoo-evaluation",
  "version": "1.0.0",
  "summary": "Configures and runs LLM evaluation using Promptfoo framework. Use when setting up prompt testing, creating evaluation configs (promptfooconfig.yaml), writing Python custom asser...",
  "description": "# Promptfoo Evaluation\n\n## Overview\n\nThis skill provides guidance for configuring and running LLM evaluations using [Promptfoo](https://www.promptfoo.dev/), an open-source CLI tool for testing and comparing LLM outputs.\n\n## Quick Start\n\n```bash\n# Initialize a new evaluation project\nnpx promptfoo@latest init\n\n# Run evaluation\nnpx promptfoo@latest eval\n\n# View results in browser\nnpx promptfoo@latest view\n```\n\n## Configuration Structure\n\nA typical Promptfoo project structure:\n\n```\nproject/\n├── promptfooconfig.yaml    # Main configuration\n├── prompts/\n│   ├── system.md           # System prompt\n│   └── chat.json           # Chat format prompt\n├── tests/\n│   └── cases.yaml          # Test cases\n└── scripts/\n    └── metrics.py          # Custom Python assertions\n```\n\n## Core Configuration (promptfooconfig.yaml)\n\n```yaml\n# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json\ndescription: \"My LLM Evaluation\"\n\n# Prompts to test\nprompts:\n  - file://prompts/system.md\n  - file://prompts/chat.json\n\n# Models to compare\nproviders:\n  - id: anthropic:messages:claude-sonnet-4-5-20250929\n    label: Claude-4.5-Sonnet\n  - id: openai:gpt-4.1\n    label: GPT-4.1\n\n# Test cases\ntests: file://tests/cases.yaml\n\n# Default assertions for all tests\ndefaultTest:\n  assert:\n    - type: python\n      value: file://scripts/metrics.py:custom_assert\n    - type: llm-rubric\n      value: |\n        Evaluate the response quality on a 0-1 scale.\n      threshold: 0.7\n\n# Output path\noutputPath: results/eval-results.json\n```\n\n## Prompt Formats\n\n### Text Prompt (system.md)\n\n```markdown\nYou are a helpful assistant.\n\nTask: {{task}}\nContext: {{context}}\n```\n\n### Chat Format (chat.json)\n\n```json\n[\n  {\"role\": \"system\", \"content\": \"{{system_prompt}}\"},\n  {\"role\": \"user\", \"content\": \"{{user_input}}\"}\n]\n```\n\n### Few-Shot Pattern\n\nEmbed examples directly in prompt or use chat format with assistant messages:\n\n```json\n[\n  {\"role\": \"system\", \"content\": \"{{system_prompt}}\"},\n  {\"role\": \"user\", \"content\": \"Example input: {{example_input}}\"},\n  {\"role\": \"assistant\", \"content\": \"{{example_output}}\"},\n  {\"role\": \"user\", \"content\": \"Now process: {{actual_input}}\"}\n]\n```\n\n## Test Cases (tests/cases.yaml)\n\n```yaml\n- description: \"Test case 1\"\n  vars:\n    system_prompt: file://prompts/system.md\n    user_input: \"Hello world\"\n    # Load content from files\n    context: file://data/context.txt\n  assert:\n    - type: contains\n      value: \"expected text\"\n    - type: python\n      value: file://scripts/metrics.py:custom_check\n      threshold: 0.8\n```\n\n## Python Custom Assertions\n\nCreate a Python file for custom assertions (e.g., `scripts/metrics.py`):\n\n```python\ndef get_assert(output: str, context: dict) -> dict:\n    \"\"\"Default assertion function.\"\"\"\n    vars_dict = context.get('vars', {})\n\n    # Access test variables\n    expected = vars_dict.get('expected', '')\n\n    # Return result\n    return {\n        \"pass\": expected in output,\n        \"score\": 0.8,\n        \"reason\": \"Contains expected content\",\n        \"named_scores\": {\"relevance\": 0.9}\n    }\n\ndef custom_check(output: str, context: dict) -> dict:\n    \"\"\"Custom named assertion.\"\"\"\n    word_count = len(output.split())\n    passed = 100 <= word_count <= 500\n\n    return {\n        \"pass\": passed,\n        \"score\": min(1.0, word_count / 300),\n        \"reason\": f\"Word count: {word_count}\"\n    }\n```\n\n**Key points:**\n- Default function name is `get_assert`\n- Specify function with `file://path.py:function_name`\n- Return `bool`, `float` (score), or `dict` with pass/score/reason\n- Access variables via `context['vars']`\n\n## LLM-as-Judge (llm-rubric)\n\n```yaml\nassert:\n  - type: llm-rubric\n    value: |\n      Evaluate the response based on:\n      1. Accuracy of information\n      2. Clarity of explanation\n      3. Completeness\n\n      Score 0.0-1.0 where 0.7+ is passing.\n    threshold: 0.7\n    provider: openai:gpt-4.1  # Optional: override grader model\n```\n\n**Best practices:**\n- Provide clear scoring criteria\n- Use `threshold` to set minimum passing score\n- Default grader uses available API keys (OpenAI → Anthropic → Google)\n\n## Common Assertion Types\n\n| Type | Usage | Example |\n|------|-------|---------|\n| `contains` | Check substring | `value: \"hello\"` |\n| `icontains` | Case-insensitive | `value: \"HELLO\"` |\n| `equals` | Exact match | `value: \"42\"` |\n| `regex` | Pattern match | `value: \"\\\\d{4}\"` |\n| `python` | Custom logic | `value: file://script.py` |\n| `llm-rubric` | LLM grading | `value: \"Is professional\"` |\n| `latency` | Response time | `threshold: 1000` |\n\n## File References\n\nAll paths are relative to config file location:\n\n```yaml\n# Load file content as variable\nvars:\n  content: file://data/input.txt\n\n# Load prompt from file\nprompts:\n  - file://prompts/main.md\n\n# Load test cases from file\ntests: file://tests/cases.yaml\n\n# Load Python assertion\nassert:\n  - type: python\n    value: file://scripts/check.py:validate\n```\n\n## Running Evaluations\n\n```bash\n# Basic run\nnpx promptfoo@latest eval\n\n# With specific config\nnpx promptfoo@latest eval --config path/to/config.yaml\n\n# Output to file\nnpx promptfoo@latest eval --output results.json\n\n# Filter tests\nnpx promptfoo@latest eval --filter-metadata category=math\n\n# View results\nnpx promptfoo@latest view\n```\n\n## Troubleshooting\n\n**Python not found:**\n```bash\nexport PROMPTFOO_PYTHON=python3\n```\n\n**Large outputs truncated:**\nOutputs over 30000 characters are truncated. Use `head_limit` in assertions.\n\n**File not found errors:**\nEnsure paths are relative to `promptfooconfig.yaml` location.\n\n## Echo Provider (Preview Mode)\n\nUse the **echo provider** to preview rendered prompts without making API calls:\n\n```yaml\n# promptfooconfig-preview.yaml\nproviders:\n  - echo  # Returns prompt as output, no API calls\n\ntests:\n  - vars:\n      input: \"test content\"\n```\n\n**Use cases:**\n- Preview prompt rendering before expensive API calls\n- Verify Few-shot examples are loaded correctly\n- Debug variable substitution issues\n- Validate prompt structure\n\n```bash\n# Run preview mode\nnpx promptfoo@latest eval --config promptfooconfig-preview.yaml\n```\n\n**Cost:** Free - no API tokens consumed.\n\n## Advanced Few-Shot Implementation\n\n### Multi-turn Conversation Pattern\n\nFor complex few-shot learning with full examples:\n\n```json\n[\n  {\"role\": \"system\", \"content\": \"{{system_prompt}}\"},\n\n  // Few-shot Example 1\n  {\"role\": \"user\", \"content\": \"Task: {{example_input_1}}\"},\n  {\"role\": \"assistant\", \"content\": \"{{example_output_1}}\"},\n\n  // Few-shot Example 2 (optional)\n  {\"role\": \"user\", \"content\": \"Task: {{example_input_2}}\"},\n  {\"role\": \"assistant\", \"content\": \"{{example_output_2}}\"},\n\n  // Actual test\n  {\"role\": \"user\", \"content\": \"Task: {{actual_input}}\"}\n]\n```\n\n**Test case configuration:**\n\n```yaml\ntests:\n  - vars:\n      system_prompt: file://prompts/system.md\n      # Few-shot examples\n      example_input_1: file://data/examples/input1.txt\n      example_output_1: file://data/examples/output1.txt\n      example_input_2: file://data/examples/input2.txt\n      example_output_2: file://data/examples/output2.txt\n      # Actual test\n      actual_input: file://data/test1.txt\n```\n\n**Best practices:**\n- Use 1-3 few-shot examples (more may dilute effectiveness)\n- Ensure examples match the task format exactly\n- Load examples from files for better maintainability\n- Use echo provider first to verify structure\n\n## Long Text Handling\n\nFor Chinese/long-form content evaluations (10k+ characters):\n\n**Configuration:**\n\n```yaml\nproviders:\n  - id: anthropic:messages:claude-sonnet-4-5-20250929\n    config:\n      max_tokens: 8192  # Increase for long outputs\n\ndefaultTest:\n  assert:\n    - type: python\n      value: file://scripts/metrics.py:check_length\n```\n\n**Python assertion for text metrics:**\n\n```python\nimport re\n\ndef strip_tags(text: str) -> str:\n    \"\"\"Remove HTML tags for pure text.\"\"\"\n    return re.sub(r'<[^>]+>', '', text)\n\ndef check_length(output: str, context: dict) -> dict:\n    \"\"\"Check output length constraints.\"\"\"\n    raw_input = context['vars'].get('raw_input', '')\n\n    input_len = len(strip_tags(raw_input))\n    output_len = len(strip_tags(output))\n\n    reduction_ratio = 1 - (output_len / input_len) if input_len > 0 else 0\n\n    return {\n        \"pass\": 0.7 <= reduction_ratio <= 0.9,\n        \"score\": reduction_ratio,\n        \"reason\": f\"Reduction: {reduction_ratio:.1%} (target: 70-90%)\",\n        \"named_scores\": {\n            \"input_length\": input_len,\n            \"output_length\": output_len,\n            \"reduction_ratio\": reduction_ratio\n        }\n    }\n```\n\n## Real-World Example\n\n**Project:** Chinese short-video content curation from long transcripts\n\n**Structure:**\n```\ntiaogaoren/\n├── promptfooconfig.yaml          # Production config\n├── promptfooconfig-preview.yaml  # Preview config (echo provider)\n├── prompts/\n│   ├── tiaogaoren-prompt.json   # Chat format with few-shot\n│   └── v4/system-v4.md          # System prompt\n├── tests/cases.yaml              # 3 test samples\n├── scripts/metrics.py            # Custom metrics (reduction ratio, etc.)\n├── data/                         # 5 samples (2 few-shot, 3 eval)\n└── results/\n```\n\n**See:** `/Users/tiansheng/Workspace/prompts/tiaogaoren/` for full implementation.\n\n## Resources\n\nFor detailed API reference and advanced patterns, see [references/promptfoo_api.md](references/promptfoo_api.md).",
  "verticals": [
    "engineering"
  ],
  "tags": [],
  "author": {
    "name": "daymade",
    "github": "daymade"
  },
  "status": "ready",
  "verified": false,
  "visibility": "public",
  "license": "MIT",
  "links": {
    "repo": "https://github.com/daymade/claude-code-skills",
    "skill_md": "https://raw.githubusercontent.com/daymade/claude-code-skills/main/promptfoo-evaluation/SKILL.md"
  },
  "stats": {
    "stars": 154,
    "forks": 13,
    "installs": 0
  },
  "last_updated": "2026-01-07T15:30:48Z",
  "created_at": "2025-10-22T11:17:31Z",
  "meta": {
    "source": "github",
    "sourceUrl": "https://github.com/daymade/claude-code-skills/blob/main/promptfoo-evaluation/SKILL.md",
    "harvestedAt": "2026-01-07T16:59:54.890Z",
    "uniqueId": "sk_9ebe19bda978"
  }
}