{
  "id": "qa-expert",
  "name": "qa-expert",
  "version": "1.0.0",
  "summary": "This skill should be used when establishing comprehensive QA testing processes for any software project. Use when creating test strategies, writing test cases following Google T...",
  "description": "# QA Expert\n\nEstablish world-class QA testing processes for any software project using proven methodologies from Google Testing Standards and OWASP security best practices.\n\n## When to Use This Skill\n\nTrigger this skill when:\n- Setting up QA infrastructure for a new or existing project\n- Writing standardized test cases (AAA pattern compliance)\n- Executing comprehensive test plans with progress tracking\n- Implementing security testing (OWASP Top 10)\n- Filing bugs with proper severity classification (P0-P4)\n- Generating QA reports (daily summaries, weekly progress)\n- Calculating quality metrics (pass rate, coverage, gates)\n- Preparing QA documentation for third-party team handoffs\n- Enabling autonomous LLM-driven test execution\n\n## Quick Start\n\n**One-command initialization**:\n```bash\npython scripts/init_qa_project.py <project-name> [output-directory]\n```\n\n**What gets created**:\n- Directory structure (`tests/docs/`, `tests/e2e/`, `tests/fixtures/`)\n- Tracking CSVs (`TEST-EXECUTION-TRACKING.csv`, `BUG-TRACKING-TEMPLATE.csv`)\n- Documentation templates (`BASELINE-METRICS.md`, `WEEKLY-PROGRESS-REPORT.md`)\n- Master QA Prompt for autonomous execution\n- README with complete quickstart guide\n\n**For autonomous execution** (recommended): See `references/master_qa_prompt.md` - single copy-paste command for 100x speedup.\n\n## Core Capabilities\n\n### 1. QA Project Initialization\n\nInitialize complete QA infrastructure with all templates:\n\n```bash\npython scripts/init_qa_project.py <project-name> [output-directory]\n```\n\nCreates directory structure, tracking CSVs, documentation templates, and master prompt for autonomous execution.\n\n**Use when**: Starting QA from scratch or migrating to structured QA process.\n\n### 2. Test Case Writing\n\nWrite standardized, reproducible test cases following AAA pattern (Arrange-Act-Assert):\n\n1. Read template: `assets/templates/TEST-CASE-TEMPLATE.md`\n2. Follow structure: Prerequisites (Arrange) → Test Steps (Act) → Expected Results (Assert)\n3. Assign priority: P0 (blocker) → P4 (low)\n4. Include edge cases and potential bugs\n\n**Test case format**: TC-[CATEGORY]-[NUMBER] (e.g., TC-CLI-001, TC-WEB-042, TC-SEC-007)\n\n**Reference**: See `references/google_testing_standards.md` for complete AAA pattern guidelines and coverage thresholds.\n\n### 3. Test Execution & Tracking\n\n**Ground Truth Principle** (critical):\n- **Test case documents** (e.g., `02-CLI-TEST-CASES.md`) = **authoritative source** for test steps\n- **Tracking CSV** = execution status only (do NOT trust CSV for test specifications)\n- See `references/ground_truth_principle.md` for preventing doc/CSV sync issues\n\n**Manual execution**:\n1. Read test case from category document (e.g., `02-CLI-TEST-CASES.md`) ← **always start here**\n2. Execute test steps exactly as documented\n3. Update `TEST-EXECUTION-TRACKING.csv` **immediately** after EACH test (never batch)\n4. File bug in `BUG-TRACKING-TEMPLATE.csv` if test fails\n\n**Autonomous execution** (recommended):\n1. Copy master prompt from `references/master_qa_prompt.md`\n2. Paste to LLM session\n3. LLM auto-executes, auto-tracks, auto-files bugs, auto-generates reports\n\n**Innovation**: 100x faster vs manual + zero human error in tracking + auto-resume capability.\n\n### 4. Bug Reporting\n\nFile bugs with proper severity classification:\n\n**Required fields**:\n- Bug ID: Sequential (BUG-001, BUG-002, ...)\n- Severity: P0 (24h fix) → P4 (optional)\n- Steps to Reproduce: Numbered, specific\n- Environment: OS, versions, configuration\n\n**Severity classification**:\n- **P0 (Blocker)**: Security vulnerability, core functionality broken, data loss\n- **P1 (Critical)**: Major feature broken with workaround\n- **P2 (High)**: Minor feature issue, edge case\n- **P3 (Medium)**: Cosmetic issue\n- **P4 (Low)**: Documentation typo\n\n**Reference**: See `BUG-TRACKING-TEMPLATE.csv` for complete template with examples.\n\n### 5. Quality Metrics Calculation\n\nCalculate comprehensive QA metrics and quality gates status:\n\n```bash\npython scripts/calculate_metrics.py <path/to/TEST-EXECUTION-TRACKING.csv>\n```\n\n**Metrics dashboard includes**:\n- Test execution progress (X/Y tests, Z% complete)\n- Pass rate (passed/executed %)\n- Bug analysis (unique bugs, P0/P1/P2 breakdown)\n- Quality gates status (✅/❌ for each gate)\n\n**Quality gates** (all must pass for release):\n| Gate | Target | Blocker |\n|------|--------|---------|\n| Test Execution | 100% | Yes |\n| Pass Rate | ≥80% | Yes |\n| P0 Bugs | 0 | Yes |\n| P1 Bugs | ≤5 | Yes |\n| Code Coverage | ≥80% | Yes |\n| Security | 90% OWASP | Yes |\n\n### 6. Progress Reporting\n\nGenerate QA reports for stakeholders:\n\n**Daily summary** (end-of-day):\n- Tests executed, pass rate, bugs filed\n- Blockers (or None)\n- Tomorrow's plan\n\n**Weekly report** (every Friday):\n- Use template: `WEEKLY-PROGRESS-REPORT.md` (created by init script)\n- Compare against baseline: `BASELINE-METRICS.md`\n- Assess quality gates and trends\n\n**Reference**: See `references/llm_prompts_library.md` for 30+ ready-to-use reporting prompts.\n\n### 7. Security Testing (OWASP)\n\nImplement OWASP Top 10 security testing:\n\n**Coverage targets**:\n1. **A01: Broken Access Control** - RLS bypass, privilege escalation\n2. **A02: Cryptographic Failures** - Token encryption, password hashing\n3. **A03: Injection** - SQL injection, XSS, command injection\n4. **A04: Insecure Design** - Rate limiting, anomaly detection\n5. **A05: Security Misconfiguration** - Verbose errors, default credentials\n6. **A07: Authentication Failures** - Session hijacking, CSRF\n7. **Others**: Data integrity, logging, SSRF\n\n**Target**: 90% OWASP coverage (9/10 threats mitigated).\n\nEach security test follows AAA pattern with specific attack vectors documented.\n\n## Day 1 Onboarding\n\nFor new QA engineers joining a project, complete 5-hour onboarding guide:\n\n**Read**: `references/day1_onboarding.md`\n\n**Timeline**:\n- Hour 1: Environment setup (database, dev server, dependencies)\n- Hour 2: Documentation review (test strategy, quality gates)\n- Hour 3: Test data setup (users, CLI, DevTools)\n- Hour 4: Execute first test case\n- Hour 5: Team onboarding & Week 1 planning\n\n**Checkpoint**: By end of Day 1, environment running, first test executed, ready for Week 1.\n\n## Autonomous Execution (⭐ Recommended)\n\nEnable LLM-driven autonomous QA testing with single master prompt:\n\n**Read**: `references/master_qa_prompt.md`\n\n**Features**:\n- Auto-resume from last completed test (reads tracking CSV)\n- Auto-execute test cases (Week 1-5 progression)\n- Auto-track results (updates CSV after each test)\n- Auto-file bugs (creates bug reports for failures)\n- Auto-generate reports (daily summaries, weekly reports)\n- Auto-escalate P0 bugs (stops testing, notifies stakeholders)\n\n**Benefits**:\n- 100x faster execution vs manual\n- Zero human error in tracking\n- Consistent bug documentation\n- Immediate progress visibility\n\n**Usage**: Copy master prompt, paste to LLM, let it run autonomously for 5 weeks.\n\n## Adapting for Your Project\n\n### Small Project (50 tests)\n- Timeline: 2 weeks\n- Categories: 2-3 (e.g., Frontend, Backend)\n- Daily: 5-7 tests\n- Reports: Daily summary only\n\n### Medium Project (200 tests)\n- Timeline: 4 weeks\n- Categories: 4-5 (CLI, Web, API, DB, Security)\n- Daily: 10-12 tests\n- Reports: Daily + weekly\n\n### Large Project (500+ tests)\n- Timeline: 8-10 weeks\n- Categories: 6-8 (multiple components)\n- Daily: 10-15 tests\n- Reports: Daily + weekly + bi-weekly stakeholder\n\n## Reference Documents\n\nAccess detailed guidelines from bundled references:\n\n- **`references/day1_onboarding.md`** - 5-hour onboarding guide for new QA engineers\n- **`references/master_qa_prompt.md`** - Single command for autonomous LLM execution (100x speedup)\n- **`references/llm_prompts_library.md`** - 30+ ready-to-use prompts for specific QA tasks\n- **`references/google_testing_standards.md`** - AAA pattern, coverage thresholds, fail-fast validation\n- **`references/ground_truth_principle.md`** - Preventing doc/CSV sync issues (critical for test suite integrity)\n\n## Assets & Templates\n\nTest case templates and bug report formats:\n\n- **`assets/templates/TEST-CASE-TEMPLATE.md`** - Complete template with CLI and security examples\n\n## Scripts\n\nAutomation scripts for QA infrastructure:\n\n- **`scripts/init_qa_project.py`** - Initialize QA infrastructure (one command setup)\n- **`scripts/calculate_metrics.py`** - Generate quality metrics dashboard\n\n## Common Patterns\n\n### Pattern 1: Starting Fresh QA\n```\n1. python scripts/init_qa_project.py my-app ./\n2. Fill in BASELINE-METRICS.md (document current state)\n3. Write test cases using assets/templates/TEST-CASE-TEMPLATE.md\n4. Copy master prompt from references/master_qa_prompt.md\n5. Paste to LLM → autonomous execution begins\n```\n\n### Pattern 2: LLM-Driven Testing (Autonomous)\n```\n1. Read references/master_qa_prompt.md\n2. Copy the single master prompt (one paragraph)\n3. Paste to LLM conversation\n4. LLM executes all 342 test cases over 5 weeks\n5. LLM updates tracking CSVs automatically\n6. LLM generates weekly reports automatically\n```\n\n### Pattern 3: Adding Security Testing\n```\n1. Read references/google_testing_standards.md (OWASP section)\n2. Write TC-SEC-XXX test cases for each OWASP threat\n3. Target 90% coverage (9/10 threats)\n4. Document mitigations in test cases\n```\n\n### Pattern 4: Third-Party QA Handoff\n```\n1. Ensure all templates populated\n2. Verify BASELINE-METRICS.md complete\n3. Package tests/docs/ folder\n4. Include references/master_qa_prompt.md for autonomous execution\n5. QA team can start immediately (Day 1 onboarding → 5 weeks testing)\n```\n\n## Success Criteria\n\nThis skill is effective when:\n- ✅ Test cases are reproducible by any engineer\n- ✅ Quality gates objectively measured\n- ✅ Bugs fully documented with repro steps\n- ✅ Progress visible in real-time (CSV tracking)\n- ✅ Autonomous execution enabled (LLM can execute full plan)\n- ✅ Third-party QA teams can start testing immediately",
  "verticals": [
    "engineering",
    "legal",
    "operations"
  ],
  "tags": [],
  "author": {
    "name": "daymade",
    "github": "daymade"
  },
  "status": "ready",
  "verified": false,
  "visibility": "public",
  "license": "MIT",
  "links": {
    "repo": "https://github.com/daymade/claude-code-skills",
    "skill_md": "https://raw.githubusercontent.com/daymade/claude-code-skills/main/qa-expert/SKILL.md"
  },
  "stats": {
    "stars": 154,
    "forks": 13,
    "installs": 0
  },
  "last_updated": "2026-01-07T15:30:48Z",
  "created_at": "2025-10-22T11:17:31Z",
  "meta": {
    "source": "github",
    "sourceUrl": "https://github.com/daymade/claude-code-skills/blob/main/qa-expert/SKILL.md",
    "harvestedAt": "2026-01-07T16:59:54.890Z",
    "uniqueId": "sk_1f1c94c60001"
  }
}