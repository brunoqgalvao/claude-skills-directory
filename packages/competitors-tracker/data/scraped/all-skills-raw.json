[
  {
    "id": "algorithmic-art",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/algorithmic-art",
      "url": "https://github.com/anthropics/skills/tree/main/skills/algorithmic-art"
    },
    "frontmatter": {
      "name": "algorithmic-art",
      "description": "Creating algorithmic art using p5.js with seeded randomness and interactive parameter exploration. Use this when users request creating art using code, generative art, algorithmic art, flow fields, or particle systems. Create original algorithmic art rather than copying existing artists' work to avoid copyright violations.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "Algorithmic philosophies are computational aesthetic movements that are then expressed through code. Output .md files (philosophy), .html files (interactive viewer), and .js files (generative algorithms).\n\nThis happens in two steps:\n1. Algorithmic Philosophy Creation (.md file)\n2. Express by creating p5.js generative art (.html + .js files)\n\nFirst, undertake this task:\n\n## ALGORITHMIC PHILOSOPHY CREATION\n\nTo begin, create an ALGORITHMIC PHILOSOPHY (not static images or templates) that will be interpreted through:\n- Computational processes, emergent behavior, mathematical beauty\n- Seeded randomness, noise fields, organic systems\n- Particles, flows, fields, forces\n- Parametric variation and controlled chaos\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user to take into account, but use as a foundation; it should not constrain creative freedom.\n- What is created: An algorithmic philosophy/generative aesthetic movement.\n- What happens next: The same version receives the philosophy and EXPRESSES IT IN CODE - creating p5.js sketches that are 90% algorithmic generation, 10% essential parameters.\n\nConsider this approach:\n- Write a manifesto for a generative art movement\n- The next phase involves writing the algorithm that brings it to life\n\nThe philosophy must emphasize: Algorithmic expression. Emergent behavior. Computational beauty. Seeded variation.\n\n### HOW TO GENERATE AN ALGORITHMIC PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Organic Turbulence\" / \"Quantum Harmonics\" / \"Emergent Stillness\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the ALGORITHMIC essence, express how this philosophy manifests through:\n- Computational processes and mathematical relationships?\n- Noise functions and randomness patterns?\n- Particle behaviors and field dynamics?\n- Temporal evolution and system states?\n- Parametric variation and emergent complexity?\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each algorithmic aspect should be mentioned once. Avoid repeating concepts about noise theory, particle dynamics, or mathematical principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final algorithm should appear as though it took countless hours to develop, was refined with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted algorithm,\" \"the product of deep computational expertise,\" \"painstaking optimization,\" \"master-level implementation.\"\n- **Leave creative space**: Be specific about the algorithmic direction, but concise enough that the next Claude has room to make interpretive implementation choices at an extremely high level of craftsmanship.\n\nThe philosophy must guide the next version to express ideas ALGORITHMICALLY, not through static images. Beauty lives in the process, not the final frame.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Organic Turbulence\"**\nPhilosophy: Chaos constrained by natural law, order emerging from disorder.\nAlgorithmic expression: Flow fields driven by layered Perlin noise. Thousands of particles following vector forces, their trails accumulating into organic density maps. Multiple noise octaves create turbulent regions and calm zones. Color emerges from velocity and density - fast particles burn bright, slow ones fade to shadow. The algorithm runs until equilibrium - a meticulously tuned balance where every parameter was refined through countless iterations by a master of computational aesthetics.\n\n**\"Quantum Harmonics\"**\nPhilosophy: Discrete entities exhibiting wave-like interference patterns.\nAlgorithmic expression: Particles initialized on a grid, each carrying a phase value that evolves through sine waves. When particles are near, their phases interfere - constructive interference creates bright nodes, destructive creates voids. Simple harmonic motion generates complex emergent mandalas. The result of painstaking frequency calibration where every ratio was carefully chosen to produce resonant beauty.\n\n**\"Recursive Whispers\"**\nPhilosophy: Self-similarity across scales, infinite depth in finite space.\nAlgorithmic expression: Branching structures that subdivide recursively. Each branch slightly randomized but constrained by golden ratios. L-systems or recursive subdivision generate tree-like forms that feel both mathematical and organic. Subtle noise perturbations break perfect symmetry. Line weights diminish with each recursion level. Every branching angle the product of deep mathematical exploration.\n\n**\"Field Dynamics\"**\nPhilosophy: Invisible forces made visible through their effects on matter.\nAlgorithmic expression: Vector fields constructed from mathematical functions or noise. Particles born at edges, flowing along field lines, dying when they reach equilibrium or boundaries. Multiple fields can attract, repel, or rotate particles. The visualization shows only the traces - ghost-like evidence of invisible forces. A computational dance meticulously choreographed through force balance.\n\n**\"Stochastic Crystallization\"**\nPhilosophy: Random processes crystallizing into ordered structures.\nAlgorithmic expression: Randomized circle packing or Voronoi tessellation. Start with random points, let them evolve through relaxation algorithms. Cells push apart until equilibrium. Color based on cell size, neighbor count, or distance from center. The organic tiling that emerges feels both random and inevitable. Every seed produces unique crystalline beauty - the mark of a master-level generative algorithm.\n\n*These are condensed examples. The actual algorithmic philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **ALGORITHMIC PHILOSOPHY**: Creating a computational worldview to be expressed through code\n- **PROCESS OVER PRODUCT**: Always emphasize that beauty emerges from the algorithm's execution - each run is unique\n- **PARAMETRIC EXPRESSION**: Ideas communicate through mathematical relationships, forces, behaviors - not static composition\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy algorithmically - provide creative implementation room\n- **PURE GENERATIVE ART**: This is about making LIVING ALGORITHMS, not static images with randomness\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final algorithm must feel meticulously crafted, refined through countless iterations, the product of deep expertise by someone at the absolute top of their field in computational aesthetics\n\n**The algorithmic philosophy should be 4-6 paragraphs long.** Fill it with poetic computational philosophy that brings together the intended vision. Avoid repeating the same points. Output this algorithmic philosophy as a .md file.\n\n---\n\n## DEDUCING THE CONCEPTUAL SEED\n\n**CRITICAL STEP**: Before implementing the algorithm, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe concept is a **subtle, niche reference embedded within the algorithm itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful generative composition. The algorithmic philosophy provides the computational language. The deduced concept provides the soul - the quiet conceptual DNA woven invisibly into parameters, behaviors, and emergence patterns.\n\nThis is **VERY IMPORTANT**: The reference must be so refined that it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song through algorithmic harmony - only those who know will catch it, but everyone appreciates the generative beauty.\n\n---\n\n## P5.JS IMPLEMENTATION\n\nWith the philosophy AND conceptual framework established, express it through code. Pause to gather thoughts before proceeding. Use only the algorithmic philosophy created and the instructions below.\n\n### ⚠️ STEP 0: READ THE TEMPLATE FIRST ⚠️\n\n**CRITICAL: BEFORE writing any HTML:**\n\n1. **Read** `templates/viewer.html` using the Read tool\n2. **Study** the exact structure, styling, and Anthropic branding\n3. **Use that file as the LITERAL STARTING POINT** - not just inspiration\n4. **Keep all FIXED sections exactly as shown** (header, sidebar structure, Anthropic colors/fonts, seed controls, action buttons)\n5. **Replace only the VARIABLE sections** marked in the file's comments (algorithm, parameters, UI controls for parameters)\n\n**Avoid:**\n- ❌ Creating HTML from scratch\n- ❌ Inventing custom styling or color schemes\n- ❌ Using system fonts or dark themes\n- ❌ Changing the sidebar structure\n\n**Follow these practices:**\n- ✅ Copy the template's exact HTML structure\n- ✅ Keep Anthropic branding (Poppins/Lora fonts, light colors, gradient backdrop)\n- ✅ Maintain the sidebar layout (Seed → Parameters → Colors? → Actions)\n- ✅ Replace only the p5.js algorithm and parameter controls\n\nThe template is the foundation. Build on it, don't rebuild it.\n\n---\n\nTo create gallery-quality computational art that lives and breathes, use the algorithmic philosophy as the foundation.\n\n### TECHNICAL REQUIREMENTS\n\n**Seeded Randomness (Art Blocks Pattern)**:\n```javascript\n// ALWAYS use a seed for reproducibility\nlet seed = 12345; // or hash from user input\nrandomSeed(seed);\nnoiseSeed(seed);\n```\n\n**Parameter Structure - FOLLOW THE PHILOSOPHY**:\n\nTo establish parameters that emerge naturally from the algorithmic philosophy, consider: \"What qualities of this system can be adjusted?\"\n\n```javascript\nlet params = {\n  seed: 12345,  // Always include seed for reproducibility\n  // colors\n  // Add parameters that control YOUR algorithm:\n  // - Quantities (how many?)\n  // - Scales (how big? how fast?)\n  // - Probabilities (how likely?)\n  // - Ratios (what proportions?)\n  // - Angles (what direction?)\n  // - Thresholds (when does behavior change?)\n};\n```\n\n**To design effective parameters, focus on the properties the system needs to be tunable rather than thinking in terms of \"pattern types\".**\n\n**Core Algorithm - EXPRESS THE PHILOSOPHY**:\n\n**CRITICAL**: The algorithmic philosophy should dictate what to build.\n\nTo express the philosophy through code, avoid thinking \"which pattern should I use?\" and instead think \"how to express this philosophy through code?\"\n\nIf the philosophy is about **organic emergence**, consider using:\n- Elements that accumulate or grow over time\n- Random processes constrained by natural rules\n- Feedback loops and interactions\n\nIf the philosophy is about **mathematical beauty**, consider using:\n- Geometric relationships and ratios\n- Trigonometric functions and harmonics\n- Precise calculations creating unexpected patterns\n\nIf the philosophy is about **controlled chaos**, consider using:\n- Random variation within strict boundaries\n- Bifurcation and phase transitions\n- Order emerging from disorder\n\n**The algorithm flows from the philosophy, not from a menu of options.**\n\nTo guide the implementation, let the conceptual essence inform creative and original choices. Build something that expresses the vision for this particular request.\n\n**Canvas Setup**: Standard p5.js structure:\n```javascript\nfunction setup() {\n  createCanvas(1200, 1200);\n  // Initialize your system\n}\n\nfunction draw() {\n  // Your generative algorithm\n  // Can be static (noLoop) or animated\n}\n```\n\n### CRAFTSMANSHIP REQUIREMENTS\n\n**CRITICAL**: To achieve mastery, create algorithms that feel like they emerged through countless iterations by a master generative artist. Tune every parameter carefully. Ensure every pattern emerges with purpose. This is NOT random noise - this is CONTROLLED CHAOS refined through deep expertise.\n\n- **Balance**: Complexity without visual noise, order without rigidity\n- **Color Harmony**: Thoughtful palettes, not random RGB values\n- **Composition**: Even in randomness, maintain visual hierarchy and flow\n- **Performance**: Smooth execution, optimized for real-time if animated\n- **Reproducibility**: Same seed ALWAYS produces identical output\n\n### OUTPUT FORMAT\n\nOutput:\n1. **Algorithmic Philosophy** - As markdown or text explaining the generative aesthetic\n2. **Single HTML Artifact** - Self-contained interactive generative art built from `templates/viewer.html` (see STEP 0 and next section)\n\nThe HTML artifact contains everything: p5.js (from CDN), the algorithm, parameter controls, and UI - all in one file that works immediately in claude.ai artifacts or any browser. Start from the template file, not from scratch.\n\n---\n\n## INTERACTIVE ARTIFACT CREATION\n\n**REMINDER: `templates/viewer.html` should have already been read (see STEP 0). Use that file as the starting point.**\n\nTo allow exploration of the generative art, create a single, self-contained HTML artifact. Ensure this artifact works immediately in claude.ai or any browser - no setup required. Embed everything inline.\n\n### CRITICAL: WHAT'S FIXED VS VARIABLE\n\nThe `templates/viewer.html` file is the foundation. It contains the exact structure and styling needed.\n\n**FIXED (always include exactly as shown):**\n- Layout structure (header, sidebar, main canvas area)\n- Anthropic branding (UI colors, fonts, gradients)\n- Seed section in sidebar:\n  - Seed display\n  - Previous/Next buttons\n  - Random button\n  - Jump to seed input + Go button\n- Actions section in sidebar:\n  - Regenerate button\n  - Reset button\n\n**VARIABLE (customize for each artwork):**\n- The entire p5.js algorithm (setup/draw/classes)\n- The parameters object (define what the art needs)\n- The Parameters section in sidebar:\n  - Number of parameter controls\n  - Parameter names\n  - Min/max/step values for sliders\n  - Control types (sliders, inputs, etc.)\n- Colors section (optional):\n  - Some art needs color pickers\n  - Some art might use fixed colors\n  - Some art might be monochrome (no color controls needed)\n  - Decide based on the art's needs\n\n**Every artwork should have unique parameters and algorithm!** The fixed parts provide consistent UX - everything else expresses the unique vision.\n\n### REQUIRED FEATURES\n\n**1. Parameter Controls**\n- Sliders for numeric parameters (particle count, noise scale, speed, etc.)\n- Color pickers for palette colors\n- Real-time updates when parameters change\n- Reset button to restore defaults\n\n**2. Seed Navigation**\n- Display current seed number\n- \"Previous\" and \"Next\" buttons to cycle through seeds\n- \"Random\" button for random seed\n- Input field to jump to specific seed\n- Generate 100 variations when requested (seeds 1-100)\n\n**3. Single Artifact Structure**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <!-- p5.js from CDN - always available -->\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/p5.min.js\"></script>\n  <style>\n    /* All styling inline - clean, minimal */\n    /* Canvas on top, controls below */\n  </style>\n</head>\n<body>\n  <div id=\"canvas-container\"></div>\n  <div id=\"controls\">\n    <!-- All parameter controls -->\n  </div>\n  <script>\n    // ALL p5.js code inline here\n    // Parameter objects, classes, functions\n    // setup() and draw()\n    // UI handlers\n    // Everything self-contained\n  </script>\n</body>\n</html>\n```\n\n**CRITICAL**: This is a single artifact. No external files, no imports (except p5.js CDN). Everything inline.\n\n**4. Implementation Details - BUILD THE SIDEBAR**\n\nThe sidebar structure:\n\n**1. Seed (FIXED)** - Always include exactly as shown:\n- Seed display\n- Prev/Next/Random/Jump buttons\n\n**2. Parameters (VARIABLE)** - Create controls for the art:\n```html\n<div class=\"control-group\">\n    <label>Parameter Name</label>\n    <input type=\"range\" id=\"param\" min=\"...\" max=\"...\" step=\"...\" value=\"...\" oninput=\"updateParam('param', this.value)\">\n    <span class=\"value-display\" id=\"param-value\">...</span>\n</div>\n```\nAdd as many control-group divs as there are parameters.\n\n**3. Colors (OPTIONAL/VARIABLE)** - Include if the art needs adjustable colors:\n- Add color pickers if users should control palette\n- Skip this section if the art uses fixed colors\n- Skip if the art is monochrome\n\n**4. Actions (FIXED)** - Always include exactly as shown:\n- Regenerate button\n- Reset button\n- Download PNG button\n\n**Requirements**:\n- Seed controls must work (prev/next/random/jump/display)\n- All parameters must have UI controls\n- Regenerate, Reset, Download buttons must work\n- Keep Anthropic branding (UI styling, not art colors)\n\n### USING THE ARTIFACT\n\nThe HTML artifact works immediately:\n1. **In claude.ai**: Displayed as an interactive artifact - runs instantly\n2. **As a file**: Save and open in any browser - no server needed\n3. **Sharing**: Send the HTML file - it's completely self-contained\n\n---\n\n## VARIATIONS & EXPLORATION\n\nThe artifact includes seed navigation by default (prev/next/random buttons), allowing users to explore variations without creating multiple files. If the user wants specific variations highlighted:\n\n- Include seed presets (buttons for \"Variation 1: Seed 42\", \"Variation 2: Seed 127\", etc.)\n- Add a \"Gallery Mode\" that shows thumbnails of multiple seeds side-by-side\n- All within the same single artifact\n\nThis is like creating a series of prints from the same plate - the algorithm is consistent, but each seed reveals different facets of its potential. The interactive nature means users discover their own favorites by exploring the seed space.\n\n---\n\n## THE CREATIVE PROCESS\n\n**User request** → **Algorithmic philosophy** → **Implementation**\n\nEach request is unique. The process involves:\n\n1. **Interpret the user's intent** - What aesthetic is being sought?\n2. **Create an algorithmic philosophy** (4-6 paragraphs) describing the computational approach\n3. **Implement it in code** - Build the algorithm that expresses this philosophy\n4. **Design appropriate parameters** - What should be tunable?\n5. **Build matching UI controls** - Sliders/inputs for those parameters\n\n**The constants**:\n- Anthropic branding (colors, fonts, layout)\n- Seed navigation (always present)\n- Self-contained HTML artifact\n\n**Everything else is variable**:\n- The algorithm itself\n- The parameters\n- The UI controls\n- The visual outcome\n\nTo achieve the best results, trust creativity and let the philosophy guide the implementation.\n\n---\n\n## RESOURCES\n\nThis skill includes helpful templates and documentation:\n\n- **templates/viewer.html**: REQUIRED STARTING POINT for all HTML artifacts.\n  - This is the foundation - contains the exact structure and Anthropic branding\n  - **Keep unchanged**: Layout structure, sidebar organization, Anthropic colors/fonts, seed controls, action buttons\n  - **Replace**: The p5.js algorithm, parameter definitions, and UI controls in Parameters section\n  - The extensive comments in the file mark exactly what to keep vs replace\n\n- **templates/generator_template.js**: Reference for p5.js best practices and code structure principles.\n  - Shows how to organize parameters, use seeded randomness, structure classes\n  - NOT a pattern menu - use these principles to build unique algorithms\n  - Embed algorithms inline in the HTML artifact (don't create separate .js files)\n\n**Critical reminder**:\n- The **template is the STARTING POINT**, not inspiration\n- The **algorithm is where to create** something unique\n- Don't copy the flow field example - build what the philosophy demands\n- But DO keep the exact UI structure and Anthropic branding from the template",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:07.880Z"
  },
  {
    "id": "brand-guidelines",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/brand-guidelines",
      "url": "https://github.com/anthropics/skills/tree/main/skills/brand-guidelines"
    },
    "frontmatter": {
      "name": "brand-guidelines",
      "description": "Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Anthropic Brand Styling\n\n## Overview\n\nTo access Anthropic's official brand identity and style resources, use this skill.\n\n**Keywords**: branding, corporate identity, visual identity, post-processing, styling, brand colors, typography, Anthropic brand, visual formatting, visual design\n\n## Brand Guidelines\n\n### Colors\n\n**Main Colors:**\n\n- Dark: `#141413` - Primary text and dark backgrounds\n- Light: `#faf9f5` - Light backgrounds and text on dark\n- Mid Gray: `#b0aea5` - Secondary elements\n- Light Gray: `#e8e6dc` - Subtle backgrounds\n\n**Accent Colors:**\n\n- Orange: `#d97757` - Primary accent\n- Blue: `#6a9bcc` - Secondary accent\n- Green: `#788c5d` - Tertiary accent\n\n### Typography\n\n- **Headings**: Poppins (with Arial fallback)\n- **Body Text**: Lora (with Georgia fallback)\n- **Note**: Fonts should be pre-installed in your environment for best results\n\n## Features\n\n### Smart Font Application\n\n- Applies Poppins font to headings (24pt and larger)\n- Applies Lora font to body text\n- Automatically falls back to Arial/Georgia if custom fonts unavailable\n- Preserves readability across all systems\n\n### Text Styling\n\n- Headings (24pt+): Poppins font\n- Body text: Lora font\n- Smart color selection based on background\n- Preserves text hierarchy and formatting\n\n### Shape and Accent Colors\n\n- Non-text shapes use accent colors\n- Cycles through orange, blue, and green accents\n- Maintains visual interest while staying on-brand\n\n## Technical Details\n\n### Font Management\n\n- Uses system-installed Poppins and Lora fonts when available\n- Provides automatic fallback to Arial (headings) and Georgia (body)\n- No font installation required - works with existing system fonts\n- For best results, pre-install Poppins and Lora fonts in your environment\n\n### Color Application\n\n- Uses RGB color values for precise brand matching\n- Applied via python-pptx's RGBColor class\n- Maintains color fidelity across different systems\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:08.134Z"
  },
  {
    "id": "canvas-design",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/canvas-design",
      "url": "https://github.com/anthropics/skills/tree/main/skills/canvas-design"
    },
    "frontmatter": {
      "name": "canvas-design",
      "description": "Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "These are instructions for creating design philosophies - aesthetic movements that are then EXPRESSED VISUALLY. Output only .md files, .pdf files, and .png files.\n\nComplete this in two steps:\n1. Design Philosophy Creation (.md file)\n2. Express by creating it on a canvas (.pdf file or .png file)\n\nFirst, undertake this task:\n\n## DESIGN PHILOSOPHY CREATION\n\nTo begin, create a VISUAL PHILOSOPHY (not layouts or templates) that will be interpreted through:\n- Form, space, color, composition\n- Images, graphics, shapes, patterns\n- Minimal text as visual accent\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user that should be taken into account, but used as a foundation; it should not constrain creative freedom.\n- What is created: A design philosophy/aesthetic movement.\n- What happens next: Then, the same version receives the philosophy and EXPRESSES IT VISUALLY - creating artifacts that are 90% visual design, 10% essential text.\n\nConsider this approach:\n- Write a manifesto for an art movement\n- The next phase involves making the artwork\n\nThe philosophy must emphasize: Visual expression. Spatial communication. Artistic interpretation. Minimal words.\n\n### HOW TO GENERATE A VISUAL PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Brutalist Joy\" / \"Chromatic Silence\" / \"Metabolist Dreams\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the VISUAL essence, express how the philosophy manifests through:\n- Space and form\n- Color and material\n- Scale and rhythm\n- Composition and balance\n- Visual hierarchy\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each design aspect should be mentioned once. Avoid repeating points about color theory, spatial relationships, or typographic principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final work should appear as though it took countless hours to create, was labored over with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted,\" \"the product of deep expertise,\" \"painstaking attention,\" \"master-level execution.\"\n- **Leave creative space**: Remain specific about the aesthetic direction, but concise enough that the next Claude has room to make interpretive choices also at a extremely high level of craftmanship.\n\nThe philosophy must guide the next version to express ideas VISUALLY, not through text. Information lives in design, not paragraphs.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Concrete Poetry\"**\nPhilosophy: Communication through monumental form and bold geometry.\nVisual expression: Massive color blocks, sculptural typography (huge single words, tiny labels), Brutalist spatial divisions, Polish poster energy meets Le Corbusier. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesture - never paragraphs, only essential words integrated into the visual architecture. Every element placed with the precision of a master craftsman.\n\n**\"Chromatic Language\"**\nPhilosophy: Color as the primary information system.\nVisual expression: Geometric precision where color zones create meaning. Typography minimal - small sans-serif labels letting chromatic fields communicate. Think Josef Albers' interaction meets data visualization. Information encoded spatially and chromatically. Words only to anchor what color already shows. The result of painstaking chromatic calibration.\n\n**\"Analog Meditation\"**\nPhilosophy: Quiet visual contemplation through texture and breathing room.\nVisual expression: Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Typography whispered (small, restrained, serving the visual). Japanese photobook aesthetic. Images breathe across pages. Text appears sparingly - short phrases, never explanatory blocks. Each composition balanced with the care of a meditation practice.\n\n**\"Organic Systems\"**\nPhilosophy: Natural clustering and modular growth patterns.\nVisual expression: Rounded forms, organic arrangements, color from nature through architecture. Information shown through visual diagrams, spatial relationships, iconography. Text only for key labels floating in space. The composition tells the story through expert spatial orchestration.\n\n**\"Geometric Silence\"**\nPhilosophy: Pure order and restraint.\nVisual expression: Grid-based precision, bold photography or stark graphics, dramatic negative space. Typography precise but minimal - small essential text, large quiet zones. Swiss formalism meets Brutalist material honesty. Structure communicates, not words. Every alignment the work of countless refinements.\n\n*These are condensed examples. The actual design philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **VISUAL PHILOSOPHY**: Create an aesthetic worldview to be expressed through design\n- **MINIMAL TEXT**: Always emphasize that text is sparse, essential-only, integrated as visual element - never lengthy\n- **SPATIAL EXPRESSION**: Ideas communicate through space, form, color, composition - not paragraphs\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy visually - provide creative room\n- **PURE DESIGN**: This is about making ART OBJECTS, not documents with decoration\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final work must look meticulously crafted, labored over with care, the product of countless hours by someone at the top of their field\n\n**The design philosophy should be 4-6 paragraphs long.** Fill it with poetic design philosophy that brings together the core vision. Avoid repeating the same points. Keep the design philosophy generic without mentioning the intention of the art, as if it can be used wherever. Output the design philosophy as a .md file.\n\n---\n\n## DEDUCING THE SUBTLE REFERENCE\n\n**CRITICAL STEP**: Before creating the canvas, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe topic is a **subtle, niche reference embedded within the art itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful abstract composition. The design philosophy provides the aesthetic language. The deduced topic provides the soul - the quiet conceptual DNA woven invisibly into form, color, and composition.\n\nThis is **VERY IMPORTANT**: The reference must be refined so it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song - only those who know will catch it, but everyone appreciates the music.\n\n---\n\n## CANVAS CREATION\n\nWith both the philosophy and the conceptual framework established, express it on a canvas. Take a moment to gather thoughts and clear the mind. Use the design philosophy created and the instructions below to craft a masterpiece, embodying all aspects of the philosophy with expert craftsmanship.\n\n**IMPORTANT**: For any type of content, even if the user requests something for a movie/game/book, the approach should still be sophisticated. Never lose sight of the idea that this should be art, not something that's cartoony or amateur.\n\nTo create museum or magazine quality work, use the design philosophy as the foundation. Create one single page, highly visual, design-forward PDF or PNG output (unless asked for more pages). Generally use repeating patterns and perfect shapes. Treat the abstract philosophical design as if it were a scientific bible, borrowing the visual language of systematic observation—dense accumulation of marks, repeated elements, or layered patterns that build meaning through patient repetition and reward sustained viewing. Add sparse, clinical typography and systematic reference markers that suggest this could be a diagram from an imaginary discipline, treating the invisible subject with the same reverence typically reserved for documenting observable phenomena. Anchor the piece with simple phrase(s) or details positioned subtly, using a limited color palette that feels intentional and cohesive. Embrace the paradox of using analytical visual language to express ideas about human experience: the result should feel like an artifact that proves something ephemeral can be studied, mapped, and understood through careful attention. This is true art. \n\n**Text as a contextual element**: Text is always minimal and visual-first, but let context guide whether that means whisper-quiet labels or bold typographic gestures. A punk venue poster might have larger, more aggressive type than a minimalist ceramics studio identity. Most of the time, font should be thin. All use of fonts must be design-forward and prioritize visual communication. Regardless of text scale, nothing falls off the page and nothing overlaps. Every element must be contained within the canvas boundaries with proper margins. Check carefully that all text, graphics, and visual elements have breathing room and clear separation. This is non-negotiable for professional execution. **IMPORTANT: Use different fonts if writing text. Search the `./canvas-fonts` directory. Regardless of approach, sophistication is non-negotiable.**\n\nDownload and use whatever fonts are needed to make this a reality. Get creative by making the typography actually part of the art itself -- if the art is abstract, bring the font onto the canvas, not typeset digitally.\n\nTo push boundaries, follow design instinct/intuition while using the philosophy as a guiding principle. Embrace ultimate design freedom and choice. Push aesthetics and design to the frontier. \n\n**CRITICAL**: To achieve human-crafted quality (not AI-generated), create work that looks like it took countless hours. Make it appear as though someone at the absolute top of their field labored over every detail with painstaking care. Ensure the composition, spacing, color choices, typography - everything screams expert-level craftsmanship. Double-check that nothing overlaps, formatting is flawless, every detail perfect. Create something that could be shown to people to prove expertise and rank as undeniably impressive.\n\nOutput the final result as a single, downloadable .pdf or .png file, alongside the design philosophy used as a .md file.\n\n---\n\n## FINAL STEP\n\n**IMPORTANT**: The user ALREADY said \"It isn't perfect enough. It must be pristine, a masterpiece if craftsmanship, as if it were about to be displayed in a museum.\"\n\n**CRITICAL**: To refine the work, avoid adding more graphics; instead refine what has been created and make it extremely crisp, respecting the design philosophy and the principles of minimalism entirely. Rather than adding a fun filter or refactoring a font, consider how to make the existing composition more cohesive with the art. If the instinct is to call a new function or draw a new shape, STOP and instead ask: \"How can I make what's already here more of a piece of art?\"\n\nTake a second pass. Go back to the code and refine/polish further to make this a philosophically designed masterpiece.\n\n## MULTI-PAGE OPTION\n\nTo create additional pages when requested, create more creative pages along the same lines as the design philosophy but distinctly different as well. Bundle those pages in the same .pdf or many .pngs. Treat the first page as just a single page in a whole coffee table book waiting to be filled. Make the next pages unique twists and memories of the original. Have them almost tell a story in a very tasteful way. Exercise full creative freedom.",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:08.387Z"
  },
  {
    "id": "doc-coauthoring",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/doc-coauthoring",
      "url": "https://github.com/anthropics/skills/tree/main/skills/doc-coauthoring"
    },
    "frontmatter": {
      "name": "doc-coauthoring",
      "description": "Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks."
    },
    "content": "# Doc Co-Authoring Workflow\n\nThis skill provides a structured workflow for guiding users through collaborative document creation. Act as an active guide, walking users through three stages: Context Gathering, Refinement & Structure, and Reader Testing.\n\n## When to Offer This Workflow\n\n**Trigger conditions:**\n- User mentions writing documentation: \"write a doc\", \"draft a proposal\", \"create a spec\", \"write up\"\n- User mentions specific doc types: \"PRD\", \"design doc\", \"decision doc\", \"RFC\"\n- User seems to be starting a substantial writing task\n\n**Initial offer:**\nOffer the user a structured workflow for co-authoring the document. Explain the three stages:\n\n1. **Context Gathering**: User provides all relevant context while Claude asks clarifying questions\n2. **Refinement & Structure**: Iteratively build each section through brainstorming and editing\n3. **Reader Testing**: Test the doc with a fresh Claude (no context) to catch blind spots before others read it\n\nExplain that this approach helps ensure the doc works well when others read it (including when they paste it into Claude). Ask if they want to try this workflow or prefer to work freeform.\n\nIf user declines, work freeform. If user accepts, proceed to Stage 1.\n\n## Stage 1: Context Gathering\n\n**Goal:** Close the gap between what the user knows and what Claude knows, enabling smart guidance later.\n\n### Initial Questions\n\nStart by asking the user for meta-context about the document:\n\n1. What type of document is this? (e.g., technical spec, decision doc, proposal)\n2. Who's the primary audience?\n3. What's the desired impact when someone reads this?\n4. Is there a template or specific format to follow?\n5. Any other constraints or context to know?\n\nInform them they can answer in shorthand or dump information however works best for them.\n\n**If user provides a template or mentions a doc type:**\n- Ask if they have a template document to share\n- If they provide a link to a shared document, use the appropriate integration to fetch it\n- If they provide a file, read it\n\n**If user mentions editing an existing shared document:**\n- Use the appropriate integration to read the current state\n- Check for images without alt-text\n- If images exist without alt-text, explain that when others use Claude to understand the doc, Claude won't be able to see them. Ask if they want alt-text generated. If so, request they paste each image into chat for descriptive alt-text generation.\n\n### Info Dumping\n\nOnce initial questions are answered, encourage the user to dump all the context they have. Request information such as:\n- Background on the project/problem\n- Related team discussions or shared documents\n- Why alternative solutions aren't being used\n- Organizational context (team dynamics, past incidents, politics)\n- Timeline pressures or constraints\n- Technical architecture or dependencies\n- Stakeholder concerns\n\nAdvise them not to worry about organizing it - just get it all out. Offer multiple ways to provide context:\n- Info dump stream-of-consciousness\n- Point to team channels or threads to read\n- Link to shared documents\n\n**If integrations are available** (e.g., Slack, Teams, Google Drive, SharePoint, or other MCP servers), mention that these can be used to pull in context directly.\n\n**If no integrations are detected and in Claude.ai or Claude app:** Suggest they can enable connectors in their Claude settings to allow pulling context from messaging apps and document storage directly.\n\nInform them clarifying questions will be asked once they've done their initial dump.\n\n**During context gathering:**\n\n- If user mentions team channels or shared documents:\n  - If integrations available: Inform them the content will be read now, then use the appropriate integration\n  - If integrations not available: Explain lack of access. Suggest they enable connectors in Claude settings, or paste the relevant content directly.\n\n- If user mentions entities/projects that are unknown:\n  - Ask if connected tools should be searched to learn more\n  - Wait for user confirmation before searching\n\n- As user provides context, track what's being learned and what's still unclear\n\n**Asking clarifying questions:**\n\nWhen user signals they've done their initial dump (or after substantial context provided), ask clarifying questions to ensure understanding:\n\nGenerate 5-10 numbered questions based on gaps in the context.\n\nInform them they can use shorthand to answer (e.g., \"1: yes, 2: see #channel, 3: no because backwards compat\"), link to more docs, point to channels to read, or just keep info-dumping. Whatever's most efficient for them.\n\n**Exit condition:**\nSufficient context has been gathered when questions show understanding - when edge cases and trade-offs can be asked about without needing basics explained.\n\n**Transition:**\nAsk if there's any more context they want to provide at this stage, or if it's time to move on to drafting the document.\n\nIf user wants to add more, let them. When ready, proceed to Stage 2.\n\n## Stage 2: Refinement & Structure\n\n**Goal:** Build the document section by section through brainstorming, curation, and iterative refinement.\n\n**Instructions to user:**\nExplain that the document will be built section by section. For each section:\n1. Clarifying questions will be asked about what to include\n2. 5-20 options will be brainstormed\n3. User will indicate what to keep/remove/combine\n4. The section will be drafted\n5. It will be refined through surgical edits\n\nStart with whichever section has the most unknowns (usually the core decision/proposal), then work through the rest.\n\n**Section ordering:**\n\nIf the document structure is clear:\nAsk which section they'd like to start with.\n\nSuggest starting with whichever section has the most unknowns. For decision docs, that's usually the core proposal. For specs, it's typically the technical approach. Summary sections are best left for last.\n\nIf user doesn't know what sections they need:\nBased on the type of document and template, suggest 3-5 sections appropriate for the doc type.\n\nAsk if this structure works, or if they want to adjust it.\n\n**Once structure is agreed:**\n\nCreate the initial document structure with placeholder text for all sections.\n\n**If access to artifacts is available:**\nUse `create_file` to create an artifact. This gives both Claude and the user a scaffold to work from.\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate artifact with all section headers and brief placeholder text like \"[To be written]\" or \"[Content here]\".\n\nProvide the scaffold link and indicate it's time to fill in each section.\n\n**If no access to artifacts:**\nCreate a markdown file in the working directory. Name it appropriately (e.g., `decision-doc.md`, `technical-spec.md`).\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate file with all section headers and placeholder text.\n\nConfirm the filename has been created and indicate it's time to fill in each section.\n\n**For each section:**\n\n### Step 1: Clarifying Questions\n\nAnnounce work will begin on the [SECTION NAME] section. Ask 5-10 clarifying questions about what should be included:\n\nGenerate 5-10 specific questions based on context and section purpose.\n\nInform them they can answer in shorthand or just indicate what's important to cover.\n\n### Step 2: Brainstorming\n\nFor the [SECTION NAME] section, brainstorm [5-20] things that might be included, depending on the section's complexity. Look for:\n- Context shared that might have been forgotten\n- Angles or considerations not yet mentioned\n\nGenerate 5-20 numbered options based on section complexity. At the end, offer to brainstorm more if they want additional options.\n\n### Step 3: Curation\n\nAsk which points should be kept, removed, or combined. Request brief justifications to help learn priorities for the next sections.\n\nProvide examples:\n- \"Keep 1,4,7,9\"\n- \"Remove 3 (duplicates 1)\"\n- \"Remove 6 (audience already knows this)\"\n- \"Combine 11 and 12\"\n\n**If user gives freeform feedback** (e.g., \"looks good\" or \"I like most of it but...\") instead of numbered selections, extract their preferences and proceed. Parse what they want kept/removed/changed and apply it.\n\n### Step 4: Gap Check\n\nBased on what they've selected, ask if there's anything important missing for the [SECTION NAME] section.\n\n### Step 5: Drafting\n\nUse `str_replace` to replace the placeholder text for this section with the actual drafted content.\n\nAnnounce the [SECTION NAME] section will be drafted now based on what they've selected.\n\n**If using artifacts:**\nAfter drafting, provide a link to the artifact.\n\nAsk them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**If using a file (no artifacts):**\nAfter drafting, confirm completion.\n\nInform them the [SECTION NAME] section has been drafted in [filename]. Ask them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**Key instruction for user (include when drafting the first section):**\nProvide a note: Instead of editing the doc directly, ask them to indicate what to change. This helps learning of their style for future sections. For example: \"Remove the X bullet - already covered by Y\" or \"Make the third paragraph more concise\".\n\n### Step 6: Iterative Refinement\n\nAs user provides feedback:\n- Use `str_replace` to make edits (never reprint the whole doc)\n- **If using artifacts:** Provide link to artifact after each edit\n- **If using files:** Just confirm edits are complete\n- If user edits doc directly and asks to read it: mentally note the changes they made and keep them in mind for future sections (this shows their preferences)\n\n**Continue iterating** until user is satisfied with the section.\n\n### Quality Checking\n\nAfter 3 consecutive iterations with no substantial changes, ask if anything can be removed without losing important information.\n\nWhen section is done, confirm [SECTION NAME] is complete. Ask if ready to move to the next section.\n\n**Repeat for all sections.**\n\n### Near Completion\n\nAs approaching completion (80%+ of sections done), announce intention to re-read the entire document and check for:\n- Flow and consistency across sections\n- Redundancy or contradictions\n- Anything that feels like \"slop\" or generic filler\n- Whether every sentence carries weight\n\nRead entire document and provide feedback.\n\n**When all sections are drafted and refined:**\nAnnounce all sections are drafted. Indicate intention to review the complete document one more time.\n\nReview for overall coherence, flow, completeness.\n\nProvide any final suggestions.\n\nAsk if ready to move to Reader Testing, or if they want to refine anything else.\n\n## Stage 3: Reader Testing\n\n**Goal:** Test the document with a fresh Claude (no context bleed) to verify it works for readers.\n\n**Instructions to user:**\nExplain that testing will now occur to see if the document actually works for readers. This catches blind spots - things that make sense to the authors but might confuse others.\n\n### Testing Approach\n\n**If access to sub-agents is available (e.g., in Claude Code):**\n\nPerform the testing directly without user involvement.\n\n### Step 1: Predict Reader Questions\n\nAnnounce intention to predict what questions readers might ask when trying to discover this document.\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Test with Sub-Agent\n\nAnnounce that these questions will be tested with a fresh Claude instance (no context from this conversation).\n\nFor each question, invoke a sub-agent with just the document content and the question.\n\nSummarize what Reader Claude got right/wrong for each question.\n\n### Step 3: Run Additional Checks\n\nAnnounce additional checks will be performed.\n\nInvoke sub-agent to check for ambiguity, false assumptions, contradictions.\n\nSummarize any issues found.\n\n### Step 4: Report and Fix\n\nIf issues found:\nReport that Reader Claude struggled with specific issues.\n\nList the specific issues.\n\nIndicate intention to fix these gaps.\n\nLoop back to refinement for problematic sections.\n\n---\n\n**If no access to sub-agents (e.g., claude.ai web interface):**\n\nThe user will need to do the testing manually.\n\n### Step 1: Predict Reader Questions\n\nAsk what questions people might ask when trying to discover this document. What would they type into Claude.ai?\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Setup Testing\n\nProvide testing instructions:\n1. Open a fresh Claude conversation: https://claude.ai\n2. Paste or share the document content (if using a shared doc platform with connectors enabled, provide the link)\n3. Ask Reader Claude the generated questions\n\nFor each question, instruct Reader Claude to provide:\n- The answer\n- Whether anything was ambiguous or unclear\n- What knowledge/context the doc assumes is already known\n\nCheck if Reader Claude gives correct answers or misinterprets anything.\n\n### Step 3: Additional Checks\n\nAlso ask Reader Claude:\n- \"What in this doc might be ambiguous or unclear to readers?\"\n- \"What knowledge or context does this doc assume readers already have?\"\n- \"Are there any internal contradictions or inconsistencies?\"\n\n### Step 4: Iterate Based on Results\n\nAsk what Reader Claude got wrong or struggled with. Indicate intention to fix those gaps.\n\nLoop back to refinement for any problematic sections.\n\n---\n\n### Exit Condition (Both Approaches)\n\nWhen Reader Claude consistently answers questions correctly and doesn't surface new gaps or ambiguities, the doc is ready.\n\n## Final Review\n\nWhen Reader Testing passes:\nAnnounce the doc has passed Reader Claude testing. Before completion:\n\n1. Recommend they do a final read-through themselves - they own this document and are responsible for its quality\n2. Suggest double-checking any facts, links, or technical details\n3. Ask them to verify it achieves the impact they wanted\n\nAsk if they want one more review, or if the work is done.\n\n**If user wants final review, provide it. Otherwise:**\nAnnounce document completion. Provide a few final tips:\n- Consider linking this conversation in an appendix so readers can see how the doc was developed\n- Use appendices to provide depth without bloating the main doc\n- Update the doc as feedback is received from real readers\n\n## Tips for Effective Guidance\n\n**Tone:**\n- Be direct and procedural\n- Explain rationale briefly when it affects user behavior\n- Don't try to \"sell\" the approach - just execute it\n\n**Handling Deviations:**\n- If user wants to skip a stage: Ask if they want to skip this and write freeform\n- If user seems frustrated: Acknowledge this is taking longer than expected. Suggest ways to move faster\n- Always give user agency to adjust the process\n\n**Context Management:**\n- Throughout, if context is missing on something mentioned, proactively ask\n- Don't let gaps accumulate - address them as they come up\n\n**Artifact Management:**\n- Use `create_file` for drafting full sections\n- Use `str_replace` for all edits\n- Provide artifact link after every change\n- Never use artifacts for brainstorming lists - that's just conversation\n\n**Quality over Speed:**\n- Don't rush through stages\n- Each iteration should make meaningful improvements\n- The goal is a document that actually works for readers\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:08.636Z"
  },
  {
    "id": "docx",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/docx",
      "url": "https://github.com/anthropics/skills/tree/main/skills/docx"
    },
    "frontmatter": {
      "name": "docx",
      "description": "\"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"",
      "license": "Proprietary. LICENSE.txt has complete terms"
    },
    "content": "# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:08.884Z"
  },
  {
    "id": "frontend-design",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/frontend-design",
      "url": "https://github.com/anthropics/skills/tree/main/skills/frontend-design"
    },
    "frontmatter": {
      "name": "frontend-design",
      "description": "Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, artifacts, posters, or applications (examples include websites, landing pages, dashboards, React components, HTML/CSS layouts, or when styling/beautifying any web UI). Generates creative, polished code and UI design that avoids generic AI aesthetics.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "This skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:09.134Z"
  },
  {
    "id": "internal-comms",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/internal-comms",
      "url": "https://github.com/anthropics/skills/tree/main/skills/internal-comms"
    },
    "frontmatter": {
      "name": "internal-comms",
      "description": "A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:09.378Z"
  },
  {
    "id": "mcp-builder",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/mcp-builder",
      "url": "https://github.com/anthropics/skills/tree/main/skills/mcp-builder"
    },
    "frontmatter": {
      "name": "mcp-builder",
      "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# MCP Server Development Guide\n\n## Overview\n\nCreate MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks.\n\n---\n\n# Process\n\n## 🚀 High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Modern MCP Design\n\n**API Coverage vs. Workflow Tools:**\nBalance comprehensive API endpoint coverage with specialized workflow tools. Workflow tools can be more convenient for specific tasks, while comprehensive coverage gives agents flexibility to compose operations. Performance varies by client—some clients benefit from code execution that combines basic tools, while others work better with higher-level workflows. When uncertain, prioritize comprehensive API coverage.\n\n**Tool Naming and Discoverability:**\nClear, descriptive tool names help agents find the right tools quickly. Use consistent prefixes (e.g., `github_create_issue`, `github_list_repos`) and action-oriented naming.\n\n**Context Management:**\nAgents benefit from concise tool descriptions and the ability to filter/paginate results. Design tools that return focused, relevant data. Some clients support code execution which can help agents filter and process data efficiently.\n\n**Actionable Error Messages:**\nError messages should guide agents toward solutions with specific suggestions and next steps.\n\n#### 1.2 Study MCP Protocol Documentation\n\n**Navigate the MCP specification:**\n\nStart with the sitemap to find relevant pages: `https://modelcontextprotocol.io/sitemap.xml`\n\nThen fetch specific pages with `.md` suffix for markdown format (e.g., `https://modelcontextprotocol.io/specification/draft.md`).\n\nKey pages to review:\n- Specification overview and architecture\n- Transport mechanisms (streamable HTTP, stdio)\n- Tool, resource, and prompt definitions\n\n#### 1.3 Study Framework Documentation\n\n**Recommended stack:**\n- **Language**: TypeScript (high-quality SDK support and good compatibility in many execution environments e.g. MCPB. Plus AI models are good at generating TypeScript code, benefiting from its broad usage, static typing and good linting tools)\n- **Transport**: Streamable HTTP for remote servers, using stateless JSON (simpler to scale and maintain, as opposed to stateful sessions and streaming responses). stdio for local servers.\n\n**Load framework documentation:**\n\n- **MCP Best Practices**: [📋 View Best Practices](./reference/mcp_best_practices.md) - Core guidelines\n\n**For TypeScript (recommended):**\n- **TypeScript SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [⚡ TypeScript Guide](./reference/node_mcp_server.md) - TypeScript patterns and examples\n\n**For Python:**\n- **Python SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [🐍 Python Guide](./reference/python_mcp_server.md) - Python patterns and examples\n\n#### 1.4 Plan Your Implementation\n\n**Understand the API:**\nReview the service's API documentation to identify key endpoints, authentication requirements, and data models. Use web search and WebFetch as needed.\n\n**Tool Selection:**\nPrioritize comprehensive API coverage. List endpoints to implement, starting with the most common operations.\n\n---\n\n### Phase 2: Implementation\n\n#### 2.1 Set Up Project Structure\n\nSee language-specific guides for project setup:\n- [⚡ TypeScript Guide](./reference/node_mcp_server.md) - Project structure, package.json, tsconfig.json\n- [🐍 Python Guide](./reference/python_mcp_server.md) - Module organization, dependencies\n\n#### 2.2 Implement Core Infrastructure\n\nCreate shared utilities:\n- API client with authentication\n- Error handling helpers\n- Response formatting (JSON/Markdown)\n- Pagination support\n\n#### 2.3 Implement Tools\n\nFor each tool:\n\n**Input Schema:**\n- Use Zod (TypeScript) or Pydantic (Python)\n- Include constraints and clear descriptions\n- Add examples in field descriptions\n\n**Output Schema:**\n- Define `outputSchema` where possible for structured data\n- Use `structuredContent` in tool responses (TypeScript SDK feature)\n- Helps clients understand and process tool outputs\n\n**Tool Description:**\n- Concise summary of functionality\n- Parameter descriptions\n- Return type schema\n\n**Implementation:**\n- Async/await for I/O operations\n- Proper error handling with actionable messages\n- Support pagination where applicable\n- Return both text content and structured data when using modern SDKs\n\n**Annotations:**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: Review and Test\n\n#### 3.1 Code Quality\n\nReview for:\n- No duplicated code (DRY principle)\n- Consistent error handling\n- Full type coverage\n- Clear tool descriptions\n\n#### 3.2 Build and Test\n\n**TypeScript:**\n- Run `npm run build` to verify compilation\n- Test with MCP Inspector: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- Verify syntax: `python -m py_compile your_server.py`\n- Test with MCP Inspector\n\nSee language-specific guides for detailed testing approaches and quality checklists.\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [✅ Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nUse evaluations to test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEnsure each question is:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n## 📚 Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Start with sitemap at `https://modelcontextprotocol.io/sitemap.xml`, then fetch specific pages with `.md` suffix\n- [📋 MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Transport selection (streamable HTTP vs stdio)\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [🐍 Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [⚡ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [✅ Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:09.626Z"
  },
  {
    "id": "pdf",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/pdf",
      "url": "https://github.com/anthropics/skills/tree/main/skills/pdf"
    },
    "frontmatter": {
      "name": "pdf",
      "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
      "license": "Proprietary. LICENSE.txt has complete terms"
    },
    "content": "# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:09.877Z"
  },
  {
    "id": "pptx",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/pptx",
      "url": "https://github.com/anthropics/skills/tree/main/skills/pptx"
    },
    "frontmatter": {
      "name": "pptx",
      "description": "\"Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks\"",
      "license": "Proprietary. LICENSE.txt has complete terms"
    },
    "content": "# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n- ✅ State your content-informed design approach BEFORE writing code\n- ✅ Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n- ✅ Create clear visual hierarchy through size, weight, and color\n- ✅ Ensure readability: strong contrast, appropriately sized text, clean alignment\n- ✅ Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90° or 270°\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (3×3, 4×4 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt × 405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (•, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (5×6)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:10.110Z"
  },
  {
    "id": "skill-creator",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/skill-creator",
      "url": "https://github.com/anthropics/skills/tree/main/skills/skill-creator"
    },
    "frontmatter": {
      "name": "skill-creator",
      "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasks—they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n├── SKILL.md (required)\n│   ├── YAML frontmatter metadata (required)\n│   │   ├── name: (required)\n│   │   └── description: (required)\n│   └── Markdown instructions (required)\n└── Bundled Resources (optional)\n    ├── scripts/          - Executable code (Python/Bash/etc.)\n    ├── references/       - Documentation intended to be loaded into context as needed\n    └── assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n├── SKILL.md (overview and navigation)\n└── reference/\n    ├── finance.md (revenue, billing metrics)\n    ├── sales.md (opportunities, pipeline)\n    ├── product.md (API usage, features)\n    └── marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n├── SKILL.md (workflow + provider selection)\n└── references/\n    ├── aws.md (AWS deployment patterns)\n    ├── gcp.md (GCP deployment patterns)\n    └── azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:10.340Z"
  },
  {
    "id": "slack-gif-creator",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/slack-gif-creator",
      "url": "https://github.com/anthropics/skills/tree/main/skills/slack-gif-creator"
    },
    "frontmatter": {
      "name": "slack-gif-creator",
      "description": "Knowledge and utilities for creating animated GIFs optimized for Slack. Provides constraints, validation tools, and animation concepts. Use when users request animated GIFs for Slack like \"make me a GIF of X doing Y for Slack.\"",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Slack GIF Creator\n\nA toolkit providing utilities and knowledge for creating animated GIFs optimized for Slack.\n\n## Slack Requirements\n\n**Dimensions:**\n- Emoji GIFs: 128x128 (recommended)\n- Message GIFs: 480x480\n\n**Parameters:**\n- FPS: 10-30 (lower is smaller file size)\n- Colors: 48-128 (fewer = smaller file size)\n- Duration: Keep under 3 seconds for emoji GIFs\n\n## Core Workflow\n\n```python\nfrom core.gif_builder import GIFBuilder\nfrom PIL import Image, ImageDraw\n\n# 1. Create builder\nbuilder = GIFBuilder(width=128, height=128, fps=10)\n\n# 2. Generate frames\nfor i in range(12):\n    frame = Image.new('RGB', (128, 128), (240, 248, 255))\n    draw = ImageDraw.Draw(frame)\n\n    # Draw your animation using PIL primitives\n    # (circles, polygons, lines, etc.)\n\n    builder.add_frame(frame)\n\n# 3. Save with optimization\nbuilder.save('output.gif', num_colors=48, optimize_for_emoji=True)\n```\n\n## Drawing Graphics\n\n### Working with User-Uploaded Images\nIf a user uploads an image, consider whether they want to:\n- **Use it directly** (e.g., \"animate this\", \"split this into frames\")\n- **Use it as inspiration** (e.g., \"make something like this\")\n\nLoad and work with images using PIL:\n```python\nfrom PIL import Image\n\nuploaded = Image.open('file.png')\n# Use directly, or just as reference for colors/style\n```\n\n### Drawing from Scratch\nWhen drawing graphics from scratch, use PIL ImageDraw primitives:\n\n```python\nfrom PIL import ImageDraw\n\ndraw = ImageDraw.Draw(frame)\n\n# Circles/ovals\ndraw.ellipse([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Stars, triangles, any polygon\npoints = [(x1, y1), (x2, y2), (x3, y3), ...]\ndraw.polygon(points, fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Lines\ndraw.line([(x1, y1), (x2, y2)], fill=(r, g, b), width=5)\n\n# Rectangles\ndraw.rectangle([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n```\n\n**Don't use:** Emoji fonts (unreliable across platforms) or assume pre-packaged graphics exist in this skill.\n\n### Making Graphics Look Good\n\nGraphics should look polished and creative, not basic. Here's how:\n\n**Use thicker lines** - Always set `width=2` or higher for outlines and lines. Thin lines (width=1) look choppy and amateurish.\n\n**Add visual depth**:\n- Use gradients for backgrounds (`create_gradient_background`)\n- Layer multiple shapes for complexity (e.g., a star with a smaller star inside)\n\n**Make shapes more interesting**:\n- Don't just draw a plain circle - add highlights, rings, or patterns\n- Stars can have glows (draw larger, semi-transparent versions behind)\n- Combine multiple shapes (stars + sparkles, circles + rings)\n\n**Pay attention to colors**:\n- Use vibrant, complementary colors\n- Add contrast (dark outlines on light shapes, light outlines on dark shapes)\n- Consider the overall composition\n\n**For complex shapes** (hearts, snowflakes, etc.):\n- Use combinations of polygons and ellipses\n- Calculate points carefully for symmetry\n- Add details (a heart can have a highlight curve, snowflakes have intricate branches)\n\nBe creative and detailed! A good Slack GIF should look polished, not like placeholder graphics.\n\n## Available Utilities\n\n### GIFBuilder (`core.gif_builder`)\nAssembles frames and optimizes for Slack:\n```python\nbuilder = GIFBuilder(width=128, height=128, fps=10)\nbuilder.add_frame(frame)  # Add PIL Image\nbuilder.add_frames(frames)  # Add list of frames\nbuilder.save('out.gif', num_colors=48, optimize_for_emoji=True, remove_duplicates=True)\n```\n\n### Validators (`core.validators`)\nCheck if GIF meets Slack requirements:\n```python\nfrom core.validators import validate_gif, is_slack_ready\n\n# Detailed validation\npasses, info = validate_gif('my.gif', is_emoji=True, verbose=True)\n\n# Quick check\nif is_slack_ready('my.gif'):\n    print(\"Ready!\")\n```\n\n### Easing Functions (`core.easing`)\nSmooth motion instead of linear:\n```python\nfrom core.easing import interpolate\n\n# Progress from 0.0 to 1.0\nt = i / (num_frames - 1)\n\n# Apply easing\ny = interpolate(start=0, end=400, t=t, easing='ease_out')\n\n# Available: linear, ease_in, ease_out, ease_in_out,\n#           bounce_out, elastic_out, back_out\n```\n\n### Frame Helpers (`core.frame_composer`)\nConvenience functions for common needs:\n```python\nfrom core.frame_composer import (\n    create_blank_frame,         # Solid color background\n    create_gradient_background,  # Vertical gradient\n    draw_circle,                # Helper for circles\n    draw_text,                  # Simple text rendering\n    draw_star                   # 5-pointed star\n)\n```\n\n## Animation Concepts\n\n### Shake/Vibrate\nOffset object position with oscillation:\n- Use `math.sin()` or `math.cos()` with frame index\n- Add small random variations for natural feel\n- Apply to x and/or y position\n\n### Pulse/Heartbeat\nScale object size rhythmically:\n- Use `math.sin(t * frequency * 2 * math.pi)` for smooth pulse\n- For heartbeat: two quick pulses then pause (adjust sine wave)\n- Scale between 0.8 and 1.2 of base size\n\n### Bounce\nObject falls and bounces:\n- Use `interpolate()` with `easing='bounce_out'` for landing\n- Use `easing='ease_in'` for falling (accelerating)\n- Apply gravity by increasing y velocity each frame\n\n### Spin/Rotate\nRotate object around center:\n- PIL: `image.rotate(angle, resample=Image.BICUBIC)`\n- For wobble: use sine wave for angle instead of linear\n\n### Fade In/Out\nGradually appear or disappear:\n- Create RGBA image, adjust alpha channel\n- Or use `Image.blend(image1, image2, alpha)`\n- Fade in: alpha from 0 to 1\n- Fade out: alpha from 1 to 0\n\n### Slide\nMove object from off-screen to position:\n- Start position: outside frame bounds\n- End position: target location\n- Use `interpolate()` with `easing='ease_out'` for smooth stop\n- For overshoot: use `easing='back_out'`\n\n### Zoom\nScale and position for zoom effect:\n- Zoom in: scale from 0.1 to 2.0, crop center\n- Zoom out: scale from 2.0 to 1.0\n- Can add motion blur for drama (PIL filter)\n\n### Explode/Particle Burst\nCreate particles radiating outward:\n- Generate particles with random angles and velocities\n- Update each particle: `x += vx`, `y += vy`\n- Add gravity: `vy += gravity_constant`\n- Fade out particles over time (reduce alpha)\n\n## Optimization Strategies\n\nOnly when asked to make the file size smaller, implement a few of the following methods:\n\n1. **Fewer frames** - Lower FPS (10 instead of 20) or shorter duration\n2. **Fewer colors** - `num_colors=48` instead of 128\n3. **Smaller dimensions** - 128x128 instead of 480x480\n4. **Remove duplicates** - `remove_duplicates=True` in save()\n5. **Emoji mode** - `optimize_for_emoji=True` auto-optimizes\n\n```python\n# Maximum optimization for emoji\nbuilder.save(\n    'emoji.gif',\n    num_colors=48,\n    optimize_for_emoji=True,\n    remove_duplicates=True\n)\n```\n\n## Philosophy\n\nThis skill provides:\n- **Knowledge**: Slack's requirements and animation concepts\n- **Utilities**: GIFBuilder, validators, easing functions\n- **Flexibility**: Create the animation logic using PIL primitives\n\nIt does NOT provide:\n- Rigid animation templates or pre-made functions\n- Emoji font rendering (unreliable across platforms)\n- A library of pre-packaged graphics built into the skill\n\n**Note on user uploads**: This skill doesn't include pre-built graphics, but if a user uploads an image, use PIL to load and work with it - interpret based on their request whether they want it used directly or just as inspiration.\n\nBe creative! Combine concepts (bouncing + rotating, pulsing + sliding, etc.) and use PIL's full capabilities.\n\n## Dependencies\n\n```bash\npip install pillow imageio numpy\n```\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:10.596Z"
  },
  {
    "id": "theme-factory",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/theme-factory",
      "url": "https://github.com/anthropics/skills/tree/main/skills/theme-factory"
    },
    "frontmatter": {
      "name": "theme-factory",
      "description": "Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:10.842Z"
  },
  {
    "id": "web-artifacts-builder",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/web-artifacts-builder",
      "url": "https://github.com/anthropics/skills/tree/main/skills/web-artifacts-builder"
    },
    "frontmatter": {
      "name": "web-artifacts-builder",
      "description": "Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Web Artifacts Builder\n\nTo build powerful frontend claude.ai artifacts, follow these steps:\n1. Initialize the frontend repo using `scripts/init-artifact.sh`\n2. Develop your artifact by editing the generated code\n3. Bundle all code into a single HTML file using `scripts/bundle-artifact.sh`\n4. Display artifact to user\n5. (Optional) Test the artifact\n\n**Stack**: React 18 + TypeScript + Vite + Parcel (bundling) + Tailwind CSS + shadcn/ui\n\n## Design & Style Guidelines\n\nVERY IMPORTANT: To avoid what is often referred to as \"AI slop\", avoid using excessive centered layouts, purple gradients, uniform rounded corners, and Inter font.\n\n## Quick Start\n\n### Step 1: Initialize Project\n\nRun the initialization script to create a new React project:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\nThis creates a fully configured project with:\n- ✅ React + TypeScript (via Vite)\n- ✅ Tailwind CSS 3.4.1 with shadcn/ui theming system\n- ✅ Path aliases (`@/`) configured\n- ✅ 40+ shadcn/ui components pre-installed\n- ✅ All Radix UI dependencies included\n- ✅ Parcel configured for bundling (via .parcelrc)\n- ✅ Node 18+ compatibility (auto-detects and pins Vite version)\n\n### Step 2: Develop Your Artifact\n\nTo build the artifact, edit the generated files. See **Common Development Tasks** below for guidance.\n\n### Step 3: Bundle to Single HTML File\n\nTo bundle the React app into a single HTML artifact:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\nThis creates `bundle.html` - a self-contained artifact with all JavaScript, CSS, and dependencies inlined. This file can be directly shared in Claude conversations as an artifact.\n\n**Requirements**: Your project must have an `index.html` in the root directory.\n\n**What the script does**:\n- Installs bundling dependencies (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- Creates `.parcelrc` config with path alias support\n- Builds with Parcel (no source maps)\n- Inlines all assets into single HTML using html-inline\n\n### Step 4: Share Artifact with User\n\nFinally, share the bundled HTML file in conversation with the user so they can view it as an artifact.\n\n### Step 5: Testing/Visualizing the Artifact (Optional)\n\nNote: This is a completely optional step. Only perform if necessary or requested.\n\nTo test/visualize the artifact, use available tools (including other Skills or built-in tools like Playwright or Puppeteer). In general, avoid testing the artifact upfront as it adds latency between the request and when the finished artifact can be seen. Test later, after presenting the artifact, if requested or if issues arise.\n\n## Reference\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:11.092Z"
  },
  {
    "id": "webapp-testing",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/webapp-testing",
      "url": "https://github.com/anthropics/skills/tree/main/skills/webapp-testing"
    },
    "frontmatter": {
      "name": "webapp-testing",
      "description": "Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task → Is it static HTML?\n    ├─ Yes → Read HTML file directly to identify selectors\n    │         ├─ Success → Write Playwright script using selectors\n    │         └─ Fails/Incomplete → Treat as dynamic (below)\n    │\n    └─ No (dynamic webapp) → Is the server already running?\n        ├─ No → Run: python scripts/with_server.py --help\n        │        Then use the helper + write simplified Playwright script\n        │\n        └─ Yes → Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\n❌ **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\n✅ **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:11.337Z"
  },
  {
    "id": "xlsx",
    "source": {
      "type": "github-folder",
      "repo": "anthropics/skills",
      "path": "skills/xlsx",
      "url": "https://github.com/anthropics/skills/tree/main/skills/xlsx"
    },
    "frontmatter": {
      "name": "xlsx",
      "description": "\"Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas\"",
      "license": "Proprietary. LICENSE.txt has complete terms"
    },
    "content": "# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n### ❌ WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n### ✅ CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:11.587Z"
  },
  {
    "id": "brainstorming",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/brainstorming",
      "url": "https://github.com/obra/superpowers/tree/main/skills/brainstorming"
    },
    "frontmatter": {
      "name": "brainstorming",
      "description": "\"You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation.\""
    },
    "content": "# Brainstorming Ideas Into Designs\n\n## Overview\n\nHelp turn ideas into fully formed designs and specs through natural collaborative dialogue.\n\nStart by understanding the current project context, then ask questions one at a time to refine the idea. Once you understand what you're building, present the design in small sections (200-300 words), checking after each section whether it looks right so far.\n\n## The Process\n\n**Understanding the idea:**\n- Check out the current project state first (files, docs, recent commits)\n- Ask questions one at a time to refine the idea\n- Prefer multiple choice questions when possible, but open-ended is fine too\n- Only one question per message - if a topic needs more exploration, break it into multiple questions\n- Focus on understanding: purpose, constraints, success criteria\n\n**Exploring approaches:**\n- Propose 2-3 different approaches with trade-offs\n- Present options conversationally with your recommendation and reasoning\n- Lead with your recommended option and explain why\n\n**Presenting the design:**\n- Once you believe you understand what you're building, present the design\n- Break it into sections of 200-300 words\n- Ask after each section whether it looks right so far\n- Cover: architecture, components, data flow, error handling, testing\n- Be ready to go back and clarify if something doesn't make sense\n\n## After the Design\n\n**Documentation:**\n- Write the validated design to `docs/plans/YYYY-MM-DD-<topic>-design.md`\n- Use elements-of-style:writing-clearly-and-concisely skill if available\n- Commit the design document to git\n\n**Implementation (if continuing):**\n- Ask: \"Ready to set up for implementation?\"\n- Use superpowers:using-git-worktrees to create isolated workspace\n- Use superpowers:writing-plans to create detailed implementation plan\n\n## Key Principles\n\n- **One question at a time** - Don't overwhelm with multiple questions\n- **Multiple choice preferred** - Easier to answer than open-ended when possible\n- **YAGNI ruthlessly** - Remove unnecessary features from all designs\n- **Explore alternatives** - Always propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n- **Be flexible** - Go back and clarify when something doesn't make sense\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:12.837Z"
  },
  {
    "id": "dispatching-parallel-agents",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/dispatching-parallel-agents",
      "url": "https://github.com/obra/superpowers/tree/main/skills/dispatching-parallel-agents"
    },
    "frontmatter": {
      "name": "dispatching-parallel-agents",
      "description": "Use when facing 2+ independent tasks that can be worked on without shared state or sequential dependencies"
    },
    "content": "# Dispatching Parallel Agents\n\n## Overview\n\nWhen you have multiple unrelated failures (different test files, different subsystems, different bugs), investigating them sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Dispatch one agent per independent problem domain. Let them work concurrently.\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Multiple failures?\" [shape=diamond];\n    \"Are they independent?\" [shape=diamond];\n    \"Single agent investigates all\" [shape=box];\n    \"One agent per problem domain\" [shape=box];\n    \"Can they work in parallel?\" [shape=diamond];\n    \"Sequential agents\" [shape=box];\n    \"Parallel dispatch\" [shape=box];\n\n    \"Multiple failures?\" -> \"Are they independent?\" [label=\"yes\"];\n    \"Are they independent?\" -> \"Single agent investigates all\" [label=\"no - related\"];\n    \"Are they independent?\" -> \"Can they work in parallel?\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Parallel dispatch\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Sequential agents\" [label=\"no - shared state\"];\n}\n```\n\n**Use when:**\n- 3+ test files failing with different root causes\n- Multiple subsystems broken independently\n- Each problem can be understood without context from others\n- No shared state between investigations\n\n**Don't use when:**\n- Failures are related (fix one might fix others)\n- Need to understand full system state\n- Agents would interfere with each other\n\n## The Pattern\n\n### 1. Identify Independent Domains\n\nGroup failures by what's broken:\n- File A tests: Tool approval flow\n- File B tests: Batch completion behavior\n- File C tests: Abort functionality\n\nEach domain is independent - fixing tool approval doesn't affect abort tests.\n\n### 2. Create Focused Agent Tasks\n\nEach agent gets:\n- **Specific scope:** One test file or subsystem\n- **Clear goal:** Make these tests pass\n- **Constraints:** Don't change other code\n- **Expected output:** Summary of what you found and fixed\n\n### 3. Dispatch in Parallel\n\n```typescript\n// In Claude Code / AI environment\nTask(\"Fix agent-tool-abort.test.ts failures\")\nTask(\"Fix batch-completion-behavior.test.ts failures\")\nTask(\"Fix tool-approval-race-conditions.test.ts failures\")\n// All three run concurrently\n```\n\n### 4. Review and Integrate\n\nWhen agents return:\n- Read each summary\n- Verify fixes don't conflict\n- Run full test suite\n- Integrate all changes\n\n## Agent Prompt Structure\n\nGood agent prompts are:\n1. **Focused** - One clear problem domain\n2. **Self-contained** - All context needed to understand the problem\n3. **Specific about output** - What should the agent return?\n\n```markdown\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n\n1. \"should abort tool with partial output capture\" - expects 'interrupted at' in message\n2. \"should handle mixed completed and aborted tools\" - fast tool aborted instead of completed\n3. \"should properly track pendingToolCount\" - expects 3 results but gets 0\n\nThese are timing/race condition issues. Your task:\n\n1. Read the test file and understand what each test verifies\n2. Identify root cause - timing issues or actual bugs?\n3. Fix by:\n   - Replacing arbitrary timeouts with event-based waiting\n   - Fixing bugs in abort implementation if found\n   - Adjusting test expectations if testing changed behavior\n\nDo NOT just increase timeouts - find the real issue.\n\nReturn: Summary of what you found and what you fixed.\n```\n\n## Common Mistakes\n\n**❌ Too broad:** \"Fix all the tests\" - agent gets lost\n**✅ Specific:** \"Fix agent-tool-abort.test.ts\" - focused scope\n\n**❌ No context:** \"Fix the race condition\" - agent doesn't know where\n**✅ Context:** Paste the error messages and test names\n\n**❌ No constraints:** Agent might refactor everything\n**✅ Constraints:** \"Do NOT change production code\" or \"Fix tests only\"\n\n**❌ Vague output:** \"Fix it\" - you don't know what changed\n**✅ Specific:** \"Return summary of root cause and changes\"\n\n## When NOT to Use\n\n**Related failures:** Fixing one might fix others - investigate together first\n**Need full context:** Understanding requires seeing entire system\n**Exploratory debugging:** You don't know what's broken yet\n**Shared state:** Agents would interfere (editing same files, using same resources)\n\n## Real Example from Session\n\n**Scenario:** 6 test failures across 3 files after major refactoring\n\n**Failures:**\n- agent-tool-abort.test.ts: 3 failures (timing issues)\n- batch-completion-behavior.test.ts: 2 failures (tools not executing)\n- tool-approval-race-conditions.test.ts: 1 failure (execution count = 0)\n\n**Decision:** Independent domains - abort logic separate from batch completion separate from race conditions\n\n**Dispatch:**\n```\nAgent 1 → Fix agent-tool-abort.test.ts\nAgent 2 → Fix batch-completion-behavior.test.ts\nAgent 3 → Fix tool-approval-race-conditions.test.ts\n```\n\n**Results:**\n- Agent 1: Replaced timeouts with event-based waiting\n- Agent 2: Fixed event structure bug (threadId in wrong place)\n- Agent 3: Added wait for async tool execution to complete\n\n**Integration:** All fixes independent, no conflicts, full suite green\n\n**Time saved:** 3 problems solved in parallel vs sequentially\n\n## Key Benefits\n\n1. **Parallelization** - Multiple investigations happen simultaneously\n2. **Focus** - Each agent has narrow scope, less context to track\n3. **Independence** - Agents don't interfere with each other\n4. **Speed** - 3 problems solved in time of 1\n\n## Verification\n\nAfter agents return:\n1. **Review each summary** - Understand what changed\n2. **Check for conflicts** - Did agents edit same code?\n3. **Run full suite** - Verify all fixes work together\n4. **Spot check** - Agents can make systematic errors\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- 6 failures across 3 files\n- 3 agents dispatched in parallel\n- All investigations completed concurrently\n- All fixes integrated successfully\n- Zero conflicts between agent changes\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:13.086Z"
  },
  {
    "id": "executing-plans",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/executing-plans",
      "url": "https://github.com/obra/superpowers/tree/main/skills/executing-plans"
    },
    "frontmatter": {
      "name": "executing-plans",
      "description": "Use when you have a written implementation plan to execute in a separate session with review checkpoints"
    },
    "content": "# Executing Plans\n\n## Overview\n\nLoad plan, review critically, execute tasks in batches, report for review between batches.\n\n**Core principle:** Batch execution with checkpoints for architect review.\n\n**Announce at start:** \"I'm using the executing-plans skill to implement this plan.\"\n\n## The Process\n\n### Step 1: Load and Review Plan\n1. Read plan file\n2. Review critically - identify any questions or concerns about the plan\n3. If concerns: Raise them with your human partner before starting\n4. If no concerns: Create TodoWrite and proceed\n\n### Step 2: Execute Batch\n**Default: First 3 tasks**\n\nFor each task:\n1. Mark as in_progress\n2. Follow each step exactly (plan has bite-sized steps)\n3. Run verifications as specified\n4. Mark as completed\n\n### Step 3: Report\nWhen batch complete:\n- Show what was implemented\n- Show verification output\n- Say: \"Ready for feedback.\"\n\n### Step 4: Continue\nBased on feedback:\n- Apply changes if needed\n- Execute next batch\n- Repeat until complete\n\n### Step 5: Complete Development\n\nAfter all tasks complete and verified:\n- Announce: \"I'm using the finishing-a-development-branch skill to complete this work.\"\n- **REQUIRED SUB-SKILL:** Use superpowers:finishing-a-development-branch\n- Follow that skill to verify tests, present options, execute choice\n\n## When to Stop and Ask for Help\n\n**STOP executing immediately when:**\n- Hit a blocker mid-batch (missing dependency, test fails, instruction unclear)\n- Plan has critical gaps preventing starting\n- You don't understand an instruction\n- Verification fails repeatedly\n\n**Ask for clarification rather than guessing.**\n\n## When to Revisit Earlier Steps\n\n**Return to Review (Step 1) when:**\n- Partner updates the plan based on your feedback\n- Fundamental approach needs rethinking\n\n**Don't force through blockers** - stop and ask.\n\n## Remember\n- Review plan critically first\n- Follow plan steps exactly\n- Don't skip verifications\n- Reference skills when plan says to\n- Between batches: just report and wait\n- Stop when blocked, don't guess\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:13.365Z"
  },
  {
    "id": "finishing-a-development-branch",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/finishing-a-development-branch",
      "url": "https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch"
    },
    "frontmatter": {
      "name": "finishing-a-development-branch",
      "description": "Use when implementation is complete, all tests pass, and you need to decide how to integrate the work - guides completion of development work by presenting structured options for merge, PR, or cleanup"
    },
    "content": "# Finishing a Development Branch\n\n## Overview\n\nGuide completion of development work by presenting clear options and handling chosen workflow.\n\n**Core principle:** Verify tests → Present options → Execute choice → Clean up.\n\n**Announce at start:** \"I'm using the finishing-a-development-branch skill to complete this work.\"\n\n## The Process\n\n### Step 1: Verify Tests\n\n**Before presenting options, verify tests pass:**\n\n```bash\n# Run project's test suite\nnpm test / cargo test / pytest / go test ./...\n```\n\n**If tests fail:**\n```\nTests failing (<N> failures). Must fix before completing:\n\n[Show failures]\n\nCannot proceed with merge/PR until tests pass.\n```\n\nStop. Don't proceed to Step 2.\n\n**If tests pass:** Continue to Step 2.\n\n### Step 2: Determine Base Branch\n\n```bash\n# Try common base branches\ngit merge-base HEAD main 2>/dev/null || git merge-base HEAD master 2>/dev/null\n```\n\nOr ask: \"This branch split from main - is that correct?\"\n\n### Step 3: Present Options\n\nPresent exactly these 4 options:\n\n```\nImplementation complete. What would you like to do?\n\n1. Merge back to <base-branch> locally\n2. Push and create a Pull Request\n3. Keep the branch as-is (I'll handle it later)\n4. Discard this work\n\nWhich option?\n```\n\n**Don't add explanation** - keep options concise.\n\n### Step 4: Execute Choice\n\n#### Option 1: Merge Locally\n\n```bash\n# Switch to base branch\ngit checkout <base-branch>\n\n# Pull latest\ngit pull\n\n# Merge feature branch\ngit merge <feature-branch>\n\n# Verify tests on merged result\n<test command>\n\n# If tests pass\ngit branch -d <feature-branch>\n```\n\nThen: Cleanup worktree (Step 5)\n\n#### Option 2: Push and Create PR\n\n```bash\n# Push branch\ngit push -u origin <feature-branch>\n\n# Create PR\ngh pr create --title \"<title>\" --body \"$(cat <<'EOF'\n## Summary\n<2-3 bullets of what changed>\n\n## Test Plan\n- [ ] <verification steps>\nEOF\n)\"\n```\n\nThen: Cleanup worktree (Step 5)\n\n#### Option 3: Keep As-Is\n\nReport: \"Keeping branch <name>. Worktree preserved at <path>.\"\n\n**Don't cleanup worktree.**\n\n#### Option 4: Discard\n\n**Confirm first:**\n```\nThis will permanently delete:\n- Branch <name>\n- All commits: <commit-list>\n- Worktree at <path>\n\nType 'discard' to confirm.\n```\n\nWait for exact confirmation.\n\nIf confirmed:\n```bash\ngit checkout <base-branch>\ngit branch -D <feature-branch>\n```\n\nThen: Cleanup worktree (Step 5)\n\n### Step 5: Cleanup Worktree\n\n**For Options 1, 2, 4:**\n\nCheck if in worktree:\n```bash\ngit worktree list | grep $(git branch --show-current)\n```\n\nIf yes:\n```bash\ngit worktree remove <worktree-path>\n```\n\n**For Option 3:** Keep worktree.\n\n## Quick Reference\n\n| Option | Merge | Push | Keep Worktree | Cleanup Branch |\n|--------|-------|------|---------------|----------------|\n| 1. Merge locally | ✓ | - | - | ✓ |\n| 2. Create PR | - | ✓ | ✓ | - |\n| 3. Keep as-is | - | - | ✓ | - |\n| 4. Discard | - | - | - | ✓ (force) |\n\n## Common Mistakes\n\n**Skipping test verification**\n- **Problem:** Merge broken code, create failing PR\n- **Fix:** Always verify tests before offering options\n\n**Open-ended questions**\n- **Problem:** \"What should I do next?\" → ambiguous\n- **Fix:** Present exactly 4 structured options\n\n**Automatic worktree cleanup**\n- **Problem:** Remove worktree when might need it (Option 2, 3)\n- **Fix:** Only cleanup for Options 1 and 4\n\n**No confirmation for discard**\n- **Problem:** Accidentally delete work\n- **Fix:** Require typed \"discard\" confirmation\n\n## Red Flags\n\n**Never:**\n- Proceed with failing tests\n- Merge without verifying tests on result\n- Delete work without confirmation\n- Force-push without explicit request\n\n**Always:**\n- Verify tests before offering options\n- Present exactly 4 options\n- Get typed confirmation for Option 4\n- Clean up worktree for Options 1 & 4 only\n\n## Integration\n\n**Called by:**\n- **subagent-driven-development** (Step 7) - After all tasks complete\n- **executing-plans** (Step 5) - After all batches complete\n\n**Pairs with:**\n- **using-git-worktrees** - Cleans up worktree created by that skill\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:13.620Z"
  },
  {
    "id": "receiving-code-review",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/receiving-code-review",
      "url": "https://github.com/obra/superpowers/tree/main/skills/receiving-code-review"
    },
    "frontmatter": {
      "name": "receiving-code-review",
      "description": "Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation"
    },
    "content": "# Code Review Reception\n\n## Overview\n\nCode review requires technical evaluation, not emotional performance.\n\n**Core principle:** Verify before implementing. Ask before assuming. Technical correctness over social comfort.\n\n## The Response Pattern\n\n```\nWHEN receiving code review feedback:\n\n1. READ: Complete feedback without reacting\n2. UNDERSTAND: Restate requirement in own words (or ask)\n3. VERIFY: Check against codebase reality\n4. EVALUATE: Technically sound for THIS codebase?\n5. RESPOND: Technical acknowledgment or reasoned pushback\n6. IMPLEMENT: One item at a time, test each\n```\n\n## Forbidden Responses\n\n**NEVER:**\n- \"You're absolutely right!\" (explicit CLAUDE.md violation)\n- \"Great point!\" / \"Excellent feedback!\" (performative)\n- \"Let me implement that now\" (before verification)\n\n**INSTEAD:**\n- Restate the technical requirement\n- Ask clarifying questions\n- Push back with technical reasoning if wrong\n- Just start working (actions > words)\n\n## Handling Unclear Feedback\n\n```\nIF any item is unclear:\n  STOP - do not implement anything yet\n  ASK for clarification on unclear items\n\nWHY: Items may be related. Partial understanding = wrong implementation.\n```\n\n**Example:**\n```\nyour human partner: \"Fix 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n\n❌ WRONG: Implement 1,2,3,6 now, ask about 4,5 later\n✅ RIGHT: \"I understand items 1,2,3,6. Need clarification on 4 and 5 before proceeding.\"\n```\n\n## Source-Specific Handling\n\n### From your human partner\n- **Trusted** - implement after understanding\n- **Still ask** if scope unclear\n- **No performative agreement**\n- **Skip to action** or technical acknowledgment\n\n### From External Reviewers\n```\nBEFORE implementing:\n  1. Check: Technically correct for THIS codebase?\n  2. Check: Breaks existing functionality?\n  3. Check: Reason for current implementation?\n  4. Check: Works on all platforms/versions?\n  5. Check: Does reviewer understand full context?\n\nIF suggestion seems wrong:\n  Push back with technical reasoning\n\nIF can't easily verify:\n  Say so: \"I can't verify this without [X]. Should I [investigate/ask/proceed]?\"\n\nIF conflicts with your human partner's prior decisions:\n  Stop and discuss with your human partner first\n```\n\n**your human partner's rule:** \"External feedback - be skeptical, but check carefully\"\n\n## YAGNI Check for \"Professional\" Features\n\n```\nIF reviewer suggests \"implementing properly\":\n  grep codebase for actual usage\n\n  IF unused: \"This endpoint isn't called. Remove it (YAGNI)?\"\n  IF used: Then implement properly\n```\n\n**your human partner's rule:** \"You and reviewer both report to me. If we don't need this feature, don't add it.\"\n\n## Implementation Order\n\n```\nFOR multi-item feedback:\n  1. Clarify anything unclear FIRST\n  2. Then implement in this order:\n     - Blocking issues (breaks, security)\n     - Simple fixes (typos, imports)\n     - Complex fixes (refactoring, logic)\n  3. Test each fix individually\n  4. Verify no regressions\n```\n\n## When To Push Back\n\nPush back when:\n- Suggestion breaks existing functionality\n- Reviewer lacks full context\n- Violates YAGNI (unused feature)\n- Technically incorrect for this stack\n- Legacy/compatibility reasons exist\n- Conflicts with your human partner's architectural decisions\n\n**How to push back:**\n- Use technical reasoning, not defensiveness\n- Ask specific questions\n- Reference working tests/code\n- Involve your human partner if architectural\n\n**Signal if uncomfortable pushing back out loud:** \"Strange things are afoot at the Circle K\"\n\n## Acknowledging Correct Feedback\n\nWhen feedback IS correct:\n```\n✅ \"Fixed. [Brief description of what changed]\"\n✅ \"Good catch - [specific issue]. Fixed in [location].\"\n✅ [Just fix it and show in the code]\n\n❌ \"You're absolutely right!\"\n❌ \"Great point!\"\n❌ \"Thanks for catching that!\"\n❌ \"Thanks for [anything]\"\n❌ ANY gratitude expression\n```\n\n**Why no thanks:** Actions speak. Just fix it. The code itself shows you heard the feedback.\n\n**If you catch yourself about to write \"Thanks\":** DELETE IT. State the fix instead.\n\n## Gracefully Correcting Your Pushback\n\nIf you pushed back and were wrong:\n```\n✅ \"You were right - I checked [X] and it does [Y]. Implementing now.\"\n✅ \"Verified this and you're correct. My initial understanding was wrong because [reason]. Fixing.\"\n\n❌ Long apology\n❌ Defending why you pushed back\n❌ Over-explaining\n```\n\nState the correction factually and move on.\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Performative agreement | State requirement or just act |\n| Blind implementation | Verify against codebase first |\n| Batch without testing | One at a time, test each |\n| Assuming reviewer is right | Check if breaks things |\n| Avoiding pushback | Technical correctness > comfort |\n| Partial implementation | Clarify all items first |\n| Can't verify, proceed anyway | State limitation, ask for direction |\n\n## Real Examples\n\n**Performative Agreement (Bad):**\n```\nReviewer: \"Remove legacy code\"\n❌ \"You're absolutely right! Let me remove that...\"\n```\n\n**Technical Verification (Good):**\n```\nReviewer: \"Remove legacy code\"\n✅ \"Checking... build target is 10.15+, this API needs 13+. Need legacy for backward compat. Current impl has wrong bundle ID - fix it or drop pre-13 support?\"\n```\n\n**YAGNI (Good):**\n```\nReviewer: \"Implement proper metrics tracking with database, date filters, CSV export\"\n✅ \"Grepped codebase - nothing calls this endpoint. Remove it (YAGNI)? Or is there usage I'm missing?\"\n```\n\n**Unclear Item (Good):**\n```\nyour human partner: \"Fix items 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n✅ \"Understand 1,2,3,6. Need clarification on 4 and 5 before implementing.\"\n```\n\n## GitHub Thread Replies\n\nWhen replying to inline review comments on GitHub, reply in the comment thread (`gh api repos/{owner}/{repo}/pulls/{pr}/comments/{id}/replies`), not as a top-level PR comment.\n\n## The Bottom Line\n\n**External feedback = suggestions to evaluate, not orders to follow.**\n\nVerify. Question. Then implement.\n\nNo performative agreement. Technical rigor always.\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:13.863Z"
  },
  {
    "id": "requesting-code-review",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/requesting-code-review",
      "url": "https://github.com/obra/superpowers/tree/main/skills/requesting-code-review"
    },
    "frontmatter": {
      "name": "requesting-code-review",
      "description": "Use when completing tasks, implementing major features, or before merging to verify work meets requirements"
    },
    "content": "# Requesting Code Review\n\nDispatch superpowers:code-reviewer subagent to catch issues before they cascade.\n\n**Core principle:** Review early, review often.\n\n## When to Request Review\n\n**Mandatory:**\n- After each task in subagent-driven development\n- After completing major feature\n- Before merge to main\n\n**Optional but valuable:**\n- When stuck (fresh perspective)\n- Before refactoring (baseline check)\n- After fixing complex bug\n\n## How to Request\n\n**1. Get git SHAs:**\n```bash\nBASE_SHA=$(git rev-parse HEAD~1)  # or origin/main\nHEAD_SHA=$(git rev-parse HEAD)\n```\n\n**2. Dispatch code-reviewer subagent:**\n\nUse Task tool with superpowers:code-reviewer type, fill template at `code-reviewer.md`\n\n**Placeholders:**\n- `{WHAT_WAS_IMPLEMENTED}` - What you just built\n- `{PLAN_OR_REQUIREMENTS}` - What it should do\n- `{BASE_SHA}` - Starting commit\n- `{HEAD_SHA}` - Ending commit\n- `{DESCRIPTION}` - Brief summary\n\n**3. Act on feedback:**\n- Fix Critical issues immediately\n- Fix Important issues before proceeding\n- Note Minor issues for later\n- Push back if reviewer is wrong (with reasoning)\n\n## Example\n\n```\n[Just completed Task 2: Add verification function]\n\nYou: Let me request code review before proceeding.\n\nBASE_SHA=$(git log --oneline | grep \"Task 1\" | head -1 | awk '{print $1}')\nHEAD_SHA=$(git rev-parse HEAD)\n\n[Dispatch superpowers:code-reviewer subagent]\n  WHAT_WAS_IMPLEMENTED: Verification and repair functions for conversation index\n  PLAN_OR_REQUIREMENTS: Task 2 from docs/plans/deployment-plan.md\n  BASE_SHA: a7981ec\n  HEAD_SHA: 3df7661\n  DESCRIPTION: Added verifyIndex() and repairIndex() with 4 issue types\n\n[Subagent returns]:\n  Strengths: Clean architecture, real tests\n  Issues:\n    Important: Missing progress indicators\n    Minor: Magic number (100) for reporting interval\n  Assessment: Ready to proceed\n\nYou: [Fix progress indicators]\n[Continue to Task 3]\n```\n\n## Integration with Workflows\n\n**Subagent-Driven Development:**\n- Review after EACH task\n- Catch issues before they compound\n- Fix before moving to next task\n\n**Executing Plans:**\n- Review after each batch (3 tasks)\n- Get feedback, apply, continue\n\n**Ad-Hoc Development:**\n- Review before merge\n- Review when stuck\n\n## Red Flags\n\n**Never:**\n- Skip review because \"it's simple\"\n- Ignore Critical issues\n- Proceed with unfixed Important issues\n- Argue with valid technical feedback\n\n**If reviewer wrong:**\n- Push back with technical reasoning\n- Show code/tests that prove it works\n- Request clarification\n\nSee template at: requesting-code-review/code-reviewer.md\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:14.114Z"
  },
  {
    "id": "subagent-driven-development",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/subagent-driven-development",
      "url": "https://github.com/obra/superpowers/tree/main/skills/subagent-driven-development"
    },
    "frontmatter": {
      "name": "subagent-driven-development",
      "description": "Use when executing implementation plans with independent tasks in the current session"
    },
    "content": "# Subagent-Driven Development\n\nExecute plan by dispatching fresh subagent per task, with two-stage review after each: spec compliance review first, then code quality review.\n\n**Core principle:** Fresh subagent per task + two-stage review (spec then quality) = high quality, fast iteration\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Have implementation plan?\" [shape=diamond];\n    \"Tasks mostly independent?\" [shape=diamond];\n    \"Stay in this session?\" [shape=diamond];\n    \"subagent-driven-development\" [shape=box];\n    \"executing-plans\" [shape=box];\n    \"Manual execution or brainstorm first\" [shape=box];\n\n    \"Have implementation plan?\" -> \"Tasks mostly independent?\" [label=\"yes\"];\n    \"Have implementation plan?\" -> \"Manual execution or brainstorm first\" [label=\"no\"];\n    \"Tasks mostly independent?\" -> \"Stay in this session?\" [label=\"yes\"];\n    \"Tasks mostly independent?\" -> \"Manual execution or brainstorm first\" [label=\"no - tightly coupled\"];\n    \"Stay in this session?\" -> \"subagent-driven-development\" [label=\"yes\"];\n    \"Stay in this session?\" -> \"executing-plans\" [label=\"no - parallel session\"];\n}\n```\n\n**vs. Executing Plans (parallel session):**\n- Same session (no context switch)\n- Fresh subagent per task (no context pollution)\n- Two-stage review after each task: spec compliance first, then code quality\n- Faster iteration (no human-in-loop between tasks)\n\n## The Process\n\n```dot\ndigraph process {\n    rankdir=TB;\n\n    subgraph cluster_per_task {\n        label=\"Per Task\";\n        \"Dispatch implementer subagent (./implementer-prompt.md)\" [shape=box];\n        \"Implementer subagent asks questions?\" [shape=diamond];\n        \"Answer questions, provide context\" [shape=box];\n        \"Implementer subagent implements, tests, commits, self-reviews\" [shape=box];\n        \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" [shape=box];\n        \"Spec reviewer subagent confirms code matches spec?\" [shape=diamond];\n        \"Implementer subagent fixes spec gaps\" [shape=box];\n        \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [shape=box];\n        \"Code quality reviewer subagent approves?\" [shape=diamond];\n        \"Implementer subagent fixes quality issues\" [shape=box];\n        \"Mark task complete in TodoWrite\" [shape=box];\n    }\n\n    \"Read plan, extract all tasks with full text, note context, create TodoWrite\" [shape=box];\n    \"More tasks remain?\" [shape=diamond];\n    \"Dispatch final code reviewer subagent for entire implementation\" [shape=box];\n    \"Use superpowers:finishing-a-development-branch\" [shape=box style=filled fillcolor=lightgreen];\n\n    \"Read plan, extract all tasks with full text, note context, create TodoWrite\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\";\n    \"Dispatch implementer subagent (./implementer-prompt.md)\" -> \"Implementer subagent asks questions?\";\n    \"Implementer subagent asks questions?\" -> \"Answer questions, provide context\" [label=\"yes\"];\n    \"Answer questions, provide context\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\";\n    \"Implementer subagent asks questions?\" -> \"Implementer subagent implements, tests, commits, self-reviews\" [label=\"no\"];\n    \"Implementer subagent implements, tests, commits, self-reviews\" -> \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\";\n    \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" -> \"Spec reviewer subagent confirms code matches spec?\";\n    \"Spec reviewer subagent confirms code matches spec?\" -> \"Implementer subagent fixes spec gaps\" [label=\"no\"];\n    \"Implementer subagent fixes spec gaps\" -> \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" [label=\"re-review\"];\n    \"Spec reviewer subagent confirms code matches spec?\" -> \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [label=\"yes\"];\n    \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" -> \"Code quality reviewer subagent approves?\";\n    \"Code quality reviewer subagent approves?\" -> \"Implementer subagent fixes quality issues\" [label=\"no\"];\n    \"Implementer subagent fixes quality issues\" -> \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [label=\"re-review\"];\n    \"Code quality reviewer subagent approves?\" -> \"Mark task complete in TodoWrite\" [label=\"yes\"];\n    \"Mark task complete in TodoWrite\" -> \"More tasks remain?\";\n    \"More tasks remain?\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\" [label=\"yes\"];\n    \"More tasks remain?\" -> \"Dispatch final code reviewer subagent for entire implementation\" [label=\"no\"];\n    \"Dispatch final code reviewer subagent for entire implementation\" -> \"Use superpowers:finishing-a-development-branch\";\n}\n```\n\n## Prompt Templates\n\n- `./implementer-prompt.md` - Dispatch implementer subagent\n- `./spec-reviewer-prompt.md` - Dispatch spec compliance reviewer subagent\n- `./code-quality-reviewer-prompt.md` - Dispatch code quality reviewer subagent\n\n## Example Workflow\n\n```\nYou: I'm using Subagent-Driven Development to execute this plan.\n\n[Read plan file once: docs/plans/feature-plan.md]\n[Extract all 5 tasks with full text and context]\n[Create TodoWrite with all tasks]\n\nTask 1: Hook installation script\n\n[Get Task 1 text and context (already extracted)]\n[Dispatch implementation subagent with full task text + context]\n\nImplementer: \"Before I begin - should the hook be installed at user or system level?\"\n\nYou: \"User level (~/.config/superpowers/hooks/)\"\n\nImplementer: \"Got it. Implementing now...\"\n[Later] Implementer:\n  - Implemented install-hook command\n  - Added tests, 5/5 passing\n  - Self-review: Found I missed --force flag, added it\n  - Committed\n\n[Dispatch spec compliance reviewer]\nSpec reviewer: ✅ Spec compliant - all requirements met, nothing extra\n\n[Get git SHAs, dispatch code quality reviewer]\nCode reviewer: Strengths: Good test coverage, clean. Issues: None. Approved.\n\n[Mark Task 1 complete]\n\nTask 2: Recovery modes\n\n[Get Task 2 text and context (already extracted)]\n[Dispatch implementation subagent with full task text + context]\n\nImplementer: [No questions, proceeds]\nImplementer:\n  - Added verify/repair modes\n  - 8/8 tests passing\n  - Self-review: All good\n  - Committed\n\n[Dispatch spec compliance reviewer]\nSpec reviewer: ❌ Issues:\n  - Missing: Progress reporting (spec says \"report every 100 items\")\n  - Extra: Added --json flag (not requested)\n\n[Implementer fixes issues]\nImplementer: Removed --json flag, added progress reporting\n\n[Spec reviewer reviews again]\nSpec reviewer: ✅ Spec compliant now\n\n[Dispatch code quality reviewer]\nCode reviewer: Strengths: Solid. Issues (Important): Magic number (100)\n\n[Implementer fixes]\nImplementer: Extracted PROGRESS_INTERVAL constant\n\n[Code reviewer reviews again]\nCode reviewer: ✅ Approved\n\n[Mark Task 2 complete]\n\n...\n\n[After all tasks]\n[Dispatch final code-reviewer]\nFinal reviewer: All requirements met, ready to merge\n\nDone!\n```\n\n## Advantages\n\n**vs. Manual execution:**\n- Subagents follow TDD naturally\n- Fresh context per task (no confusion)\n- Parallel-safe (subagents don't interfere)\n- Subagent can ask questions (before AND during work)\n\n**vs. Executing Plans:**\n- Same session (no handoff)\n- Continuous progress (no waiting)\n- Review checkpoints automatic\n\n**Efficiency gains:**\n- No file reading overhead (controller provides full text)\n- Controller curates exactly what context is needed\n- Subagent gets complete information upfront\n- Questions surfaced before work begins (not after)\n\n**Quality gates:**\n- Self-review catches issues before handoff\n- Two-stage review: spec compliance, then code quality\n- Review loops ensure fixes actually work\n- Spec compliance prevents over/under-building\n- Code quality ensures implementation is well-built\n\n**Cost:**\n- More subagent invocations (implementer + 2 reviewers per task)\n- Controller does more prep work (extracting all tasks upfront)\n- Review loops add iterations\n- But catches issues early (cheaper than debugging later)\n\n## Red Flags\n\n**Never:**\n- Skip reviews (spec compliance OR code quality)\n- Proceed with unfixed issues\n- Dispatch multiple implementation subagents in parallel (conflicts)\n- Make subagent read plan file (provide full text instead)\n- Skip scene-setting context (subagent needs to understand where task fits)\n- Ignore subagent questions (answer before letting them proceed)\n- Accept \"close enough\" on spec compliance (spec reviewer found issues = not done)\n- Skip review loops (reviewer found issues = implementer fixes = review again)\n- Let implementer self-review replace actual review (both are needed)\n- **Start code quality review before spec compliance is ✅** (wrong order)\n- Move to next task while either review has open issues\n\n**If subagent asks questions:**\n- Answer clearly and completely\n- Provide additional context if needed\n- Don't rush them into implementation\n\n**If reviewer finds issues:**\n- Implementer (same subagent) fixes them\n- Reviewer reviews again\n- Repeat until approved\n- Don't skip the re-review\n\n**If subagent fails task:**\n- Dispatch fix subagent with specific instructions\n- Don't try to fix manually (context pollution)\n\n## Integration\n\n**Required workflow skills:**\n- **superpowers:writing-plans** - Creates the plan this skill executes\n- **superpowers:requesting-code-review** - Code review template for reviewer subagents\n- **superpowers:finishing-a-development-branch** - Complete development after all tasks\n\n**Subagents should use:**\n- **superpowers:test-driven-development** - Subagents follow TDD for each task\n\n**Alternative workflow:**\n- **superpowers:executing-plans** - Use for parallel session instead of same-session execution\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:14.358Z"
  },
  {
    "id": "systematic-debugging",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/systematic-debugging",
      "url": "https://github.com/obra/superpowers/tree/main/skills/systematic-debugging"
    },
    "frontmatter": {
      "name": "systematic-debugging",
      "description": "Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes"
    },
    "content": "# Systematic Debugging\n\n## Overview\n\nRandom fixes waste time and create new bugs. Quick patches mask underlying issues.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n**Violating the letter of this process is violating the spirit of debugging.**\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n- Integration issues\n\n**Use this ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- Previous fix didn't work\n- You don't fully understand the issue\n\n**Don't skip when:**\n- Issue seems simple (simple bugs have root causes too)\n- You're in a hurry (rushing guarantees rework)\n- Manager wants it fixed NOW (systematic is faster than thrashing)\n\n## The Four Phases\n\nYou MUST complete each phase before proceeding to the next.\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - They often contain the exact solution\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - Does it happen every time?\n   - If not reproducible → gather more data, don't guess\n\n3. **Check Recent Changes**\n   - What changed that could cause this?\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Gather Evidence in Multi-Component Systems**\n\n   **WHEN system has multiple components (CI → build → signing, API → service → database):**\n\n   **BEFORE proposing fixes, add diagnostic instrumentation:**\n   ```\n   For EACH component boundary:\n     - Log what data enters component\n     - Log what data exits component\n     - Verify environment/config propagation\n     - Check state at each layer\n\n   Run once to gather evidence showing WHERE it breaks\n   THEN analyze evidence to identify failing component\n   THEN investigate that specific component\n   ```\n\n   **Example (multi-layer system):**\n   ```bash\n   # Layer 1: Workflow\n   echo \"=== Secrets available in workflow: ===\"\n   echo \"IDENTITY: ${IDENTITY:+SET}${IDENTITY:-UNSET}\"\n\n   # Layer 2: Build script\n   echo \"=== Env vars in build script: ===\"\n   env | grep IDENTITY || echo \"IDENTITY not in environment\"\n\n   # Layer 3: Signing script\n   echo \"=== Keychain state: ===\"\n   security list-keychains\n   security find-identity -v\n\n   # Layer 4: Actual signing\n   codesign --sign \"$IDENTITY\" --verbose=4 \"$APP\"\n   ```\n\n   **This reveals:** Which layer fails (secrets → workflow ✓, workflow → build ✗)\n\n5. **Trace Data Flow**\n\n   **WHEN error is deep in call stack:**\n\n   See `root-cause-tracing.md` in this directory for the complete backward tracing technique.\n\n   **Quick version:**\n   - Where does bad value originate?\n   - What called this with bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n### Phase 2: Pattern Analysis\n\n**Find the pattern before fixing:**\n\n1. **Find Working Examples**\n   - Locate similar working code in same codebase\n   - What works that's similar to what's broken?\n\n2. **Compare Against References**\n   - If implementing pattern, read reference implementation COMPLETELY\n   - Don't skim - read every line\n   - Understand the pattern fully before applying\n\n3. **Identify Differences**\n   - What's different between working and broken?\n   - List every difference, however small\n   - Don't assume \"that can't matter\"\n\n4. **Understand Dependencies**\n   - What other components does this need?\n   - What settings, config, environment?\n   - What assumptions does it make?\n\n### Phase 3: Hypothesis and Testing\n\n**Scientific method:**\n\n1. **Form Single Hypothesis**\n   - State clearly: \"I think X is the root cause because Y\"\n   - Write it down\n   - Be specific, not vague\n\n2. **Test Minimally**\n   - Make the SMALLEST possible change to test hypothesis\n   - One variable at a time\n   - Don't fix multiple things at once\n\n3. **Verify Before Continuing**\n   - Did it work? Yes → Phase 4\n   - Didn't work? Form NEW hypothesis\n   - DON'T add more fixes on top\n\n4. **When You Don't Know**\n   - Say \"I don't understand X\"\n   - Don't pretend to know\n   - Ask for help\n   - Research more\n\n### Phase 4: Implementation\n\n**Fix the root cause, not the symptom:**\n\n1. **Create Failing Test Case**\n   - Simplest possible reproduction\n   - Automated test if possible\n   - One-off test script if no framework\n   - MUST have before fixing\n   - Use the `superpowers:test-driven-development` skill for writing proper failing tests\n\n2. **Implement Single Fix**\n   - Address the root cause identified\n   - ONE change at a time\n   - No \"while I'm here\" improvements\n   - No bundled refactoring\n\n3. **Verify Fix**\n   - Test passes now?\n   - No other tests broken?\n   - Issue actually resolved?\n\n4. **If Fix Doesn't Work**\n   - STOP\n   - Count: How many fixes have you tried?\n   - If < 3: Return to Phase 1, re-analyze with new information\n   - **If ≥ 3: STOP and question the architecture (step 5 below)**\n   - DON'T attempt Fix #4 without architectural discussion\n\n5. **If 3+ Fixes Failed: Question Architecture**\n\n   **Pattern indicating architectural problem:**\n   - Each fix reveals new shared state/coupling/problem in different place\n   - Fixes require \"massive refactoring\" to implement\n   - Each fix creates new symptoms elsewhere\n\n   **STOP and question fundamentals:**\n   - Is this pattern fundamentally sound?\n   - Are we \"sticking with it through sheer inertia\"?\n   - Should we refactor architecture vs. continue fixing symptoms?\n\n   **Discuss with your human partner before attempting more fixes**\n\n   This is NOT a failed hypothesis - this is a wrong architecture.\n\n## Red Flags - STOP and Follow Process\n\nIf you catch yourself thinking:\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"Add multiple changes, run tests\"\n- \"Skip the test, I'll manually verify\"\n- \"It's probably X, let me fix that\"\n- \"I don't fully understand but this might work\"\n- \"Pattern says X but I'll adapt it differently\"\n- \"Here are the main problems: [lists fixes without investigation]\"\n- Proposing solutions before tracing data flow\n- **\"One more fix attempt\" (when already tried 2+)**\n- **Each fix reveals new problem in different place**\n\n**ALL of these mean: STOP. Return to Phase 1.**\n\n**If 3+ fixes failed:** Question the architecture (see Phase 4.5)\n\n## your human partner's Signals You're Doing It Wrong\n\n**Watch for these redirections:**\n- \"Is that not happening?\" - You assumed without verifying\n- \"Will it show us...?\" - You should have added evidence gathering\n- \"Stop guessing\" - You're proposing fixes without understanding\n- \"Ultrathink this\" - Question fundamentals, not just symptoms\n- \"We're stuck?\" (frustrated) - Your approach isn't working\n\n**When you see these:** STOP. Return to Phase 1.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple, don't need process\" | Simple issues have root causes too. Process is fast for simple bugs. |\n| \"Emergency, no time for process\" | Systematic debugging is FASTER than guess-and-check thrashing. |\n| \"Just try this first, then investigate\" | First fix sets the pattern. Do it right from the start. |\n| \"I'll write test after confirming fix works\" | Untested fixes don't stick. Test first proves it. |\n| \"Multiple fixes at once saves time\" | Can't isolate what worked. Causes new bugs. |\n| \"Reference too long, I'll adapt the pattern\" | Partial understanding guarantees bugs. Read it completely. |\n| \"I see the problem, let me fix it\" | Seeing symptoms ≠ understanding root cause. |\n| \"One more fix attempt\" (after 2+ failures) | 3+ failures = architectural problem. Question pattern, don't fix again. |\n\n## Quick Reference\n\n| Phase | Key Activities | Success Criteria |\n|-------|---------------|------------------|\n| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |\n| **2. Pattern** | Find working examples, compare | Identify differences |\n| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |\n| **4. Implementation** | Create test, fix, verify | Bug resolved, tests pass |\n\n## When Process Reveals \"No Root Cause\"\n\nIf systematic investigation reveals issue is truly environmental, timing-dependent, or external:\n\n1. You've completed the process\n2. Document what you investigated\n3. Implement appropriate handling (retry, timeout, error message)\n4. Add monitoring/logging for future investigation\n\n**But:** 95% of \"no root cause\" cases are incomplete investigation.\n\n## Supporting Techniques\n\nThese techniques are part of systematic debugging and available in this directory:\n\n- **`root-cause-tracing.md`** - Trace bugs backward through call stack to find original trigger\n- **`defense-in-depth.md`** - Add validation at multiple layers after finding root cause\n- **`condition-based-waiting.md`** - Replace arbitrary timeouts with condition polling\n\n**Related skills:**\n- **superpowers:test-driven-development** - For creating failing test case (Phase 4, Step 1)\n- **superpowers:verification-before-completion** - Verify fix worked before claiming success\n\n## Real-World Impact\n\nFrom debugging sessions:\n- Systematic approach: 15-30 minutes to fix\n- Random fixes approach: 2-3 hours of thrashing\n- First-time fix rate: 95% vs 40%\n- New bugs introduced: Near zero vs common\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:14.604Z"
  },
  {
    "id": "test-driven-development",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/test-driven-development",
      "url": "https://github.com/obra/superpowers/tree/main/skills/test-driven-development"
    },
    "frontmatter": {
      "name": "test-driven-development",
      "description": "Use when implementing any feature or bugfix, before writing implementation code"
    },
    "content": "# Test-Driven Development (TDD)\n\n## Overview\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n**Violating the letter of the rules is violating the spirit of the rules.**\n\n## When to Use\n\n**Always:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions (ask your human partner):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor\n\n```dot\ndigraph tdd_cycle {\n    rankdir=LR;\n    red [label=\"RED\\nWrite failing test\", shape=box, style=filled, fillcolor=\"#ffcccc\"];\n    verify_red [label=\"Verify fails\\ncorrectly\", shape=diamond];\n    green [label=\"GREEN\\nMinimal code\", shape=box, style=filled, fillcolor=\"#ccffcc\"];\n    verify_green [label=\"Verify passes\\nAll green\", shape=diamond];\n    refactor [label=\"REFACTOR\\nClean up\", shape=box, style=filled, fillcolor=\"#ccccff\"];\n    next [label=\"Next\", shape=ellipse];\n\n    red -> verify_red;\n    verify_red -> green [label=\"yes\"];\n    verify_red -> red [label=\"wrong\\nfailure\"];\n    green -> verify_green;\n    verify_green -> refactor [label=\"yes\"];\n    verify_green -> green [label=\"no\"];\n    refactor -> verify_green [label=\"stay\\ngreen\"];\n    verify_green -> next;\n    next -> red;\n}\n```\n\n### RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n<Good>\n```typescript\ntest('retries failed operations 3 times', async () => {\n  let attempts = 0;\n  const operation = () => {\n    attempts++;\n    if (attempts < 3) throw new Error('fail');\n    return 'success';\n  };\n\n  const result = await retryOperation(operation);\n\n  expect(result).toBe('success');\n  expect(attempts).toBe(3);\n});\n```\nClear name, tests real behavior, one thing\n</Good>\n\n<Bad>\n```typescript\ntest('retry works', async () => {\n  const mock = jest.fn()\n    .mockRejectedValueOnce(new Error())\n    .mockRejectedValueOnce(new Error())\n    .mockResolvedValueOnce('success');\n  await retryOperation(mock);\n  expect(mock).toHaveBeenCalledTimes(3);\n});\n```\nVague name, tests mock not code\n</Bad>\n\n**Requirements:**\n- One behavior\n- Clear name\n- Real code (no mocks unless unavoidable)\n\n### Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test fails (not errors)\n- Failure message is expected\n- Fails because feature missing (not typos)\n\n**Test passes?** You're testing existing behavior. Fix test.\n\n**Test errors?** Fix error, re-run until it fails correctly.\n\n### GREEN - Minimal Code\n\nWrite simplest code to pass the test.\n\n<Good>\n```typescript\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\n  for (let i = 0; i < 3; i++) {\n    try {\n      return await fn();\n    } catch (e) {\n      if (i === 2) throw e;\n    }\n  }\n  throw new Error('unreachable');\n}\n```\nJust enough to pass\n</Good>\n\n<Bad>\n```typescript\nasync function retryOperation<T>(\n  fn: () => Promise<T>,\n  options?: {\n    maxRetries?: number;\n    backoff?: 'linear' | 'exponential';\n    onRetry?: (attempt: number) => void;\n  }\n): Promise<T> {\n  // YAGNI\n}\n```\nOver-engineered\n</Bad>\n\nDon't add features, refactor other code, or \"improve\" beyond the test.\n\n### Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test passes\n- Other tests still pass\n- Output pristine (no errors, warnings)\n\n**Test fails?** Fix code, not test.\n\n**Other tests fail?** Fix now.\n\n### REFACTOR - Clean Up\n\nAfter green only:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### Repeat\n\nNext failing test for next feature.\n\n## Good Tests\n\n| Quality | Good | Bad |\n|---------|------|-----|\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\n| **Clear** | Name describes behavior | `test('test1')` |\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |\n\n## Why Order Matters\n\n**\"I'll write tests after to verify it works\"**\n\nTests written after code pass immediately. Passing immediately proves nothing:\n- Might test wrong thing\n- Might test implementation, not behavior\n- Might miss edge cases you forgot\n- You never saw it catch the bug\n\nTest-first forces you to see the test fail, proving it actually tests something.\n\n**\"I already manually tested all the edge cases\"**\n\nManual testing is ad-hoc. You think you tested everything but:\n- No record of what you tested\n- Can't re-run when code changes\n- Easy to forget cases under pressure\n- \"It worked when I tried it\" ≠ comprehensive\n\nAutomated tests are systematic. They run the same way every time.\n\n**\"Deleting X hours of work is wasteful\"**\n\nSunk cost fallacy. The time is already gone. Your choice now:\n- Delete and rewrite with TDD (X more hours, high confidence)\n- Keep it and add tests after (30 min, low confidence, likely bugs)\n\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\n\n**\"TDD is dogmatic, being pragmatic means adapting\"**\n\nTDD IS pragmatic:\n- Finds bugs before commit (faster than debugging after)\n- Prevents regressions (tests catch breaks immediately)\n- Documents behavior (tests show how to use code)\n- Enables refactoring (change freely, tests catch breaks)\n\n\"Pragmatic\" shortcuts = debugging in production = slower.\n\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\n\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\n\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\n\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\n\n30 minutes of tests after ≠ TDD. You get coverage, lose proof tests work.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n| \"Already manually tested\" | Ad-hoc ≠ systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |\n\n## Red Flags - STOP and Start Over\n\n- Code before test\n- Test after implementation\n- Test passes immediately\n- Can't explain why test failed\n- Tests added \"later\"\n- Rationalizing \"just this once\"\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"Keep as reference\" or \"adapt existing code\"\n- \"Already spent X hours, deleting is wasteful\"\n- \"TDD is dogmatic, I'm being pragmatic\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n\n## Example: Bug Fix\n\n**Bug:** Empty email accepted\n\n**RED**\n```typescript\ntest('rejects empty email', async () => {\n  const result = await submitForm({ email: '' });\n  expect(result.error).toBe('Email required');\n});\n```\n\n**Verify RED**\n```bash\n$ npm test\nFAIL: expected 'Email required', got undefined\n```\n\n**GREEN**\n```typescript\nfunction submitForm(data: FormData) {\n  if (!data.email?.trim()) {\n    return { error: 'Email required' };\n  }\n  // ...\n}\n```\n\n**Verify GREEN**\n```bash\n$ npm test\nPASS\n```\n\n**REFACTOR**\nExtract validation for multiple fields if needed.\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Output pristine (no errors, warnings)\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## When Stuck\n\n| Problem | Solution |\n|---------|----------|\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\n| Test too complicated | Design too complicated. Simplify interface. |\n| Must mock everything | Code too coupled. Use dependency injection. |\n| Test setup huge | Extract helpers. Still complex? Simplify design. |\n\n## Debugging Integration\n\nBug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\n\nNever fix bugs without a test.\n\n## Testing Anti-Patterns\n\nWhen adding mocks or test utilities, read @testing-anti-patterns.md to avoid common pitfalls:\n- Testing mock behavior instead of real behavior\n- Adding test-only methods to production classes\n- Mocking without understanding dependencies\n\n## Final Rule\n\n```\nProduction code → test exists and failed first\nOtherwise → not TDD\n```\n\nNo exceptions without your human partner's permission.\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:14.846Z"
  },
  {
    "id": "using-git-worktrees",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/using-git-worktrees",
      "url": "https://github.com/obra/superpowers/tree/main/skills/using-git-worktrees"
    },
    "frontmatter": {
      "name": "using-git-worktrees",
      "description": "Use when starting feature work that needs isolation from current workspace or before executing implementation plans - creates isolated git worktrees with smart directory selection and safety verification"
    },
    "content": "# Using Git Worktrees\n\n## Overview\n\nGit worktrees create isolated workspaces sharing the same repository, allowing work on multiple branches simultaneously without switching.\n\n**Core principle:** Systematic directory selection + safety verification = reliable isolation.\n\n**Announce at start:** \"I'm using the using-git-worktrees skill to set up an isolated workspace.\"\n\n## Directory Selection Process\n\nFollow this priority order:\n\n### 1. Check Existing Directories\n\n```bash\n# Check in priority order\nls -d .worktrees 2>/dev/null     # Preferred (hidden)\nls -d worktrees 2>/dev/null      # Alternative\n```\n\n**If found:** Use that directory. If both exist, `.worktrees` wins.\n\n### 2. Check CLAUDE.md\n\n```bash\ngrep -i \"worktree.*director\" CLAUDE.md 2>/dev/null\n```\n\n**If preference specified:** Use it without asking.\n\n### 3. Ask User\n\nIf no directory exists and no CLAUDE.md preference:\n\n```\nNo worktree directory found. Where should I create worktrees?\n\n1. .worktrees/ (project-local, hidden)\n2. ~/.config/superpowers/worktrees/<project-name>/ (global location)\n\nWhich would you prefer?\n```\n\n## Safety Verification\n\n### For Project-Local Directories (.worktrees or worktrees)\n\n**MUST verify directory is ignored before creating worktree:**\n\n```bash\n# Check if directory is ignored (respects local, global, and system gitignore)\ngit check-ignore -q .worktrees 2>/dev/null || git check-ignore -q worktrees 2>/dev/null\n```\n\n**If NOT ignored:**\n\nPer Jesse's rule \"Fix broken things immediately\":\n1. Add appropriate line to .gitignore\n2. Commit the change\n3. Proceed with worktree creation\n\n**Why critical:** Prevents accidentally committing worktree contents to repository.\n\n### For Global Directory (~/.config/superpowers/worktrees)\n\nNo .gitignore verification needed - outside project entirely.\n\n## Creation Steps\n\n### 1. Detect Project Name\n\n```bash\nproject=$(basename \"$(git rev-parse --show-toplevel)\")\n```\n\n### 2. Create Worktree\n\n```bash\n# Determine full path\ncase $LOCATION in\n  .worktrees|worktrees)\n    path=\"$LOCATION/$BRANCH_NAME\"\n    ;;\n  ~/.config/superpowers/worktrees/*)\n    path=\"~/.config/superpowers/worktrees/$project/$BRANCH_NAME\"\n    ;;\nesac\n\n# Create worktree with new branch\ngit worktree add \"$path\" -b \"$BRANCH_NAME\"\ncd \"$path\"\n```\n\n### 3. Run Project Setup\n\nAuto-detect and run appropriate setup:\n\n```bash\n# Node.js\nif [ -f package.json ]; then npm install; fi\n\n# Rust\nif [ -f Cargo.toml ]; then cargo build; fi\n\n# Python\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f pyproject.toml ]; then poetry install; fi\n\n# Go\nif [ -f go.mod ]; then go mod download; fi\n```\n\n### 4. Verify Clean Baseline\n\nRun tests to ensure worktree starts clean:\n\n```bash\n# Examples - use project-appropriate command\nnpm test\ncargo test\npytest\ngo test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed or investigate.\n\n**If tests pass:** Report ready.\n\n### 5. Report Location\n\n```\nWorktree ready at <full-path>\nTests passing (<N> tests, 0 failures)\nReady to implement <feature-name>\n```\n\n## Quick Reference\n\n| Situation | Action |\n|-----------|--------|\n| `.worktrees/` exists | Use it (verify ignored) |\n| `worktrees/` exists | Use it (verify ignored) |\n| Both exist | Use `.worktrees/` |\n| Neither exists | Check CLAUDE.md → Ask user |\n| Directory not ignored | Add to .gitignore + commit |\n| Tests fail during baseline | Report failures + ask |\n| No package.json/Cargo.toml | Skip dependency install |\n\n## Common Mistakes\n\n### Skipping ignore verification\n\n- **Problem:** Worktree contents get tracked, pollute git status\n- **Fix:** Always use `git check-ignore` before creating project-local worktree\n\n### Assuming directory location\n\n- **Problem:** Creates inconsistency, violates project conventions\n- **Fix:** Follow priority: existing > CLAUDE.md > ask\n\n### Proceeding with failing tests\n\n- **Problem:** Can't distinguish new bugs from pre-existing issues\n- **Fix:** Report failures, get explicit permission to proceed\n\n### Hardcoding setup commands\n\n- **Problem:** Breaks on projects using different tools\n- **Fix:** Auto-detect from project files (package.json, etc.)\n\n## Example Workflow\n\n```\nYou: I'm using the using-git-worktrees skill to set up an isolated workspace.\n\n[Check .worktrees/ - exists]\n[Verify ignored - git check-ignore confirms .worktrees/ is ignored]\n[Create worktree: git worktree add .worktrees/auth -b feature/auth]\n[Run npm install]\n[Run npm test - 47 passing]\n\nWorktree ready at /Users/jesse/myproject/.worktrees/auth\nTests passing (47 tests, 0 failures)\nReady to implement auth feature\n```\n\n## Red Flags\n\n**Never:**\n- Create worktree without verifying it's ignored (project-local)\n- Skip baseline test verification\n- Proceed with failing tests without asking\n- Assume directory location when ambiguous\n- Skip CLAUDE.md check\n\n**Always:**\n- Follow directory priority: existing > CLAUDE.md > ask\n- Verify directory is ignored for project-local\n- Auto-detect and run project setup\n- Verify clean test baseline\n\n## Integration\n\n**Called by:**\n- **brainstorming** (Phase 4) - REQUIRED when design is approved and implementation follows\n- Any skill needing isolated workspace\n\n**Pairs with:**\n- **finishing-a-development-branch** - REQUIRED for cleanup after work complete\n- **executing-plans** or **subagent-driven-development** - Work happens in this worktree\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:15.096Z"
  },
  {
    "id": "using-superpowers",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/using-superpowers",
      "url": "https://github.com/obra/superpowers/tree/main/skills/using-superpowers"
    },
    "frontmatter": {
      "name": "using-superpowers",
      "description": "Use when starting any conversation - establishes how to find and use skills, requiring Skill tool invocation before ANY response including clarifying questions"
    },
    "content": "<EXTREMELY-IMPORTANT>\nIf you think there is even a 1% chance a skill might apply to what you are doing, you ABSOLUTELY MUST invoke the skill.\n\nIF A SKILL APPLIES TO YOUR TASK, YOU DO NOT HAVE A CHOICE. YOU MUST USE IT.\n\nThis is not negotiable. This is not optional. You cannot rationalize your way out of this.\n</EXTREMELY-IMPORTANT>\n\n## How to Access Skills\n\n**In Claude Code:** Use the `Skill` tool. When you invoke a skill, its content is loaded and presented to you—follow it directly. Never use the Read tool on skill files.\n\n**In other environments:** Check your platform's documentation for how skills are loaded.\n\n# Using Skills\n\n## The Rule\n\n**Invoke relevant or requested skills BEFORE any response or action.** Even a 1% chance a skill might apply means that you should invoke the skill to check. If an invoked skill turns out to be wrong for the situation, you don't need to use it.\n\n```dot\ndigraph skill_flow {\n    \"User message received\" [shape=doublecircle];\n    \"Might any skill apply?\" [shape=diamond];\n    \"Invoke Skill tool\" [shape=box];\n    \"Announce: 'Using [skill] to [purpose]'\" [shape=box];\n    \"Has checklist?\" [shape=diamond];\n    \"Create TodoWrite todo per item\" [shape=box];\n    \"Follow skill exactly\" [shape=box];\n    \"Respond (including clarifications)\" [shape=doublecircle];\n\n    \"User message received\" -> \"Might any skill apply?\";\n    \"Might any skill apply?\" -> \"Invoke Skill tool\" [label=\"yes, even 1%\"];\n    \"Might any skill apply?\" -> \"Respond (including clarifications)\" [label=\"definitely not\"];\n    \"Invoke Skill tool\" -> \"Announce: 'Using [skill] to [purpose]'\";\n    \"Announce: 'Using [skill] to [purpose]'\" -> \"Has checklist?\";\n    \"Has checklist?\" -> \"Create TodoWrite todo per item\" [label=\"yes\"];\n    \"Has checklist?\" -> \"Follow skill exactly\" [label=\"no\"];\n    \"Create TodoWrite todo per item\" -> \"Follow skill exactly\";\n}\n```\n\n## Red Flags\n\nThese thoughts mean STOP—you're rationalizing:\n\n| Thought | Reality |\n|---------|---------|\n| \"This is just a simple question\" | Questions are tasks. Check for skills. |\n| \"I need more context first\" | Skill check comes BEFORE clarifying questions. |\n| \"Let me explore the codebase first\" | Skills tell you HOW to explore. Check first. |\n| \"I can check git/files quickly\" | Files lack conversation context. Check for skills. |\n| \"Let me gather information first\" | Skills tell you HOW to gather information. |\n| \"This doesn't need a formal skill\" | If a skill exists, use it. |\n| \"I remember this skill\" | Skills evolve. Read current version. |\n| \"This doesn't count as a task\" | Action = task. Check for skills. |\n| \"The skill is overkill\" | Simple things become complex. Use it. |\n| \"I'll just do this one thing first\" | Check BEFORE doing anything. |\n| \"This feels productive\" | Undisciplined action wastes time. Skills prevent this. |\n| \"I know what that means\" | Knowing the concept ≠ using the skill. Invoke it. |\n\n## Skill Priority\n\nWhen multiple skills could apply, use this order:\n\n1. **Process skills first** (brainstorming, debugging) - these determine HOW to approach the task\n2. **Implementation skills second** (frontend-design, mcp-builder) - these guide execution\n\n\"Let's build X\" → brainstorming first, then implementation skills.\n\"Fix this bug\" → debugging first, then domain-specific skills.\n\n## Skill Types\n\n**Rigid** (TDD, debugging): Follow exactly. Don't adapt away discipline.\n\n**Flexible** (patterns): Adapt principles to context.\n\nThe skill itself tells you which.\n\n## User Instructions\n\nInstructions say WHAT, not HOW. \"Add X\" or \"Fix Y\" doesn't mean skip workflows.\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:15.344Z"
  },
  {
    "id": "verification-before-completion",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/verification-before-completion",
      "url": "https://github.com/obra/superpowers/tree/main/skills/verification-before-completion"
    },
    "frontmatter": {
      "name": "verification-before-completion",
      "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always"
    },
    "content": "# Verification Before Completion\n\n## Overview\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n**Violating the letter of this rule is violating the spirit of this rule.**\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n## The Gate Function\n\n```\nBEFORE claiming any status or expressing satisfaction:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n## Common Failures\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test command output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check, extrapolation |\n| Build succeeds | Build command: exit 0 | Linter passing, logs look good |\n| Bug fixed | Test original symptom: passes | Code changed, assumed fixed |\n| Regression test works | Red-green cycle verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification (\"Great!\", \"Perfect!\", \"Done!\", etc.)\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- Tired and wanting work over\n- **ANY wording implying success without having run verification**\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence ≠ evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter ≠ compiler |\n| \"Agent said success\" | Verify independently |\n| \"I'm tired\" | Exhaustion ≠ excuse |\n| \"Partial check is enough\" | Partial proves nothing |\n| \"Different words so rule doesn't apply\" | Spirit over letter |\n\n## Key Patterns\n\n**Tests:**\n```\n✅ [Run test command] [See: 34/34 pass] \"All tests pass\"\n❌ \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\n✅ Write → Run (pass) → Revert fix → Run (MUST FAIL) → Restore → Run (pass)\n❌ \"I've written a regression test\" (without red-green verification)\n```\n\n**Build:**\n```\n✅ [Run build] [See: exit 0] \"Build passes\"\n❌ \"Linter passed\" (linter doesn't check compilation)\n```\n\n**Requirements:**\n```\n✅ Re-read plan → Create checklist → Verify each → Report gaps or completion\n❌ \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\n✅ Agent reports success → Check VCS diff → Verify changes → Report actual state\n❌ Trust agent report\n```\n\n## Why This Matters\n\nFrom 24 failure memories:\n- your human partner said \"I don't believe you\" - trust broken\n- Undefined functions shipped - would crash\n- Missing requirements shipped - incomplete features\n- Time wasted on false completion → redirect → rework\n- Violates: \"Honesty is a core value. If you lie, you'll be replaced.\"\n\n## When To Apply\n\n**ALWAYS before:**\n- ANY variation of success/completion claims\n- ANY expression of satisfaction\n- ANY positive statement about work state\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n**Rule applies to:**\n- Exact phrases\n- Paraphrases and synonyms\n- Implications of success\n- ANY communication suggesting completion/correctness\n\n## The Bottom Line\n\n**No shortcuts for verification.**\n\nRun the command. Read the output. THEN claim the result.\n\nThis is non-negotiable.\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:15.590Z"
  },
  {
    "id": "writing-plans",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/writing-plans",
      "url": "https://github.com/obra/superpowers/tree/main/skills/writing-plans"
    },
    "frontmatter": {
      "name": "writing-plans",
      "description": "Use when you have a spec or requirements for a multi-step task, before touching code"
    },
    "content": "# Writing Plans\n\n## Overview\n\nWrite comprehensive implementation plans assuming the engineer has zero context for our codebase and questionable taste. Document everything they need to know: which files to touch for each task, code, testing, docs they might need to check, how to test it. Give them the whole plan as bite-sized tasks. DRY. YAGNI. TDD. Frequent commits.\n\nAssume they are a skilled developer, but know almost nothing about our toolset or problem domain. Assume they don't know good test design very well.\n\n**Announce at start:** \"I'm using the writing-plans skill to create the implementation plan.\"\n\n**Context:** This should be run in a dedicated worktree (created by brainstorming skill).\n\n**Save plans to:** `docs/plans/YYYY-MM-DD-<feature-name>.md`\n\n## Bite-Sized Task Granularity\n\n**Each step is one action (2-5 minutes):**\n- \"Write the failing test\" - step\n- \"Run it to make sure it fails\" - step\n- \"Implement the minimal code to make the test pass\" - step\n- \"Run the tests and make sure they pass\" - step\n- \"Commit\" - step\n\n## Plan Document Header\n\n**Every plan MUST start with this header:**\n\n```markdown\n# [Feature Name] Implementation Plan\n\n> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.\n\n**Goal:** [One sentence describing what this builds]\n\n**Architecture:** [2-3 sentences about approach]\n\n**Tech Stack:** [Key technologies/libraries]\n\n---\n```\n\n## Task Structure\n\n```markdown\n### Task N: [Component Name]\n\n**Files:**\n- Create: `exact/path/to/file.py`\n- Modify: `exact/path/to/existing.py:123-145`\n- Test: `tests/exact/path/to/test.py`\n\n**Step 1: Write the failing test**\n\n```python\ndef test_specific_behavior():\n    result = function(input)\n    assert result == expected\n```\n\n**Step 2: Run test to verify it fails**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: FAIL with \"function not defined\"\n\n**Step 3: Write minimal implementation**\n\n```python\ndef function(input):\n    return expected\n```\n\n**Step 4: Run test to verify it passes**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: PASS\n\n**Step 5: Commit**\n\n```bash\ngit add tests/path/test.py src/path/file.py\ngit commit -m \"feat: add specific feature\"\n```\n```\n\n## Remember\n- Exact file paths always\n- Complete code in plan (not \"add validation\")\n- Exact commands with expected output\n- Reference relevant skills with @ syntax\n- DRY, YAGNI, TDD, frequent commits\n\n## Execution Handoff\n\nAfter saving the plan, offer execution choice:\n\n**\"Plan complete and saved to `docs/plans/<filename>.md`. Two execution options:**\n\n**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration\n\n**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints\n\n**Which approach?\"**\n\n**If Subagent-Driven chosen:**\n- **REQUIRED SUB-SKILL:** Use superpowers:subagent-driven-development\n- Stay in this session\n- Fresh subagent per task + code review\n\n**If Parallel Session chosen:**\n- Guide them to open new session in worktree\n- **REQUIRED SUB-SKILL:** New session uses superpowers:executing-plans\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:15.839Z"
  },
  {
    "id": "writing-skills",
    "source": {
      "type": "github-folder",
      "repo": "obra/superpowers",
      "path": "skills/writing-skills",
      "url": "https://github.com/obra/superpowers/tree/main/skills/writing-skills"
    },
    "frontmatter": {
      "name": "writing-skills",
      "description": "Use when creating new skills, editing existing skills, or verifying skills work before deployment"
    },
    "content": "# Writing Skills\n\n## Overview\n\n**Writing skills IS Test-Driven Development applied to process documentation.**\n\n**Personal skills live in agent-specific directories (`~/.claude/skills` for Claude Code, `~/.codex/skills` for Codex)** \n\nYou write test cases (pressure scenarios with subagents), watch them fail (baseline behavior), write the skill (documentation), watch tests pass (agents comply), and refactor (close loopholes).\n\n**Core principle:** If you didn't watch an agent fail without the skill, you don't know if the skill teaches the right thing.\n\n**REQUIRED BACKGROUND:** You MUST understand superpowers:test-driven-development before using this skill. That skill defines the fundamental RED-GREEN-REFACTOR cycle. This skill adapts TDD to documentation.\n\n**Official guidance:** For Anthropic's official skill authoring best practices, see anthropic-best-practices.md. This document provides additional patterns and guidelines that complement the TDD-focused approach in this skill.\n\n## What is a Skill?\n\nA **skill** is a reference guide for proven techniques, patterns, or tools. Skills help future Claude instances find and apply effective approaches.\n\n**Skills are:** Reusable techniques, patterns, tools, reference guides\n\n**Skills are NOT:** Narratives about how you solved a problem once\n\n## TDD Mapping for Skills\n\n| TDD Concept | Skill Creation |\n|-------------|----------------|\n| **Test case** | Pressure scenario with subagent |\n| **Production code** | Skill document (SKILL.md) |\n| **Test fails (RED)** | Agent violates rule without skill (baseline) |\n| **Test passes (GREEN)** | Agent complies with skill present |\n| **Refactor** | Close loopholes while maintaining compliance |\n| **Write test first** | Run baseline scenario BEFORE writing skill |\n| **Watch it fail** | Document exact rationalizations agent uses |\n| **Minimal code** | Write skill addressing those specific violations |\n| **Watch it pass** | Verify agent now complies |\n| **Refactor cycle** | Find new rationalizations → plug → re-verify |\n\nThe entire skill creation process follows RED-GREEN-REFACTOR.\n\n## When to Create a Skill\n\n**Create when:**\n- Technique wasn't intuitively obvious to you\n- You'd reference this again across projects\n- Pattern applies broadly (not project-specific)\n- Others would benefit\n\n**Don't create for:**\n- One-off solutions\n- Standard practices well-documented elsewhere\n- Project-specific conventions (put in CLAUDE.md)\n- Mechanical constraints (if it's enforceable with regex/validation, automate it—save documentation for judgment calls)\n\n## Skill Types\n\n### Technique\nConcrete method with steps to follow (condition-based-waiting, root-cause-tracing)\n\n### Pattern\nWay of thinking about problems (flatten-with-flags, test-invariants)\n\n### Reference\nAPI docs, syntax guides, tool documentation (office docs)\n\n## Directory Structure\n\n\n```\nskills/\n  skill-name/\n    SKILL.md              # Main reference (required)\n    supporting-file.*     # Only if needed\n```\n\n**Flat namespace** - all skills in one searchable namespace\n\n**Separate files for:**\n1. **Heavy reference** (100+ lines) - API docs, comprehensive syntax\n2. **Reusable tools** - Scripts, utilities, templates\n\n**Keep inline:**\n- Principles and concepts\n- Code patterns (< 50 lines)\n- Everything else\n\n## SKILL.md Structure\n\n**Frontmatter (YAML):**\n- Only two fields supported: `name` and `description`\n- Max 1024 characters total\n- `name`: Use letters, numbers, and hyphens only (no parentheses, special chars)\n- `description`: Third-person, describes ONLY when to use (NOT what it does)\n  - Start with \"Use when...\" to focus on triggering conditions\n  - Include specific symptoms, situations, and contexts\n  - **NEVER summarize the skill's process or workflow** (see CSO section for why)\n  - Keep under 500 characters if possible\n\n```markdown\n---\nname: Skill-Name-With-Hyphens\ndescription: Use when [specific triggering conditions and symptoms]\n---\n\n# Skill Name\n\n## Overview\nWhat is this? Core principle in 1-2 sentences.\n\n## When to Use\n[Small inline flowchart IF decision non-obvious]\n\nBullet list with SYMPTOMS and use cases\nWhen NOT to use\n\n## Core Pattern (for techniques/patterns)\nBefore/after code comparison\n\n## Quick Reference\nTable or bullets for scanning common operations\n\n## Implementation\nInline code for simple patterns\nLink to file for heavy reference or reusable tools\n\n## Common Mistakes\nWhat goes wrong + fixes\n\n## Real-World Impact (optional)\nConcrete results\n```\n\n\n## Claude Search Optimization (CSO)\n\n**Critical for discovery:** Future Claude needs to FIND your skill\n\n### 1. Rich Description Field\n\n**Purpose:** Claude reads description to decide which skills to load for a given task. Make it answer: \"Should I read this skill right now?\"\n\n**Format:** Start with \"Use when...\" to focus on triggering conditions\n\n**CRITICAL: Description = When to Use, NOT What the Skill Does**\n\nThe description should ONLY describe triggering conditions. Do NOT summarize the skill's process or workflow in the description.\n\n**Why this matters:** Testing revealed that when a description summarizes the skill's workflow, Claude may follow the description instead of reading the full skill content. A description saying \"code review between tasks\" caused Claude to do ONE review, even though the skill's flowchart clearly showed TWO reviews (spec compliance then code quality).\n\nWhen the description was changed to just \"Use when executing implementation plans with independent tasks\" (no workflow summary), Claude correctly read the flowchart and followed the two-stage review process.\n\n**The trap:** Descriptions that summarize workflow create a shortcut Claude will take. The skill body becomes documentation Claude skips.\n\n```yaml\n# ❌ BAD: Summarizes workflow - Claude may follow this instead of reading skill\ndescription: Use when executing plans - dispatches subagent per task with code review between tasks\n\n# ❌ BAD: Too much process detail\ndescription: Use for TDD - write test first, watch it fail, write minimal code, refactor\n\n# ✅ GOOD: Just triggering conditions, no workflow summary\ndescription: Use when executing implementation plans with independent tasks in the current session\n\n# ✅ GOOD: Triggering conditions only\ndescription: Use when implementing any feature or bugfix, before writing implementation code\n```\n\n**Content:**\n- Use concrete triggers, symptoms, and situations that signal this skill applies\n- Describe the *problem* (race conditions, inconsistent behavior) not *language-specific symptoms* (setTimeout, sleep)\n- Keep triggers technology-agnostic unless the skill itself is technology-specific\n- If skill is technology-specific, make that explicit in the trigger\n- Write in third person (injected into system prompt)\n- **NEVER summarize the skill's process or workflow**\n\n```yaml\n# ❌ BAD: Too abstract, vague, doesn't include when to use\ndescription: For async testing\n\n# ❌ BAD: First person\ndescription: I can help you with async tests when they're flaky\n\n# ❌ BAD: Mentions technology but skill isn't specific to it\ndescription: Use when tests use setTimeout/sleep and are flaky\n\n# ✅ GOOD: Starts with \"Use when\", describes problem, no workflow\ndescription: Use when tests have race conditions, timing dependencies, or pass/fail inconsistently\n\n# ✅ GOOD: Technology-specific skill with explicit trigger\ndescription: Use when using React Router and handling authentication redirects\n```\n\n### 2. Keyword Coverage\n\nUse words Claude would search for:\n- Error messages: \"Hook timed out\", \"ENOTEMPTY\", \"race condition\"\n- Symptoms: \"flaky\", \"hanging\", \"zombie\", \"pollution\"\n- Synonyms: \"timeout/hang/freeze\", \"cleanup/teardown/afterEach\"\n- Tools: Actual commands, library names, file types\n\n### 3. Descriptive Naming\n\n**Use active voice, verb-first:**\n- ✅ `creating-skills` not `skill-creation`\n- ✅ `condition-based-waiting` not `async-test-helpers`\n\n### 4. Token Efficiency (Critical)\n\n**Problem:** getting-started and frequently-referenced skills load into EVERY conversation. Every token counts.\n\n**Target word counts:**\n- getting-started workflows: <150 words each\n- Frequently-loaded skills: <200 words total\n- Other skills: <500 words (still be concise)\n\n**Techniques:**\n\n**Move details to tool help:**\n```bash\n# ❌ BAD: Document all flags in SKILL.md\nsearch-conversations supports --text, --both, --after DATE, --before DATE, --limit N\n\n# ✅ GOOD: Reference --help\nsearch-conversations supports multiple modes and filters. Run --help for details.\n```\n\n**Use cross-references:**\n```markdown\n# ❌ BAD: Repeat workflow details\nWhen searching, dispatch subagent with template...\n[20 lines of repeated instructions]\n\n# ✅ GOOD: Reference other skill\nAlways use subagents (50-100x context savings). REQUIRED: Use [other-skill-name] for workflow.\n```\n\n**Compress examples:**\n```markdown\n# ❌ BAD: Verbose example (42 words)\nyour human partner: \"How did we handle authentication errors in React Router before?\"\nYou: I'll search past conversations for React Router authentication patterns.\n[Dispatch subagent with search query: \"React Router authentication error handling 401\"]\n\n# ✅ GOOD: Minimal example (20 words)\nPartner: \"How did we handle auth errors in React Router?\"\nYou: Searching...\n[Dispatch subagent → synthesis]\n```\n\n**Eliminate redundancy:**\n- Don't repeat what's in cross-referenced skills\n- Don't explain what's obvious from command\n- Don't include multiple examples of same pattern\n\n**Verification:**\n```bash\nwc -w skills/path/SKILL.md\n# getting-started workflows: aim for <150 each\n# Other frequently-loaded: aim for <200 total\n```\n\n**Name by what you DO or core insight:**\n- ✅ `condition-based-waiting` > `async-test-helpers`\n- ✅ `using-skills` not `skill-usage`\n- ✅ `flatten-with-flags` > `data-structure-refactoring`\n- ✅ `root-cause-tracing` > `debugging-techniques`\n\n**Gerunds (-ing) work well for processes:**\n- `creating-skills`, `testing-skills`, `debugging-with-logs`\n- Active, describes the action you're taking\n\n### 4. Cross-Referencing Other Skills\n\n**When writing documentation that references other skills:**\n\nUse skill name only, with explicit requirement markers:\n- ✅ Good: `**REQUIRED SUB-SKILL:** Use superpowers:test-driven-development`\n- ✅ Good: `**REQUIRED BACKGROUND:** You MUST understand superpowers:systematic-debugging`\n- ❌ Bad: `See skills/testing/test-driven-development` (unclear if required)\n- ❌ Bad: `@skills/testing/test-driven-development/SKILL.md` (force-loads, burns context)\n\n**Why no @ links:** `@` syntax force-loads files immediately, consuming 200k+ context before you need them.\n\n## Flowchart Usage\n\n```dot\ndigraph when_flowchart {\n    \"Need to show information?\" [shape=diamond];\n    \"Decision where I might go wrong?\" [shape=diamond];\n    \"Use markdown\" [shape=box];\n    \"Small inline flowchart\" [shape=box];\n\n    \"Need to show information?\" -> \"Decision where I might go wrong?\" [label=\"yes\"];\n    \"Decision where I might go wrong?\" -> \"Small inline flowchart\" [label=\"yes\"];\n    \"Decision where I might go wrong?\" -> \"Use markdown\" [label=\"no\"];\n}\n```\n\n**Use flowcharts ONLY for:**\n- Non-obvious decision points\n- Process loops where you might stop too early\n- \"When to use A vs B\" decisions\n\n**Never use flowcharts for:**\n- Reference material → Tables, lists\n- Code examples → Markdown blocks\n- Linear instructions → Numbered lists\n- Labels without semantic meaning (step1, helper2)\n\nSee @graphviz-conventions.dot for graphviz style rules.\n\n**Visualizing for your human partner:** Use `render-graphs.js` in this directory to render a skill's flowcharts to SVG:\n```bash\n./render-graphs.js ../some-skill           # Each diagram separately\n./render-graphs.js ../some-skill --combine # All diagrams in one SVG\n```\n\n## Code Examples\n\n**One excellent example beats many mediocre ones**\n\nChoose most relevant language:\n- Testing techniques → TypeScript/JavaScript\n- System debugging → Shell/Python\n- Data processing → Python\n\n**Good example:**\n- Complete and runnable\n- Well-commented explaining WHY\n- From real scenario\n- Shows pattern clearly\n- Ready to adapt (not generic template)\n\n**Don't:**\n- Implement in 5+ languages\n- Create fill-in-the-blank templates\n- Write contrived examples\n\nYou're good at porting - one great example is enough.\n\n## File Organization\n\n### Self-Contained Skill\n```\ndefense-in-depth/\n  SKILL.md    # Everything inline\n```\nWhen: All content fits, no heavy reference needed\n\n### Skill with Reusable Tool\n```\ncondition-based-waiting/\n  SKILL.md    # Overview + patterns\n  example.ts  # Working helpers to adapt\n```\nWhen: Tool is reusable code, not just narrative\n\n### Skill with Heavy Reference\n```\npptx/\n  SKILL.md       # Overview + workflows\n  pptxgenjs.md   # 600 lines API reference\n  ooxml.md       # 500 lines XML structure\n  scripts/       # Executable tools\n```\nWhen: Reference material too large for inline\n\n## The Iron Law (Same as TDD)\n\n```\nNO SKILL WITHOUT A FAILING TEST FIRST\n```\n\nThis applies to NEW skills AND EDITS to existing skills.\n\nWrite skill before testing? Delete it. Start over.\nEdit skill without testing? Same violation.\n\n**No exceptions:**\n- Not for \"simple additions\"\n- Not for \"just adding a section\"\n- Not for \"documentation updates\"\n- Don't keep untested changes as \"reference\"\n- Don't \"adapt\" while running tests\n- Delete means delete\n\n**REQUIRED BACKGROUND:** The superpowers:test-driven-development skill explains why this matters. Same principles apply to documentation.\n\n## Testing All Skill Types\n\nDifferent skill types need different test approaches:\n\n### Discipline-Enforcing Skills (rules/requirements)\n\n**Examples:** TDD, verification-before-completion, designing-before-coding\n\n**Test with:**\n- Academic questions: Do they understand the rules?\n- Pressure scenarios: Do they comply under stress?\n- Multiple pressures combined: time + sunk cost + exhaustion\n- Identify rationalizations and add explicit counters\n\n**Success criteria:** Agent follows rule under maximum pressure\n\n### Technique Skills (how-to guides)\n\n**Examples:** condition-based-waiting, root-cause-tracing, defensive-programming\n\n**Test with:**\n- Application scenarios: Can they apply the technique correctly?\n- Variation scenarios: Do they handle edge cases?\n- Missing information tests: Do instructions have gaps?\n\n**Success criteria:** Agent successfully applies technique to new scenario\n\n### Pattern Skills (mental models)\n\n**Examples:** reducing-complexity, information-hiding concepts\n\n**Test with:**\n- Recognition scenarios: Do they recognize when pattern applies?\n- Application scenarios: Can they use the mental model?\n- Counter-examples: Do they know when NOT to apply?\n\n**Success criteria:** Agent correctly identifies when/how to apply pattern\n\n### Reference Skills (documentation/APIs)\n\n**Examples:** API documentation, command references, library guides\n\n**Test with:**\n- Retrieval scenarios: Can they find the right information?\n- Application scenarios: Can they use what they found correctly?\n- Gap testing: Are common use cases covered?\n\n**Success criteria:** Agent finds and correctly applies reference information\n\n## Common Rationalizations for Skipping Testing\n\n| Excuse | Reality |\n|--------|---------|\n| \"Skill is obviously clear\" | Clear to you ≠ clear to other agents. Test it. |\n| \"It's just a reference\" | References can have gaps, unclear sections. Test retrieval. |\n| \"Testing is overkill\" | Untested skills have issues. Always. 15 min testing saves hours. |\n| \"I'll test if problems emerge\" | Problems = agents can't use skill. Test BEFORE deploying. |\n| \"Too tedious to test\" | Testing is less tedious than debugging bad skill in production. |\n| \"I'm confident it's good\" | Overconfidence guarantees issues. Test anyway. |\n| \"Academic review is enough\" | Reading ≠ using. Test application scenarios. |\n| \"No time to test\" | Deploying untested skill wastes more time fixing it later. |\n\n**All of these mean: Test before deploying. No exceptions.**\n\n## Bulletproofing Skills Against Rationalization\n\nSkills that enforce discipline (like TDD) need to resist rationalization. Agents are smart and will find loopholes when under pressure.\n\n**Psychology note:** Understanding WHY persuasion techniques work helps you apply them systematically. See persuasion-principles.md for research foundation (Cialdini, 2021; Meincke et al., 2025) on authority, commitment, scarcity, social proof, and unity principles.\n\n### Close Every Loophole Explicitly\n\nDon't just state the rule - forbid specific workarounds:\n\n<Bad>\n```markdown\nWrite code before test? Delete it.\n```\n</Bad>\n\n<Good>\n```markdown\nWrite code before test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n```\n</Good>\n\n### Address \"Spirit vs Letter\" Arguments\n\nAdd foundational principle early:\n\n```markdown\n**Violating the letter of the rules is violating the spirit of the rules.**\n```\n\nThis cuts off entire class of \"I'm following the spirit\" rationalizations.\n\n### Build Rationalization Table\n\nCapture rationalizations from baseline testing (see Testing section below). Every excuse agents make goes in the table:\n\n```markdown\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n```\n\n### Create Red Flags List\n\nMake it easy for agents to self-check when rationalizing:\n\n```markdown\n## Red Flags - STOP and Start Over\n\n- Code before test\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n```\n\n### Update CSO for Violation Symptoms\n\nAdd to description: symptoms of when you're ABOUT to violate the rule:\n\n```yaml\ndescription: use when implementing any feature or bugfix, before writing implementation code\n```\n\n## RED-GREEN-REFACTOR for Skills\n\nFollow the TDD cycle:\n\n### RED: Write Failing Test (Baseline)\n\nRun pressure scenario with subagent WITHOUT the skill. Document exact behavior:\n- What choices did they make?\n- What rationalizations did they use (verbatim)?\n- Which pressures triggered violations?\n\nThis is \"watch the test fail\" - you must see what agents naturally do before writing the skill.\n\n### GREEN: Write Minimal Skill\n\nWrite skill that addresses those specific rationalizations. Don't add extra content for hypothetical cases.\n\nRun same scenarios WITH skill. Agent should now comply.\n\n### REFACTOR: Close Loopholes\n\nAgent found new rationalization? Add explicit counter. Re-test until bulletproof.\n\n**Testing methodology:** See @testing-skills-with-subagents.md for the complete testing methodology:\n- How to write pressure scenarios\n- Pressure types (time, sunk cost, authority, exhaustion)\n- Plugging holes systematically\n- Meta-testing techniques\n\n## Anti-Patterns\n\n### ❌ Narrative Example\n\"In session 2025-10-03, we found empty projectDir caused...\"\n**Why bad:** Too specific, not reusable\n\n### ❌ Multi-Language Dilution\nexample-js.js, example-py.py, example-go.go\n**Why bad:** Mediocre quality, maintenance burden\n\n### ❌ Code in Flowcharts\n```dot\nstep1 [label=\"import fs\"];\nstep2 [label=\"read file\"];\n```\n**Why bad:** Can't copy-paste, hard to read\n\n### ❌ Generic Labels\nhelper1, helper2, step3, pattern4\n**Why bad:** Labels should have semantic meaning\n\n## STOP: Before Moving to Next Skill\n\n**After writing ANY skill, you MUST STOP and complete the deployment process.**\n\n**Do NOT:**\n- Create multiple skills in batch without testing each\n- Move to next skill before current one is verified\n- Skip testing because \"batching is more efficient\"\n\n**The deployment checklist below is MANDATORY for EACH skill.**\n\nDeploying untested skills = deploying untested code. It's a violation of quality standards.\n\n## Skill Creation Checklist (TDD Adapted)\n\n**IMPORTANT: Use TodoWrite to create todos for EACH checklist item below.**\n\n**RED Phase - Write Failing Test:**\n- [ ] Create pressure scenarios (3+ combined pressures for discipline skills)\n- [ ] Run scenarios WITHOUT skill - document baseline behavior verbatim\n- [ ] Identify patterns in rationalizations/failures\n\n**GREEN Phase - Write Minimal Skill:**\n- [ ] Name uses only letters, numbers, hyphens (no parentheses/special chars)\n- [ ] YAML frontmatter with only name and description (max 1024 chars)\n- [ ] Description starts with \"Use when...\" and includes specific triggers/symptoms\n- [ ] Description written in third person\n- [ ] Keywords throughout for search (errors, symptoms, tools)\n- [ ] Clear overview with core principle\n- [ ] Address specific baseline failures identified in RED\n- [ ] Code inline OR link to separate file\n- [ ] One excellent example (not multi-language)\n- [ ] Run scenarios WITH skill - verify agents now comply\n\n**REFACTOR Phase - Close Loopholes:**\n- [ ] Identify NEW rationalizations from testing\n- [ ] Add explicit counters (if discipline skill)\n- [ ] Build rationalization table from all test iterations\n- [ ] Create red flags list\n- [ ] Re-test until bulletproof\n\n**Quality Checks:**\n- [ ] Small flowchart only if decision non-obvious\n- [ ] Quick reference table\n- [ ] Common mistakes section\n- [ ] No narrative storytelling\n- [ ] Supporting files only for tools or heavy reference\n\n**Deployment:**\n- [ ] Commit skill to git and push to your fork (if configured)\n- [ ] Consider contributing back via PR (if broadly useful)\n\n## Discovery Workflow\n\nHow future Claude finds your skill:\n\n1. **Encounters problem** (\"tests are flaky\")\n3. **Finds SKILL** (description matches)\n4. **Scans overview** (is this relevant?)\n5. **Reads patterns** (quick reference table)\n6. **Loads example** (only when implementing)\n\n**Optimize for this flow** - put searchable terms early and often.\n\n## The Bottom Line\n\n**Creating skills IS TDD for process documentation.**\n\nSame Iron Law: No skill without failing test first.\nSame cycle: RED (baseline) → GREEN (write skill) → REFACTOR (close loopholes).\nSame benefits: Better quality, fewer surprises, bulletproof results.\n\nIf you follow TDD for code, follow it for skills. It's the same discipline applied to documentation.\n",
    "github": {
      "stars": 13645,
      "forks": 1125,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-27T04:58:41Z",
      "createdAt": "2025-10-09T19:45:18Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:16.094Z"
  },
  {
    "id": "superpowers-skills",
    "source": {
      "type": "github-repo",
      "repo": "obra/superpowers-skills",
      "url": "https://github.com/obra/superpowers-skills"
    },
    "frontmatter": {
      "name": "superpowers-skills",
      "description": "Community-editable skills repository"
    },
    "content": "",
    "github": {
      "stars": 461,
      "forks": 110,
      "owner": "obra",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/45416?v=4",
      "license": "MIT",
      "lastCommit": "2025-10-14T20:52:50Z",
      "createdAt": "2025-10-11T17:06:05Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:18.178Z"
  },
  {
    "id": "anthropics-skills",
    "source": {
      "type": "github-repo",
      "repo": "anthropics/skills",
      "url": "https://github.com/anthropics/skills"
    },
    "frontmatter": {
      "name": "anthropics/skills",
      "description": "Official public repository for Skills"
    },
    "content": "",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:18.178Z"
  },
  {
    "id": "claude-cookbooks-skills",
    "source": {
      "type": "github-repo",
      "repo": "anthropics/claude-cookbooks",
      "url": "https://github.com/anthropics/claude-cookbooks/tree/main/skills"
    },
    "frontmatter": {
      "name": "Claude Cookbooks - Skills",
      "description": "Example notebooks and tutorials"
    },
    "content": "",
    "github": {
      "stars": 30493,
      "forks": 3106,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-30T17:47:19Z",
      "createdAt": "2023-08-15T20:23:51Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:18.178Z"
  },
  {
    "id": "issue-268",
    "source": {
      "type": "github-repo",
      "repo": "anthropics/claude-agent-sdk-python",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/issues/268"
    },
    "frontmatter": {
      "name": "Issue #268",
      "description": "Workaround: Manually specify skill paths"
    },
    "content": "",
    "github": {
      "stars": 3816,
      "forks": 507,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-23T19:33:01Z",
      "createdAt": "2025-06-11T21:33:43Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:18.179Z"
  },
  {
    "id": "official-skills-repository-issues",
    "source": {
      "type": "github-repo",
      "repo": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/issues"
    },
    "frontmatter": {
      "name": "Official Skills Repository Issues",
      "description": "[Claude Documentation](https://docs.claude.com/)"
    },
    "content": "",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:18.179Z"
  },
  {
    "id": "community-discussions",
    "source": {
      "type": "github-repo",
      "repo": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/discussions"
    },
    "frontmatter": {
      "name": "Community Discussions",
      "description": "</details>"
    },
    "content": "",
    "github": {
      "stars": 32783,
      "forks": 2969,
      "owner": "anthropics",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "lastCommit": "2025-12-20T18:09:45Z",
      "createdAt": "2025-09-22T15:53:31Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:18.179Z"
  },
  {
    "id": "reference",
    "source": {
      "type": "github-search",
      "repo": "openrewrite/rewrite-docs",
      "path": "docs/reference/claude-skill.md",
      "url": "https://github.com/openrewrite/rewrite-docs/blob/main/docs/reference/claude-skill.md"
    },
    "frontmatter": {
      "description": "How to install and use the OpenRewrite Recipe Writer Claude Skill for expert guidance on writing OpenRewrite recipes."
    },
    "content": "# Claude Skill for Recipe Writing\n\nThe OpenRewrite Recipe Writer is a Claude Skill that provides expert guidance for writing OpenRewrite recipes. When installed in your Claude account, it automatically activates when you ask questions about recipe development, offering step-by-step instructions, code templates, and best practices.\n\n## What is a Claude Skill?\n\nClaude Skills are modular capabilities that extend Claude's functionality. They package instructions, metadata, and resources that Claude uses automatically when relevant to your conversation. The OpenRewrite Recipe Writer skill includes:\n\n- Recipe type selection guidance (Declarative vs Refaster vs Imperative)\n- Step-by-step recipe development workflows\n- Code templates for recipes, visitors, and tests\n- Complete working examples\n- JavaTemplate usage patterns\n- Testing strategies with RewriteTest\n- Best practices and conventions\n- A comprehensive development checklist\n\n## Prerequisites\n\n- **Claude API account** with an API key ([Get one here](https://console.anthropic.com/))\n- **Git** (to clone the OpenRewrite documentation repository)\n- **curl** and **jq** (command-line tools, usually pre-installed on macOS/Linux)\n\n## Installation\n\nThere are two ways to install this skill, depending on where you want to use it:\n\n### Option 1: Install for Claude.ai (Web Interface)\n\nThis is the recommended approach for most users.\n\n#### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/openrewrite/rewrite-docs.git\ncd rewrite-docs/claude-skill\n```\n\n#### Step 2: Create a ZIP File\n\nCreate a ZIP file containing all the skill files (do **not** include the parent directory):\n\n```bash\n# On macOS/Linux\nzip -r openrewrite-recipe-writer.zip SKILL.md *.java *.yml *.md -x README.md\n\n# Or manually: Select all files (not the folder), right-click, and compress\n```\n\n**Important**: The ZIP must contain the files at the root level, not in a subdirectory. When you open the ZIP, you should immediately see `SKILL.md`, not a folder.\n\n#### Step 3: Upload to Claude.ai\n\n1. Go to [https://claude.ai](https://claude.ai)\n2. Sign in to your account\n3. Navigate to **Settings** (gear icon in bottom left)\n4. Go to **Capabilities** → **Skills**\n5. Click **Upload Skill** (or similar button)\n6. Select your `openrewrite-recipe-writer.zip` file\n7. Wait for the upload to complete\n\nOnce uploaded, the skill will appear in your skills list and automatically activate when you ask questions about OpenRewrite recipe development.\n\n### Option 2: Install via API (For Programmatic Use)\n\nUse this approach if you're building applications that use the Claude API.\n\n#### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/openrewrite/rewrite-docs.git\ncd rewrite-docs/claude-skill\n```\n\n#### Step 2: Set Up Your API Key\n\nCreate a file to store your Anthropic API key (this file is already in `.gitignore` and won't be committed):\n\n```bash\necho \"your-anthropic-api-key-here\" > anthropic.key\n```\n\nAlternatively, you can export it as an environment variable:\n\n```bash\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key-here\"\n```\n\n#### Step 3: Upload the Skill\n\nRun the upload script to create the skill in your API workspace:\n\n```bash\n./upload-skill.sh\n```\n\nYou should see output like:\n\n```\nLoading API key from anthropic.key file...\nNo existing skill ID found.\nCreating new OpenRewrite Recipe Writing skill...\n\n✓ Success! Skill created successfully.\n\nResponse:\n{\n  \"type\": \"skill\",\n  \"id\": \"skill_xxxxxxxxxxxxx\",\n  \"created_at\": \"2025-10-17T...\",\n  \"updated_at\": \"2025-10-17T...\",\n  \"display_title\": \"OpenRewrite Recipe Writing\",\n  \"source\": \"custom\",\n  \"latest_version\": \"...\"\n}\n\nSkill ID saved to skill-id.txt for future updates.\n```\n\nThe script automatically saves your skill ID to `skill-id.txt` so future runs will update the existing skill instead of creating duplicates.\n\n**Note**: Skills uploaded via the API are **not** available on claude.ai. You must upload separately to each platform where you want to use the skill.\n\n## Using the Skill\n\n### With Claude.ai\n\nIf you uploaded the skill via the web interface (Option 1 above):\n\n1. Go to [https://claude.ai](https://claude.ai)\n2. Start a new conversation\n3. Ask questions about OpenRewrite recipe development\n\nThe skill will automatically activate when your questions are relevant to recipe writing. You can verify it's working if responses include detailed OpenRewrite-specific guidance with code templates and best practices.\n\n### With Claude API\n\nIf you uploaded the skill via the API (Option 2 above), the skill automatically activates when you ask questions related to OpenRewrite recipe development:\n\n```python\nimport anthropic\n\nclient = anthropic.Anthropic(api_key=\"your-api-key\")\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"How do I write an OpenRewrite recipe to rename a Java method?\"\n    }]\n)\n\nprint(response.content[0].text)\n```\n\n**Important**: Skills are **not shared** between platforms. A skill uploaded via the API is only available in API calls. A skill uploaded to claude.ai is only available on the web interface. You must upload separately to each platform where you want to use it.\n\n## Example Questions\n\nHere are some questions that will activate the skill:\n\n**Recipe Type Selection:**\n- \"What type of recipe should I use to replace one method call with another?\"\n- \"Should I use a declarative or imperative recipe for this migration?\"\n\n**Getting Started:**\n- \"Show me a template for an imperative recipe\"\n- \"How do I structure a recipe test?\"\n- \"Give me an example of a simple recipe\"\n\n**Specific Techniques:**\n- \"How do I use JavaTemplate to add a method to a class?\"\n- \"How do I write a ScanningRecipe that analyzes multiple files?\"\n- \"What are preconditions and when should I use them?\"\n\n**Best Practices:**\n- \"What does 'do no harm' mean for recipes?\"\n- \"How do I ensure my recipe is idempotent?\"\n- \"How should I test that my recipe doesn't make unnecessary changes?\"\n\n## Updating the Skill\n\n### Updating on Claude.ai\n\nWhen the skill is updated (new examples, improved templates, etc.):\n\n1. Pull the latest changes:\n   ```bash\n   cd rewrite-docs/claude-skill\n   git pull origin master\n   ```\n\n2. Create a new ZIP file:\n   ```bash\n   zip -r openrewrite-recipe-writer.zip SKILL.md *.java *.yml *.md -x README.md\n   ```\n\n3. Go to claude.ai → Settings → Capabilities → Skills\n4. Remove the old version of the skill\n5. Upload the new ZIP file\n\n### Updating via API\n\nWhen the skill is updated, you can update your API-installed version:\n\n```bash\ncd rewrite-docs/claude-skill\ngit pull origin master\n./upload-skill.sh\n```\n\nThe script will detect the existing `skill-id.txt` file and update your skill instead of creating a new one.\n\n## What's Included\n\nThe skill includes the following resources:\n\n### Templates\n- **Imperative Recipe Template** - Boilerplate for Java-based recipes with annotations, visitor patterns, and helper methods\n- **Declarative YAML Template** - Structure for composing existing recipes with configuration\n- **Recipe Test Template** - Complete test class with multiple testing patterns\n\n### Examples\n- **SayHello Recipe** - Complete working example that adds a method to a class\n- **ScanningRecipe Example** - Advanced pattern showing multi-file analysis with accumulator\n- **Declarative Migration Examples** - Real-world YAML recipes for framework migrations, security fixes, and code quality improvements\n\n### Documentation\n- **Development Checklist** - 200+ verification items covering planning, implementation, testing, and distribution\n\n## Skill Capabilities\n\nWhen you ask recipe-related questions, the skill provides:\n\n- **Contextual Guidance** - Recommendations based on your specific use case\n- **Code Generation** - Templates and examples customized to your needs\n- **Best Practices** - Conventions for naming, testing, and structuring recipes\n- **Troubleshooting** - Help with common issues and pitfalls\n- **Decision Support** - Choosing between declarative, Refaster, and imperative approaches\n\n## Progressive Disclosure\n\nThe skill uses Claude's progressive disclosure model to be efficient with context:\n\n1. **Metadata** (~100 tokens) - Always loaded, helps Claude know when to activate the skill\n2. **Instructions** (~4.5k tokens) - Loaded when the skill is activated\n3. **Resources** - Individual files loaded only when needed for specific questions\n\nThis means you get comprehensive guidance without consuming your entire context window.\n\n## Troubleshooting\n\n### Skill Not Activating\n\nIf the skill doesn't seem to activate:\n- Make sure you're signed in to the same Claude account where you uploaded the skill\n- Try asking more specific questions about OpenRewrite recipe development\n- The skill name is \"openrewrite-recipe-writer\" - it activates based on conversation context\n\n### Upload Fails\n\nIf the upload script fails:\n- Verify your API key is correct: `cat anthropic.key`\n- Check that all skill files are present: `ls *.md *.java *.yml`\n- Ensure you have network connectivity\n- Check that `jq` is installed: `jq --version`\n\n### Updating Issues\n\nIf updates fail:\n- Verify `skill-id.txt` exists and contains a valid skill ID\n- Try deleting `skill-id.txt` and running the script again (creates a new skill)\n- Check the API response for specific error messages\n\n## Technical Details\n\n### Skill Metadata\n\n- **Name**: `openrewrite-recipe-writer` (lowercase, hyphens only)\n- **Display Title**: OpenRewrite Recipe Writing\n- **Type**: Custom skill\n- **Files**: 9 total (1 SKILL.md + 3 templates + 3 examples + 1 checklist + 1 README)\n- **Size**: ~85KB total\n\n### API Requirements\n\n- **API Version**: 2023-06-01\n- **Beta Header**: skills-2025-10-02\n- **File Format**: Multipart form data with directory prefix\n- **Directory Name**: Must match skill name (`openrewrite-recipe-writer`)\n\n## Privacy and Security\n\n- Your API key is stored locally in `anthropic.key` (gitignored)\n- Your skill ID is stored locally in `skill-id.txt` (gitignored)\n- Skills are private to your account - others cannot see or use your installed skills\n- No recipe code you discuss with Claude is shared or stored by the skill\n\n## Source Code and License\n\nThe OpenRewrite Recipe Writer skill is open source:\n\n- **Repository**: [openrewrite/rewrite-docs](https://github.com/openrewrite/rewrite-docs)\n- **Location**: `/claude-skill` directory\n- **License**: Apache 2.0\n\nYou can view the source code, contribute improvements, or customize it for your organization's needs.\n\n## Contributing\n\nTo improve the skill:\n\n1. Fork the [rewrite-docs repository](https://github.com/openrewrite/rewrite-docs)\n2. Make changes to files in the `claude-skill/` directory\n3. Test your changes by uploading to your account\n4. Submit a pull request\n\nImprovements we welcome:\n- New recipe examples\n- Additional templates\n- Updated best practices\n- Bug fixes in existing templates\n- Better explanations and documentation\n\n## Support\n\nFor help with the skill or recipe development:\n\n- **OpenRewrite Documentation**: [https://docs.openrewrite.org](https://docs.openrewrite.org)\n- **OpenRewrite Slack**: [Join here](https://join.slack.com/t/rewriteoss/shared_invite/zt-nj42n3ea-b~62rIHzb3Vo0E1APKCXEA)\n- **OpenRewrite Discord**: [https://discord.gg/xk3ZKrhWAb](https://discord.gg/xk3ZKrhWAb)\n- **Claude Skills Documentation**: [https://docs.claude.com/en/docs/agents-and-tools/agent-skills](https://docs.claude.com/en/docs/agents-and-tools/agent-skills)\n\n## See Also\n\n- [Recipe Development Environment](../authoring-recipes/recipe-development-environment.md)\n- [Writing a Java Refactoring Recipe](../authoring-recipes/writing-a-java-refactoring-recipe.md)\n- [Recipe Testing](../authoring-recipes/recipe-testing.md)\n- [Recipe Conventions and Best Practices](../authoring-recipes/recipe-conventions-and-best-practices.md)\n- [Declarative YAML Format Reference](./yaml-format-reference.md)\n",
    "github": {
      "stars": 49,
      "forks": 74,
      "owner": "openrewrite",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/61478321?v=4",
      "license": "Apache-2.0",
      "lastCommit": "2025-12-31T22:25:44Z",
      "createdAt": "2020-08-21T22:31:07Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:20.625Z"
  },
  {
    "id": "commands",
    "source": {
      "type": "github-search",
      "repo": "wcygan/dotfiles",
      "path": "config/claude/commands/new-skill.md",
      "url": "https://github.com/wcygan/dotfiles/blob/main/config/claude/commands/new-skill.md"
    },
    "frontmatter": {
      "description": "Interactive wizard to create new agent skills for Claude Code"
    },
    "content": "You are helping the user create a new agent skill for this project.\n\nOfficial reference: https://docs.claude.com/en/docs/claude-code/skills — always read this to understand proper skill formatting\n\nYou can see proper examples in https://github.com/anthropics/skills, such as https://raw.githubusercontent.com/anthropics/skills/refs/heads/main/skill-creator/SKILL.md\n\n# Overview\n\nAgent Skills are modular capabilities that Claude automatically uses when relevant to the user's request. Unlike slash commands (user-invoked with `/command`), skills are **model-invoked**—Claude autonomously decides when to activate them based on the skill's description matching the context.\n\nSkills use a progressive loading system:\n1. **Metadata** (always loaded): Name and description for discovery\n2. **Instructions** (loaded when triggered): Main SKILL.md guidance\n3. **Resources** (loaded as needed): Scripts, templates, reference docs\n\n# File Structure\n\nSkills are organized in directories:\n```\n.claude/skills/<skill-name>/\n├── SKILL.md (required - main instructions)\n├── REFERENCE.md (optional - detailed guidance)\n├── scripts/ (optional - executable helpers)\n└── templates/ (optional - file templates)\n```\n\n**Location options:**\n- `.claude/skills/` - Project-specific, shared with team (git-tracked)\n\n# Skill Creation Process\n\nFollow this interactive workflow to create a new skill:\n\n## STEP 1: Understand Requirements\n\nAsk the user these questions using the AskUserQuestion tool:\n\n1. **What capability should this skill provide?**\n   - Header: \"Capability\"\n   - Options:\n     - \"Code Analysis\" / \"Analyze code for patterns, quality, security issues\"\n     - \"Test Generation\" / \"Generate comprehensive test suites\"\n     - \"Documentation\" / \"Create docs, READMEs, API documentation\"\n     - \"Data Processing\" / \"Process files like PDFs, CSVs, JSON\"\n   - Multi-select: false\n\n2. **Skill scope - simple or advanced?**\n   - Header: \"Complexity\"\n   - Options:\n     - \"Simple (single SKILL.md)\" / \"Just instructions, no scripts or templates\"\n     - \"Advanced (with resources)\" / \"Include scripts, templates, or reference docs\"\n   - Multi-select: false\n\n3. **Should tools be restricted?**\n   - Header: \"Tool Access\"\n   - Options:\n     - \"Full access\" / \"Skill can use all available tools\"\n     - \"Restricted\" / \"Limit to specific tools (Read, Grep, etc.)\"\n   - Multi-select: false\n\n## STEP 2: Generate Skill Name\n\nBased on capability, suggest an appropriate name:\n- For \"Code Analysis\" → suggest: `code-quality-analyzer`\n- For \"Test Generation\" → suggest: `test-suite-generator`\n- For \"Documentation\" → suggest: `doc-generator`\n- For \"Data Processing\" → suggest: `data-processor`\n- For custom capability → suggest descriptive kebab-case name\n\n**Naming rules:**\n- Lowercase letters, numbers, hyphens only\n- Max 64 characters\n- Descriptive (what it does, not how)\n- Examples: `pdf-text-extractor`, `api-doc-generator`, `test-coverage-analyzer`\n\n## STEP 3: Craft the Description\n\nThe description is CRITICAL—it determines when Claude activates the skill. Must include:\n\n1. **What it does** (functionality)\n2. **When to use it** (trigger conditions)\n3. **Trigger keywords** (terms users might mention)\n\n**Template structure:**\n```\n[Primary function]. [Secondary capabilities]. Use when [trigger scenarios]. Keywords: [comma-separated terms].\n```\n\n**Examples of good descriptions:**\n\n✅ \"Extract text and tables from PDF files, fill forms, merge documents. Use when working with PDF files, documents, or form processing. Keywords: PDF, extract, document, form, merge\"\n\n✅ \"Generate comprehensive unit and integration tests with high coverage. Use when adding tests, improving coverage, or implementing TDD. Keywords: test, testing, coverage, unit test, integration test, TDD\"\n\n✅ \"Analyze code for security vulnerabilities, performance issues, and maintainability problems. Use when reviewing code or conducting security audits. Keywords: security, vulnerability, code review, audit, performance\"\n\n❌ \"Helps with documents\" (too vague)\n❌ \"Processes things\" (no triggers)\n\n## STEP 4: Generate Skill Content\n\nBased on the selected capability, create the skill using one of these templates:\n\n### Code Analysis Skill Template\n\n**SKILL.md:**\n```markdown\n---\nname: code-quality-analyzer\ndescription: Analyze code for security vulnerabilities, performance issues, code smells, and maintainability problems. Use when reviewing code, conducting security audits, or improving code quality. Keywords: security, vulnerability, code review, audit, performance, code smell, refactor\n---\n\n# Code Quality Analyzer\n\nAnalyzes code comprehensively across multiple quality dimensions.\n\n## Instructions\n\nWhen activated, perform the following analysis:\n\n### 1. Security Analysis\n- Input validation and sanitization\n- Authentication and authorization checks\n- Injection vulnerabilities (SQL, XSS, command injection)\n- Sensitive data exposure\n- Insecure dependencies\n\n### 2. Performance Analysis\n- Algorithmic complexity (O(n²) and worse)\n- Unnecessary loops or operations\n- Memory leaks or excessive allocations\n- Database query optimization opportunities\n- Caching opportunities\n\n### 3. Maintainability\n- Code smells (long methods, large classes, duplicate code)\n- Naming conventions and clarity\n- Comment quality and documentation\n- SOLID principle violations\n- Separation of concerns\n\n### 4. Testing\n- Test coverage gaps\n- Edge cases not covered\n- Flaky test patterns\n- Missing error scenario tests\n\n## Output Format\n\nPresent findings organized by severity:\n\n**🔴 Critical Issues** (security vulnerabilities, major bugs)\n**🟡 Warnings** (performance issues, maintainability concerns)\n**🟢 Suggestions** (improvements, best practices)\n\nFor each issue:\n- **File:Line**: `src/auth.ts:45`\n- **Issue**: Clear description\n- **Impact**: Why it matters\n- **Fix**: Code example showing correction\n\n## Project Context\n\nThis is a Deno + Fresh + Preact project. Apply Deno/Fresh-specific best practices.\n```\n\n### Test Generation Skill Template\n\n**SKILL.md:**\n```markdown\n---\nname: test-suite-generator\ndescription: Generate comprehensive unit and integration tests with high coverage. Use when adding tests, improving test coverage, implementing TDD, or ensuring code quality. Keywords: test, testing, coverage, unit test, integration test, TDD, test-driven\n---\n\n# Test Suite Generator\n\nCreates comprehensive test suites following TDD best practices.\n\n## Instructions\n\n### 1. Analyze Code Under Test\n- Identify all public interfaces and functions\n- Map dependencies and side effects\n- List edge cases and error conditions\n- Check existing test coverage\n\n### 2. Generate Test Structure\n\n**For Deno projects:**\n```typescript\nimport { assertEquals, assertThrows } from \"@std/assert\";\nimport { describe, it } from \"@std/testing/bdd\";\n\ndescribe(\"ComponentName\", () => {\n  it(\"should handle normal case\", () => {\n    // Arrange\n    // Act\n    // Assert\n  });\n\n  it(\"should handle edge case\", () => {\n    // Test edge case\n  });\n\n  it(\"should throw on invalid input\", () => {\n    assertThrows(() => {\n      // Test error condition\n    });\n  });\n});\n```\n\n### 3. Test Coverage Goals\n- 90%+ code coverage\n- All public APIs tested\n- All error paths tested\n- Edge cases covered\n- Integration points mocked appropriately\n\n### 4. Test Organization\n- Group related tests with describe blocks\n- Use table-driven tests for multiple similar cases\n- One assertion concept per test\n- Descriptive test names (behavior, not implementation)\n\n## Output\n\nCreate test files following project structure:\n- Adjacent to source: `component.test.ts`\n- Or in `/tests/` directory matching structure\n\nUse Write tool to create test files with complete test suite.\n```\n\n### Documentation Generation Skill Template\n\n**SKILL.md:**\n```markdown\n---\nname: documentation-generator\ndescription: Generate comprehensive documentation including READMEs, API docs, JSDoc comments, and architecture guides. Use when documenting code, creating READMEs, or writing API documentation. Keywords: documentation, docs, README, API docs, JSDoc, comments, guide\n---\n\n# Documentation Generator\n\nCreates clear, comprehensive documentation for code and systems.\n\n## Instructions\n\n### 1. Determine Documentation Type\n\n**Code Documentation (JSDoc):**\n- Function/method signatures\n- Parameter descriptions with types\n- Return value descriptions\n- Usage examples for complex functions\n- Error conditions\n\n**README Files:**\n- Purpose (1-2 sentences)\n- Quick start (single command if possible)\n- Key commands/usage\n- Project structure (if complex)\n- Keep under 50 lines total\n\n**API Documentation:**\n- Endpoint descriptions\n- Request/response schemas\n- Authentication requirements\n- Example requests and responses\n- Error codes and meanings\n\n**Architecture Docs:**\n- System overview diagram (ASCII)\n- Component responsibilities\n- Data flow\n- Key architectural decisions\n\n### 2. Documentation Standards\n\n- **Clarity**: Write for developers unfamiliar with the code\n- **Examples**: Include concrete usage examples\n- **Conciseness**: Be thorough but not verbose\n- **Format**: Use proper Markdown formatting\n- **Maintenance**: Include version/date information\n\n### 3. Project-Specific Guidelines\n\nFrom CLAUDE.md:\n- README max 50 lines\n- Essential sections only\n- Single quick-start command when possible\n- No excessive emojis\n\n## Output\n\nUse Write or Edit tools to create/update documentation files.\n```\n\n### Data Processing Skill Template\n\n**SKILL.md:**\n```markdown\n---\nname: data-file-processor\ndescription: Process and transform data files including JSON, CSV, YAML, and structured text. Use when working with data files, transforming formats, or extracting information. Keywords: JSON, CSV, YAML, data, parse, transform, extract\nallowed-tools: Read, Write, Bash, Grep\n---\n\n# Data File Processor\n\nProcesses various data file formats with transformations and analysis.\n\n## Instructions\n\n### 1. Identify File Format\n- JSON: Parse and validate structure\n- CSV: Handle headers, delimiters, escaping\n- YAML: Parse configuration files\n- Text: Pattern extraction and transformation\n\n### 2. Common Operations\n\n**Validation:**\n- Schema validation\n- Format checking\n- Data integrity verification\n\n**Transformation:**\n- Format conversion (JSON ↔ CSV ↔ YAML)\n- Field mapping and renaming\n- Data filtering and aggregation\n- Normalization\n\n**Extraction:**\n- Specific field extraction\n- Pattern matching\n- Aggregation and statistics\n\n### 3. Processing Approach\n\n```bash\n# Use jq for JSON\njq '.field | map(select(.status == \"active\"))' input.json\n\n# Use standard tools for CSV\nawk -F',' '{print $1,$3}' data.csv\n\n# YAML processing\nyq eval '.services.*.image' docker-compose.yml\n```\n\n### 4. Output Format\n\n- Preserve data integrity\n- Clear transformation logs\n- Validate output format\n- Provide summary statistics\n\n## Error Handling\n\n- Invalid format detection\n- Missing field handling\n- Data type mismatches\n- Encoding issues\n```\n\n## STEP 5: Create Skill Directory and Files\n\n1. Create directory: `.claude/skills/<skill-name>/`\n2. Create `SKILL.md` with the generated template\n3. If advanced scope:\n   - Create `REFERENCE.md` for detailed docs\n   - Create `scripts/` for helper scripts\n   - Create `templates/` for file templates\n4. Customize based on:\n   - Project tech stack (deno.json, CLAUDE.md)\n   - Existing patterns in codebase\n   - User's specific requirements\n   - Tool restrictions if specified\n\n## STEP 6: Add Tool Restrictions (if needed)\n\nIf user chose \"Restricted\" tools, add to frontmatter:\n\n```yaml\n---\nname: skill-name\ndescription: Description here\nallowed-tools: Read, Grep, Glob, Bash(jq:*), Bash(yq:*)\n---\n```\n\nCommon tool combinations:\n- **Read-only analysis**: `Read, Grep, Glob`\n- **Data processing**: `Read, Write, Bash(jq:*), Bash(yq:*)`\n- **Code review**: `Read, Grep, Glob`\n- **File generation**: `Read, Write, Grep`\n\n## STEP 7: Confirm and Test\n\nAfter creating the skill:\n\n1. Show the created file structure\n2. Explain the skill's description and triggers\n3. Provide example user requests that would activate it:\n   - \"Can you review this code for security issues?\" → code-quality-analyzer\n   - \"I need tests for this module\" → test-suite-generator\n   - \"Generate a README for this project\" → documentation-generator\n   - \"Parse this JSON file and extract the IDs\" → data-file-processor\n4. Suggest testing with a relevant request\n\n# Best Practices for Skills\n\nWhen generating skills, ensure:\n\n✅ **Descriptive names** - What it does, not how (max 64 chars)\n✅ **Rich descriptions** - Functionality + triggers + keywords (max 1024 chars)\n✅ **Clear instructions** - Step-by-step guidance for Claude\n✅ **Project context** - Reference tech stack, conventions, CLAUDE.md\n✅ **Structured output** - Specify format, tools, file locations\n✅ **Examples** - Show expected patterns and formats\n✅ **Tool specification** - Mention which tools to use\n✅ **Progressive disclosure** - Main SKILL.md + optional REFERENCE.md\n\n# Skill Activation\n\nSkills activate when user requests match the description. Good trigger phrases:\n\n- \"Review this code\" → code-quality-analyzer\n- \"Add tests\" / \"Improve coverage\" → test-suite-generator\n- \"Document this\" / \"Create README\" → documentation-generator\n- \"Parse this JSON\" / \"Transform CSV\" → data-file-processor\n\n---\n\n**Now execute the workflow above, starting with STEP 1 to gather requirements from the user.**\n",
    "github": {
      "stars": 182,
      "forks": 16,
      "owner": "wcygan",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/32029716?v=4",
      "lastCommit": "2026-01-02T17:58:01Z",
      "createdAt": "2023-03-10T04:06:04Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:21.995Z"
  },
  {
    "id": "pages",
    "source": {
      "type": "github-search",
      "repo": "codekiln/logseq-encode-garden",
      "path": "pages/Claude Code___Skill.md",
      "url": "https://github.com/codekiln/logseq-encode-garden/blob/main/pages/Claude Code___Skill.md"
    },
    "frontmatter": {},
    "content": "alias:: [[Claude Code Skills]]\n",
    "github": {
      "stars": 4,
      "forks": 0,
      "owner": "codekiln",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/140930?v=4",
      "lastCommit": "2026-01-03T00:11:35Z",
      "createdAt": "2025-01-30T11:59:26Z",
      "topics": [
        "ai",
        "logseq",
        "pkm",
        "python"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:22.633Z"
  },
  {
    "id": "posts",
    "source": {
      "type": "github-search",
      "repo": "Dustyposa/dustyposa.github.io",
      "path": "src/data/blog/_posts/2025-10-claude-code-skill.md",
      "url": "https://github.com/Dustyposa/dustyposa.github.io/blob/main/src/data/blog/_posts/2025-10-claude-code-skill.md"
    },
    "frontmatter": {
      "title": "Claude Code Skill",
      "description": "在 Claude 的生态系统中,**Skill** 和 **MCP (Model Context Protocol)** 是两种为 AI Agent 提供额外能力的机制。<cite/>虽然它们都旨在扩展 Agent 的功能,但它们在设计理念、实现方式和适用场景上有着本质的区别。<cite/>",
      "pubDatetime": "2025-10-17T10:29:00Z",
      "modDatetime": "2025-10-17T10:29:00Z",
      "tags": "",
      "draft": "false",
      "featured": "false"
    },
    "content": "# Skill 与 MCP:Claude Agent 能力扩展的两种方式\n\n## 概述\n\n在 Claude 的生态系统中,**Skill** 和 **MCP (Model Context Protocol)** 是两种为 AI Agent 提供额外能力的机制。<cite/>虽然它们都旨在扩展 Agent 的功能,但它们在设计理念、实现方式和适用场景上有着本质的区别。<cite/>\n\n## 核心定位\n\n### Skill:内部能力扩展\n\n**Skill 是 Claude 专有的模块化扩展系统**,通过动态加载专业化知识和工作流程来提升 Claude 在特定任务上的表现。<cite ref=\"README.md:0-1\"/>\n\nSkill 解决的核心问题是:**如何让 Claude 在特定领域或任务上表现得像一个专家**。<cite/>它通过将指令、脚本和资源注入到 Claude 的上下文中,让 Claude \"学习\"如何完成特定任务。<cite ref=\"skill-creator/SKILL.md:11-22\"/>\n\n### MCP:外部服务集成\n\n**MCP 是一个开放的标准化协议**,定义了 LLM 如何与外部系统进行结构化交互。<cite ref=\"mcp-builder/SKILL.md:7-10\"/>\n\nMCP 解决的核心问题是:**如何让 LLM 安全、可靠地调用外部 API 和服务**。<cite/>它提供了工具发现、调用和响应的标准化接口。<cite ref=\"mcp-builder/reference/mcp_best_practices.md:436-441\"/>\n\n## 关键区别对比\n\n| 维度 | Skill | MCP |\n|------|-------|-----|\n| **平台依赖** | Claude 专有 | 开放协议,任何 LLM 可用 |\n| **抽象层级** | 应用层 - 教 Claude \"怎么做\" | 协议层 - 定义 LLM \"如何调用\" |\n| **执行主体** | Claude 自己 | 外部 MCP 服务器 |\n| **知识位置** | Claude 上下文内 | 外部服务器 |\n| **工作方式** | 阅读→理解→执行 | 调用→等待→接收结果 |\n| **状态管理** | 无状态(每次重新加载) | 无状态(独立请求-响应) |\n| **复用性** | 通常是单一 Claude 实例使用 | 设计为多客户端复用 |\n| **定制程度** | 高度定制(公司/个人专属) | 标准化(通用工具) |\n\n## 上下文管理差异\n\n### MCP:最小化上下文占用\n\nMCP Tool 只将**工具元数据和执行结果**传递给 Agent:<cite/>\n\n- 上下文中只包含工具的名称、描述和输入 schema<cite ref=\"mcp-builder/reference/mcp_best_practices.md:444-464\"/>\n- 工具调用后,只返回最终结果(文本或结构化数据)<cite/>\n- **不会**将 MCP 服务器的内部实现加载到上下文中<cite/>\n\n**上下文占用示例**:\n```\n上下文中的内容:\n├── Tool 元数据: \"slack_send_message\" 工具的描述和参数 schema\n└── Tool 调用结果: \"Message sent successfully\"\n```\n\n### Skill:渐进式上下文加载\n\nSkill 采用三层渐进式加载机制:<cite ref=\"skill-creator/SKILL.md:75-83\"/>\n\n1. **元数据层**(始终在上下文):约 100 词的 `name` 和 `description`\n2. **SKILL.md 正文**(触发时加载):完整的工作流程指令,建议 <5k 词\n3. **打包资源**(按需加载):\n   - `references/` - 文档和参考资料<cite ref=\"skill-creator/SKILL.md:56-65\"/>\n   - `scripts/` - 可执行脚本<cite ref=\"skill-creator/SKILL.md:47-54\"/>\n   - `assets/` - 输出文件和模板<cite ref=\"skill-creator/SKILL.md:67-74\"/>\n\n这种设计让 Skill 能够将**完整的指令、工作流程和领域知识**加载到 Claude 的上下文中,使 Claude 能够深度理解并执行复杂任务。<cite/>\n\n**上下文占用示例** (以 `internal-comms` skill 为例):\n```\n上下文中的内容:\n├── 元数据(始终): name=\"internal-comms\", description=\"...\"\n├── SKILL.md 正文(触发时): \n│   ├── 何时使用此 skill\n│   ├── 如何使用此 skill\n│   └── 工作流程指导\n└── References(按需):\n    ├── examples/3p-updates.md\n    ├── examples/company-newsletter.md\n    └── examples/faq-answers.md\n```\n\n## 实际应用场景\n\n### Skill 的典型应用\n\n- **创意设计**(`canvas-design`):教 Claude 创建视觉艺术<cite/>\n- **算法艺术**(`algorithmic-art`):生成可运行的 p5.js HTML 艺术作品<cite ref=\"algorithmic-art/SKILL.md:301-343\"/>\n- **内部沟通**(`internal-comms`):教 Claude 按公司格式写状态报告<cite/>\n- **品牌指南**(`brand-guidelines`):教 Claude 应用公司品牌规范<cite/>\n\n### MCP 的典型应用\n\n- 让 LLM 调用 Slack API 发送消息\n- 让 LLM 查询 GitHub API 获取代码信息\n- 让 LLM 操作 Jira API 管理任务\n\n## 概念对应关系\n\n虽然 Skill 和 MCP 都有类似名称的概念,但它们的含义完全不同:<cite/>\n\n### Skill 的组成\n\n- **指令(Instructions)**:SKILL.md 中的 Markdown 内容,提供工作流程指导<cite ref=\"README.md:0-1\"/>\n- **脚本(Scripts)**:`scripts/` 目录中的可执行代码<cite ref=\"skill-creator/SKILL.md:46-53\"/>\n- **资源(Resources)**:\n  - `references/` - 参考文档<cite ref=\"skill-creator/SKILL.md:55-64\"/>\n  - `assets/` - 输出文件<cite ref=\"skill-creator/SKILL.md:66-73\"/>\n\n### MCP 的组成\n\n- **Tools(工具)**:MCP 服务器暴露的可执行函数<cite ref=\"mcp-builder/reference/mcp_best_practices.md:436-441\"/>\n- **Resources(资源)**:通过 URI 模板访问的数据接口<cite ref=\"mcp-builder/reference/python_mcp_server.md:553-574\"/>\n- **Prompts(提示)**:MCP 协议中的提示管理功能<cite/>\n\n**重要提示**:这些概念**不是一一对应的关系**,它们在各自系统中有完全不同的含义和用途。<cite/>\n\n## 执行方式的本质区别\n\n### Skill:Claude 自己阅读并执行\n\n**Skill 的工作流程**:<cite/>\n1. 将指令、脚本和资源加载到 Claude 的上下文中<cite ref=\"skill-creator/SKILL.md:12-15\"/>\n2. Claude 阅读这些内容后,按照指导**自己完成任务**<cite ref=\"skill-creator/SKILL.md:78-83\"/>\n3. 执行主体是 **Claude 本身**<cite/>\n\n**实际示例** - `algorithmic-art` skill:\n- 第一步:创建算法哲学(.md 文件)\n- 第二步:用 p5.js 代码实现这个哲学(.html + .js 文件)\n- 最终产出:用户可以在浏览器中打开并交互的艺术作品<cite ref=\"algorithmic-art/SKILL.md:337-343\"/>\n\n### MCP:调用外部服务执行\n\n**MCP 的工作流程**:<cite/>\n1. MCP 服务器提供可调用的工具(tools)<cite ref=\"mcp-builder/reference/mcp_best_practices.md:437-441\"/>\n2. Claude 通过协议调用这些工具,**外部服务执行操作**<cite/>\n3. 执行主体是 **MCP 服务器**<cite/>\n\n**实际示例** - Slack MCP 服务器:\n```\n输入: slack_send_message(channel=\"general\", text=\"Hello\")\n处理: MCP 服务器调用 Slack API\n输出: {\"status\": \"success\", \"message_id\": \"123\"}\n```\n\n## 上下文与状态的关系\n\n### 理解\"上下文\"和\"状态\"的区别\n\n**上下文(Context)** - Skill 确实利用:<cite/>\n- SKILL.md 正文加载到 Claude 的上下文窗口中<cite ref=\"skill-creator/SKILL.md:81-82\"/>\n- `references/` 文件按需加载到上下文<cite ref=\"skill-creator/SKILL.md:56-65\"/>\n- Claude 阅读这些上下文内容后,理解如何执行任务<cite/>\n\n**状态(State)** - Skill 不保留:<cite/>\n- 每次触发 Skill 时,重新加载 SKILL.md<cite/>\n- 不记忆上一次任务的执行结果<cite/>\n- 不累积历史信息<cite/>\n\n### 为什么这样设计是合理的\n\n1. **上下文是临时的工作空间**:用于当前任务的知识和指令,任务完成后可以释放<cite/>\n2. **无状态保证一致性**:每次使用 Skill 都基于相同的指令,确保行为可重复和可靠<cite/>\n3. **类比函数调用**:就像函数可以访问参数(上下文),但不保留局部变量(状态)<cite/>\n\n## Skill 的设计原则\n\n### 自包含性\n\n**Skill 应该是自包含的,不依赖 Claude 当前会话中已有的上下文**:<cite/>\n\n1. **可移植性**:Skill 可能在不同的会话中被触发,不能假设之前的对话内容<cite/>\n2. **独立性**:SKILL.md 应该包含**所有必要的信息**来完成任务<cite ref=\"skill-creator/SKILL.md:156-174\"/>\n3. **可预测性**:无论何时触发,Skill 的行为应该一致<cite/>\n\n### 写作风格\n\n文档明确要求使用**客观、指令性的语言**:<cite/>\n\n> 使用**命令式/不定式形式**(动词优先的指令),而不是第二人称。使用客观、指令性的语言(例如\"要完成 X,执行 Y\"而不是\"你应该做 X\"或\"如果你需要做 X\")。<cite ref=\"skill-creator/SKILL.md:166-168\"/>\n\n这种写作风格确保 Skill 是**自我描述的**,而不是假设 Claude 已经在某个特定的对话上下文中。<cite/>\n\n## Skill 与 MCP 的协作\n\n### Skill 可以调用 MCP\n\nSkill 和 MCP **不是互斥的,而是可以配合使用的**。<cite/>\n\nSkill 的脚本可以作为 MCP 客户端来调用 MCP 服务器。<cite/>例如,`mcp-builder/scripts/connections.py` 就实现了 MCP 客户端连接逻辑,可以连接到 MCP 服务器、列出可用工具并调用工具。<cite/>\n\n### 何时使用 MCP vs 直接脚本\n\n**使用 MCP 的场景**:<cite/>\n- 多个 LLM 客户端需要访问同一服务\n- 需要标准化的工具接口和错误处理\n- 希望工具可以被其他 MCP 客户端复用\n- 需要跨平台的标准化接口\n\n**直接使用脚本的场景**:<cite/>\n- 操作是 Skill 专有的,不需要被其他系统调用\n- 脚本逻辑简单,直接执行即可\n- 不需要复杂的工具发现机制\n- 不需要跨平台的标准化接口\n\n## 能力包含关系\n\n### Skill 理论上可以实现 MCP 的所有功能\n\n**从能力角度看,Skill 理论上可以完成 MCP 能完成的所有事情**:<cite/>\n\n1. **直接调用外部 API**:Skill 的脚本可以直接使用 HTTP 客户端调用 Slack、GitHub、Jira 等 API<cite ref=\"skill-creator/SKILL.md:46-54\"/>\n2. **作为 MCP 客户端**:Skill 的脚本甚至可以作为 MCP 客户端来调用 MCP 服务器<cite/>\n\n### 设计目标的差异\n\n虽然 Skill **能做** MCP 能做的事,但它们的**设计目标**不同:<cite/>\n\n**MCP 的设计目标**:标准化和复用<cite/>\n- 开放协议:任何 LLM 都可以使用<cite ref=\"mcp-builder/SKILL.md:8-10\"/>\n- 标准化接口:工具发现、调用都有统一规范<cite ref=\"mcp-builder/reference/mcp_best_practices.md:439-441\"/>\n- 独立服务:MCP 服务器是独立运行的进程,可以被多个客户端同时使用<cite/>\n\n**Skill 的设计目标**:Claude 专属定制<cite/>\n- Claude\n\n",
    "github": {
      "stars": 4,
      "forks": 0,
      "owner": "Dustyposa",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/27180793?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-10T08:49:37Z",
      "createdAt": "2020-04-06T02:30:39Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:23.279Z"
  },
  {
    "id": "",
    "source": {
      "type": "github-search",
      "repo": "justinabrahms/confluence-md",
      "path": "CLAUDE_SKILL.md",
      "url": "https://github.com/justinabrahms/confluence-md/blob/main/CLAUDE_SKILL.md"
    },
    "frontmatter": {},
    "content": "# Confluence Document Reader\n\nRead and search Confluence pages as markdown using the confluence-md CLI tool.\n\n## What this skill does\n\nThis skill allows Claude to:\n- Search for Confluence pages by query\n- Fetch specific Confluence pages by URL\n- Filter searches by space or creator\n- Return content as clean markdown for easy reading and analysis\n\n## When to use this skill\n\nUse this skill when the user asks to:\n- Read a Confluence page\n- Search for Confluence documentation\n- Find pages about a specific topic\n- Get content from a Confluence URL\n- Review their own Confluence pages (with --mine flag)\n\n## Usage\n\nThe skill uses the `confluence-md` command-line tool installed on the system.\n\n### Search for pages\n\n```bash\nconfluence-md search \"query\" [flags]\n```\n\nCommon flags:\n- `--lucky` - Fetch the first result immediately\n- `--mine` - Only search pages you created\n- `--space SPACE` - Limit search to specific space\n- `--limit N` - Maximum number of results (default 10)\n- `--index N` - Fetch specific result by number\n\n### Fetch a specific page\n\n```bash\nconfluence-md fetch <url>\n```\n\n## Examples\n\n```bash\n# Search and get first result about API documentation\nconfluence-md search \"API documentation\" --lucky\n\n# Find your own proposals\nconfluence-md search \"proposal\" --mine --limit 5\n\n# Search in specific space\nconfluence-md search \"onboarding\" --space HR --lucky\n\n# Get specific page\nconfluence-md fetch https://company.atlassian.net/wiki/spaces/ENG/pages/123456/Page+Title\n```\n\n## Output\n\nAll commands return markdown-formatted content that can be directly analyzed and discussed.\n\n## Requirements\n\n- The `confluence-md` binary must be in your PATH\n- Configuration must be set up in `~/.config/confluence-md/config.yaml` or via environment variables\n- Valid Confluence credentials (email and API token)\n\n## Tips\n\n- Use `--lucky` when you want the most relevant result quickly\n- Use `--mine` to find pages you authored\n- The tool converts HTML to clean markdown automatically\n- You can pipe output to other tools for further processing\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "justinabrahms",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/3853?v=4",
      "lastCommit": "2025-12-09T15:36:58Z",
      "createdAt": "2025-11-02T19:19:06Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:23.937Z"
  },
  {
    "id": "spec",
    "source": {
      "type": "github-search",
      "repo": "unit-mesh/autodev-docs",
      "path": "docs/spec/claude-skill.md",
      "url": "https://github.com/unit-mesh/autodev-docs/blob/main/docs/spec/claude-skill.md"
    },
    "frontmatter": {
      "layout": "default",
      "title": "Claude Skill",
      "nav_order": "2",
      "parent": "Spec Driven Development",
      "permalink": "/spec/github-spec-kit"
    },
    "content": "## Claude Skills 集成\n\n### 什么是 Claude Skills？\n\n[Anthropic Claude Skills](https://github.com/anthropics/skills) 是包含指令、脚本和资源的文件夹，Claude 动态加载以提高专业任务的性能。\n\n**主要特征：**\n- 自包含的技能模块\n- 用于元数据的 YAML 前置元数据\n- 用于 AI 代理的 Markdown 指令\n- 可包含脚本、模板和资源\n\n### AutoDev 如何集成 Claude Skills\n\nAutoDev 从多个位置发现 Claude Skills：\n\n**发现位置：**\n\n1. **项目级技能**：项目根目录中包含 `SKILL.md` 的目录\n2. **用户级技能**：`~/.claude/skills/` 目录\n3. **嵌套技能**：技能目录中的子目录\n\n**发现过程：**\n\n1. AutoDev 扫描配置的位置查找 `SKILL.md` 文件\n2. 解析 YAML 前置元数据获取技能元数据\n3. 将技能注册为 `/skill.<名称>` 命令\n4. 提供带描述的智能提示\n\n**技能结构：**\n\n```\nmy-skill/\n├── SKILL.md              # 主技能定义\n├── examples/             # 示例文件（可选）\n├── templates/            # 模板文件（可选）\n└── scripts/              # 辅助脚本（可选）\n```\n\n### 技能定义格式\n\n```markdown\n---\nname: my-skill-name\ndescription: 清晰描述此技能的功能和使用时机\nversion: 1.0.0\nauthor: 你的名字\ntags:\n  - 类别1\n  - 类别2\n---\n\n# 我的技能名称\n\n## 目的\n\n[解释此技能的作用和使用时机]\n\n## 指令\n\n[AI 代理的详细指令]\n\n## 示例\n\n### 示例 1：基本用法\n```\n输入：...\n输出：...\n```\n\n### 示例 2：高级用法\n```\n输入：...\n输出：...\n```\n\n## 指南\n\n- 指南 1：...\n- 指南 2：...\n- 指南 3：...\n\n## 限制\n\n- 限制 1：...\n- 限制 2：...\n```\n\n### Anthropic 提供的示例技能\n\n#### 文档技能\n\n```devin\n# 创建带修订跟踪的 Word 文档\n/skill.docx 创建项目提案，包含：\n- 执行摘要\n- 技术方法\n- 时间表和里程碑\n- 预算明细\n启用修订跟踪以供审查\n\n# 从 PDF 提取数据\n/skill.pdf 从 invoice.pdf 提取所有表单字段并输出为 JSON\n\n# 创建 PowerPoint 演示文稿\n/skill.pptx 创建关于我们 Q4 结果的 10 张幻灯片演示，包含：\n- 标题幻灯片\n- 执行摘要\n- 收入图表\n- 关键指标\n- 未来展望\n\n# 分析 Excel 电子表格\n/skill.xlsx 分析 sales-data.xlsx 并创建：\n- 按地区的数据透视表\n- 月度趋势图\n- 前 10 产品表\n```\n\n#### 开发技能\n\n```devin\n# 构建 React 工件\n/skill.artifacts-builder 创建仪表板，包含：\n- 实时数据可视化\n- 使用 Tailwind 的响应式布局\n- 深色模式支持\n- shadcn/ui 组件\n\n# 测试 Web 应用\n/skill.webapp-testing 测试 http://localhost:3000 的登录流程：\n- 验证表单验证\n- 测试成功登录\n- 测试错误处理\n- 检查响应式设计\n\n# 创建 MCP 服务器\n/skill.mcp-builder 为 GitHub API 创建 MCP 服务器，功能包括：\n- 列出仓库\n- 创建问题\n- 管理拉取请求\n- 处理身份验证\n```\n\n#### 创意技能\n\n```devin\n# 生成算法艺术\n/skill.algorithmic-art 使用以下方式创建生成艺术：\n- 流场算法\n- 1000 个粒子的粒子系统\n- 种子随机性（种子：42）\n- 调色板：蓝色和紫色\n\n# 设计画布\n/skill.canvas-design 为我们的产品发布创建极简海报：\n- 尺寸：1920x1080\n- 品牌颜色\n- 简洁排版\n- 导出为 PNG 和 PDF\n```\n",
    "github": {
      "stars": 4,
      "forks": 5,
      "owner": "unit-mesh",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/129267571?v=4",
      "lastCommit": "2025-11-28T09:20:07Z",
      "createdAt": "2025-04-22T05:32:29Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:24.893Z"
  },
  {
    "id": "templates",
    "source": {
      "type": "github-search",
      "repo": "ryancnelson/deli-gator",
      "path": "templates/SKILL.md.template",
      "url": "https://github.com/ryancnelson/deli-gator/blob/main/templates/SKILL.md.template"
    },
    "frontmatter": {},
    "content": "# SERVICE Delegation Skill\n\n## Overview\n\nThis skill implements the delegation pattern for SERVICE queries, keeping the main Claude context clean by offloading SERVICE-specific operations to a specialized sub-agent.\n\n## Keywords (Triggers Delegation)\n\n- service\n- resource\n- item\n- query\n- IDENTIFIER-PATTERN (e.g., PROJ-####)\n\n## Pattern\n\nWhen user mentions any keyword above:\n\n1. **DO NOT** handle the query directly\n2. **DO NOT** load SERVICE API documentation\n3. **DO NOT** construct API calls yourself\n\nInstead:\n\n1. Read `AGENT-INSTRUCTIONS.md` from this skill directory\n2. Delegate to sub-agent via Task tool\n3. Present results to user\n\n## Delegation Steps\n\n### Step 1: Read Agent Instructions\n\n```\nRead(\"path/to/skills/delegating-to-service-agent/AGENT-INSTRUCTIONS.md\")\n```\n\n### Step 2: Invoke Task Tool\n\n```\nTask(\n  subagent_type: \"general-purpose\",\n  description: \"Query SERVICE\",\n  prompt: \"<paste full agent instructions here>\n\nUSER REQUEST: <user's exact request>\n\nReturn clean formatted results.\"\n)\n```\n\n### Step 3: Present Results\n\nShow the sub-agent's response to the user without exposing delegation mechanics.\n\n## Example\n\n**User says:** \"show me my service items\"\n\n**You do:**\n\n1. Recognize keyword: \"service\"\n2. Read AGENT-INSTRUCTIONS.md\n3. Delegate via Task tool\n4. Present: \"Found 5 items: ITEM-1 (status), ITEM-2 (status), ...\"\n\n## What You Should NEVER Do\n\n❌ Run `~/bin/service-*` commands yourself\n❌ Construct API calls manually\n❌ Try to authenticate with SERVICE API\n❌ Load SERVICE API documentation\n\n**Those resources are for the SUB-AGENT, not for you!**\n\n## Benefits\n\n✅ Your context stays clean (< 1KB vs 5-10KB)\n✅ Sub-agent uses cheaper model\n✅ Faster responses\n✅ Scalable pattern (reuse for other services)\n\n## Files in This Skill\n\n- `SKILL.md` (this file) - When and how to delegate\n- `AGENT-INSTRUCTIONS.md` - Complete knowledge for sub-agent\n- `USAGE-EXAMPLE.md` - Example delegation flow\n\n## Shell Wrappers Used by Sub-Agent\n\n```bash\n~/bin/service-list          # List items\n~/bin/service-show          # Show specific item\n~/bin/service-search        # Search items\n~/bin/service-create        # Create new item\n~/bin/service-update        # Update existing item\n```\n\n## Testing\n\nVerify this skill works:\n\n1. Start fresh Claude session\n2. Say: \"show me my service items\"\n3. Verify delegation happens automatically\n4. Verify sub-agent uses correct wrapper\n5. Verify results are accurate\n6. Verify main context < 1KB\n\n## Troubleshooting\n\n**Delegation didn't happen?**\n- Check startup doc is loaded (should have ⛔ warnings)\n- Verify keywords are in user query\n- Check if user explicitly asked you NOT to delegate\n\n**Sub-agent chose wrong tool?**\n- Review decision tree in AGENT-INSTRUCTIONS.md\n- Add more specific patterns\n- Test with different phrasings\n\n**Results are inaccurate?**\n- Test shell wrappers manually: `~/bin/service-list`\n- Check authentication tokens\n- Verify API responses\n\n## Related Documentation\n\n- **Startup Doc:** `config/startup-docs/0X-service-delegation.md` (⛔ loaded every session)\n- **Concept File:** `config/concepts/service-access.md` (STOP warnings)\n- **Shell Wrappers:** `~/bin/service-*`\n\n---\n\n**Created:** YYYY-MM-DD\n**Status:** Active\n**Pattern:** Delegation Agent v1.0\n",
    "github": {
      "stars": 32,
      "forks": 5,
      "owner": "ryancnelson",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/347171?v=4",
      "license": "NOASSERTION",
      "lastCommit": "2025-10-23T16:28:28Z",
      "createdAt": "2025-10-15T17:24:40Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:25.515Z"
  },
  {
    "id": "docs",
    "source": {
      "type": "github-search",
      "repo": "klaudworks/universal-skills",
      "path": "docs/creating-a-skill.md",
      "url": "https://github.com/klaudworks/universal-skills/blob/main/docs/creating-a-skill.md"
    },
    "frontmatter": {},
    "content": "A short, actionable guide to how I use skills.\n\nHigh-level:\n\n1. Work on a problem.\n2. Recognize grunt work.\n3. Load the Skill Creator meta-skill to create a new skill.\n4. Write a skill to (semi-)automate your problem.\n5. Profit.\n\nExample: Publishing an npm package\n\nI’ll walk you through the process I used to create the simple npm publisher skill in [.agent/skills/npm-publisher/SKILL.md](../.agent/skills/npm-publisher/SKILL.md).\n\n### Prerequisites\n\nInstall Anthropic’s Skill Creator skill. It’s a skill that helps you create new skills.\n\n```bash\nnpx universal-skills install --repo https://github.com/anthropics/skills --repo-dir skill-creator/ --local-dir ~/.agent/skills\n```\n\nI recommend installing the skill-creator skill into `~/.agent/skills`. If you also want to work with Claude Code’s native skills feature, store the skill in `~/.claude/skills`.\n\n### Identify a problem and solve it once\n\nI don’t usually publish npm packages. However, TypeScript provides the best MCP implementation, so I chose it for this project. To publish my first npm package, I prompted my coding agent and provided it with external docs. After completing this task the first time, my agent’s session contained all the knowledge needed to publish npm packages. A few examples of knowledge I want my agent to have when I repeat the process:\n\n- The overall steps involved to publish an npm package.\n- The agent should not actually publish the package; that’s done by a GitHub workflow.\n- The agent should run npm version patch/minor/major to bump the version.\n- The agent should not add annoying attributions for my AI agent to my commit messages.\n\n### Profit\n\nFrom now on, I can use the npm-publisher skill. Its metadata is automatically loaded into my context. Unlike MCP servers, a skill usually takes fewer than 100 tokens, which is negligible compared to the GitHub MCP server using ~10k tokens.\n\n---\n\n```yaml\n---\nname: npm-publisher\ndescription: >\n  Use this skill when the user wants to publish, release, or deploy an npm package.\n  It handles the complete release workflow, including staging changes, committing,\n  version bumping, tagging, and pushing to trigger automated CI/CD publishing.\n---\n```\n\nWhenever I write something like:\n\n- “Publish the npm package”\n- “Release a new version”\n- “Deploy to npm”\n- “Create a new release”\n- “Bump the version and publish”\n\nThe agent can infer, based on the description in its context, that it should load the entire skill. At this point, my session contains all the knowledge needed to publish an npm package.\n\n### Evolve the skill\n\n1. Manually review the first draft of the skill and fix it where necessary.\n2. Mostly, I just use it. When I want to improve the skill, I tell the agent what to do differently and ask it to update .agent/skills/SKILL.md with that knowledge. This way, I only have to intervene once to improve the automation.\n",
    "github": {
      "stars": 150,
      "forks": 9,
      "owner": "klaudworks",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/36307788?v=4",
      "license": "MIT",
      "lastCommit": "2025-11-12T09:01:00Z",
      "createdAt": "2025-11-02T21:13:11Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:26.768Z"
  },
  {
    "id": "daily-pulse",
    "source": {
      "type": "github-search",
      "repo": "picahq/awesome-pica",
      "path": "claude-skills/daily-pulse/Skill.md",
      "url": "https://github.com/picahq/awesome-pica/blob/main/claude-skills/daily-pulse/Skill.md"
    },
    "frontmatter": {
      "name": "daily-pulse",
      "description": "Generates a personalized daily briefing by analyzing Google Calendar events and unread Gmail messages through Pica MCP Server. Use this when users request their daily overview, morning briefing, day's agenda, pulse check, or want to see what's important today.",
      "license": "Complete terms in LICENSE.txt"
    },
    "content": "# Daily Pulse\n\n## Overview\n\nDaily Pulse generates a personalized morning briefing that synthesizes your calendar and email into actionable insights. By connecting to Google Calendar and Gmail through the Pica MCP Server, it identifies priorities, surfaces urgent items, and helps you start your day with clarity.\n\n## When to Use This Skill\n\nTrigger this skill when users request:\n- \"What's my daily pulse?\"\n- \"Give me my morning briefing\"\n- \"What should I focus on today?\"\n- \"Show me today's priorities\"\n- \"What's on my agenda?\"\n- \"Help me plan my day\"\n- Any variation requesting a daily overview or pulse check\n\n## How It Works\n\nThe skill follows a three-phase process to generate your daily briefing:\n\n### Phase 1: Data Collection\n\n**Calendar Events (via Pica MCP Server)**\n- Retrieve today's calendar events from Google Calendar\n- Include event titles, times, attendees, and descriptions\n- Capture meeting durations and locations (if applicable)\n\n**Email Summary (via Pica MCP Server)**\n- Fetch unread Gmail messages from today\n- Extract sender, subject, timestamp, and preview content\n- Identify emails requiring action vs. informational emails\n\n### Phase 2: Analysis & Prioritization\n\n**Event Intelligence**\n- Identify high-impact meetings (1-on-1s, client calls, presentations)\n- Detect preparation requirements from event descriptions\n- Calculate available focus blocks between meetings\n- Flag scheduling conflicts or back-to-back meetings\n\n**Email Triage**\n- Categorize by urgency (time-sensitive, actionable, informational)\n- Identify emails from key people or about ongoing projects\n- Surface items requiring responses before EOD\n- Detect opportunities or inbound requests\n\n**Time Analysis**\n- Calculate total meeting time vs. available focus time\n- Identify optimal work blocks for deep work\n- Flag overbooked periods or insufficient breaks\n\n### Phase 3: Briefing Generation\n\nGenerate a structured daily pulse containing:\n\n**1. Executive Summary (Top 3 Priorities)**\n- The most important items requiring attention today\n- Action-oriented statements with context\n\n**2. Calendar Intelligence**\n- Total meetings with breakdown by type\n- High-priority events requiring preparation\n- Available focus blocks for deep work\n- Recommendations for rescheduling low-priority items\n\n**3. Email Priorities**\n- Urgent emails requiring immediate action\n- Important emails needing responses today\n- Quick wins (emails that take <5 minutes)\n- Can-wait items (FYI or low priority)\n\n**4. Time Optimization**\n- Best windows for focused work\n- Suggested task sequencing\n- Energy management recommendations\n\n**5. Quick Actions**\n- Immediate 2-minute tasks to clear\n- Preparation items for upcoming meetings\n- Follow-ups to send\n\n## Output Format\n\nStructure the daily pulse as follows:\n```\n🌅 Daily Pulse for [Date]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n🎯 TOP 3 PRIORITIES\n1. [Most important item with context]\n2. [Second priority with context]\n3. [Third priority with context]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n📅 CALENDAR INTELLIGENCE ([X] meetings, [Y]h total)\n\nHIGH-IMPACT EVENTS\n- [Time] - [Event Name]\n  → [Why it matters / Prep needed]\n\nFOCUS BLOCKS AVAILABLE\n- [Time Range] - [Duration] \n  → [Recommendation for use]\n\n⚠️ SCHEDULING NOTES\n- [Any conflicts, back-to-backs, or recommendations]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n📧 EMAIL PRIORITIES ([X] unread)\n\n🔴 URGENT (Action Required Today)\n- From: [Sender] | Re: [Subject]\n  → [Why urgent / What's needed]\n\n🟡 IMPORTANT (Response Needed)\n- From: [Sender] | Re: [Subject]\n  → [Context / Suggested action]\n\n⚡ QUICK WINS (<5 min)\n- [Brief email summaries that can be handled quickly]\n\n📥 FYI / Lower Priority\n- [Count] informational emails can wait\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n⏰ TIME OPTIMIZATION\n\nBEST FOCUS WINDOWS\n- [Time] - Deep work on [suggested task]\n- [Time] - Batch email responses\n\nENERGY MANAGEMENT\n- [Recommendations based on meeting density]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n✅ QUICK ACTIONS (Do These First)\n\n1. [Immediate 2-min action]\n2. [Prep for upcoming meeting]\n3. [Quick follow-up to send]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Best Practices\n\n**Tone & Language**\n- Be concise and action-oriented\n- Use clear, scannable formatting with emojis for visual hierarchy\n- Focus on \"what matters\" and \"what to do\" not just \"what exists\"\n- Provide context for why things are prioritized\n\n**Prioritization Logic**\n- Weight meetings by: attendee seniority, meeting type, preparation needs\n- Weight emails by: sender importance, subject keywords, time-sensitivity\n- Consider recency (items happening soon > items later in day)\n\n**Time Management**\n- Always identify at least one substantial focus block if possible\n- Flag when the day is overbooked (>80% meeting time)\n- Suggest protecting focus time when applicable\n\n**Contextual Intelligence**\n- Connect related items (e.g., email about a project + meeting about same project)\n- Identify patterns (e.g., multiple requests from same person/topic)\n- Surface anomalies (unusual sender, unexpected meeting, urgent language)\n\n## Error Handling\n\n**If Calendar Access Fails**\n- Inform user that calendar couldn't be accessed via Pica\n- Provide email-only pulse with disclaimer\n- Suggest troubleshooting Pica MCP Server connection\n\n**If Gmail Access Fails**\n- Inform user that email couldn't be accessed via Pica\n- Provide calendar-only pulse with disclaimer\n- Suggest checking Pica authentication\n\n**If Both Fail**\n- Clearly explain the connection issue\n- Provide instructions for reconnecting Pica MCP Server\n- Offer to generate pulse again once connection is restored\n\n**If No Events or Emails**\n- Generate a positive pulse noting the light schedule\n- Suggest proactive tasks or goals to work on\n- Highlight this as an opportunity for deep work or planning\n\n## Privacy & Sensitivity\n\n- Never quote full email contents verbatim in the pulse\n- Summarize sensitive information professionally\n- If calendar events seem private, reference them discreetly\n- Focus on actionable insights rather than raw data dumps\n\n## Example Interactions\n\n**User:** \"What's my daily pulse?\"\n\n**Claude:** [Fetches calendar events via Pica, fetches unread emails via Pica, analyzes data, generates formatted pulse as shown above]\n\n**User:** \"Give me my morning briefing\"\n\n**Claude:** [Same process - recognizes this as a pulse request]\n\n**User:** \"What should I focus on first?\"\n\n**Claude:** [Generates pulse with extra emphasis on the Quick Actions section]",
    "github": {
      "stars": 26,
      "forks": 6,
      "owner": "picahq",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/63831834?v=4",
      "lastCommit": "2025-10-28T22:27:12Z",
      "createdAt": "2025-02-12T15:29:58Z",
      "topics": [
        "agentic-ai",
        "examples",
        "pica"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:27.476Z"
  },
  {
    "id": "debug-cuda-crash",
    "source": {
      "type": "github-search",
      "repo": "flashinfer-ai/flashinfer",
      "path": ".claude/skills/debug-cuda-crash/SKILL.md",
      "url": "https://github.com/flashinfer-ai/flashinfer/blob/main/.claude/skills/debug-cuda-crash/SKILL.md"
    },
    "frontmatter": {
      "name": "debug-cuda-crash",
      "description": "Tutorial for debugging CUDA crashes using API logging"
    },
    "content": "# Tutorial: Debugging CUDA Crashes with API Logging\n\nThis tutorial shows you how to debug CUDA crashes and errors in FlashInfer using the `@flashinfer_api` logging decorator.\n\n## Goal\n\nWhen your code crashes with CUDA errors (illegal memory access, out-of-bounds, NaN/Inf), use API logging to:\n- Capture input tensors BEFORE the crash occurs\n- Understand what data caused the problem\n- Track tensor shapes, dtypes, and values through your pipeline\n- Detect numerical issues (NaN, Inf, wrong shapes)\n\n## Why Use API Logging?\n\n**Problem**: CUDA errors often crash the program, leaving no debugging information.\n\n**Solution**: FlashInfer's `@flashinfer_api` decorator logs inputs BEFORE execution, so you can see what caused the crash even after the program terminates.\n\n## Step 1: Enable API Logging\n\n### Basic Logging (Function Names Only)\n\n```bash\nexport FLASHINFER_LOGLEVEL=1        # Log function names\nexport FLASHINFER_LOGDEST=stdout    # Log to console\n\npython my_script.py\n```\n\nOutput:\n```\n[2025-12-18 10:30:45] FlashInfer API Call: batch_decode_with_padded_kv_cache\n```\n\n### Detailed Logging (Inputs/Outputs with Metadata)\n\n```bash\nexport FLASHINFER_LOGLEVEL=3        # Log inputs/outputs with metadata\nexport FLASHINFER_LOGDEST=debug.log # Save to file\n\npython my_script.py\n```\n\nOutput in `debug.log`:\n```\n================================================================================\n[2025-12-18 10:30:45] FlashInfer API Logging - System Information\n================================================================================\nFlashInfer version: 0.6.0\nCUDA toolkit version: 12.1\nGPU 0: NVIDIA H100 PCIe\n  Compute capability: 9.0 (SM90)\nPyTorch version: 2.1.0\n================================================================================\n\n================================================================================\n[2025-12-18 10:30:46] FlashInfer API Call: batch_decode_with_padded_kv_cache\n--------------------------------------------------------------------------------\nPositional input arguments:\n  arg[0]:\n    Tensor(\n      shape=(32, 8, 128)\n      dtype=torch.bfloat16\n      device=cuda:0\n      requires_grad=False\n      is_contiguous=True\n    )\nKeyword input arguments:\n  kv_cache=\n    Tensor(\n      shape=(1024, 2, 8, 128)\n      dtype=torch.bfloat16\n      device=cuda:0\n      requires_grad=False\n      is_contiguous=True\n    )\n```\n\n### Full Logging (With Tensor Statistics)\n\n```bash\nexport FLASHINFER_LOGLEVEL=5        # Log with min/max/mean/nan/inf\nexport FLASHINFER_LOGDEST=debug.log\n\npython my_script.py\n```\n\nAdditional output:\n```\n  Tensor(\n    shape=(32, 8, 128)\n    dtype=torch.bfloat16\n    device=cuda:0\n    requires_grad=False\n    is_contiguous=True\n    min=-3.125000\n    max=4.250000\n    mean=0.015625\n    nan_count=0\n    inf_count=0\n  )\n```\n\n## Step 2: Reproduce the Crash\n\n### Example: Shape Mismatch\n\nYour code crashes with:\n```\nRuntimeError: CUDA error: an illegal memory access was encountered\n```\n\nEnable logging and run again:\n\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=crash_log.txt\n\npython my_script.py\n```\n\nThe log shows inputs before the crash:\n```\n[2025-12-18 10:32:15] FlashInfer API Call: batch_decode_with_padded_kv_cache\nPositional input arguments:\n  arg[0]:\n    Tensor(\n      shape=(32, 8, 128)      # Query tensor\n      ...\n    )\nKeyword input arguments:\n  kv_cache=\n    Tensor(\n      shape=(1024, 2, 8, 64)  # ❌ Wrong! Should be (..., 128) not (..., 64)\n      ...\n    )\n```\n\n**Found the bug**: `head_dim` mismatch (64 vs 128)\n\n## Step 3: Common CUDA Errors and How to Debug\n\n### Error 1: Illegal Memory Access\n\n**Error Message**:\n```\nRuntimeError: CUDA error: an illegal memory access was encountered\n```\n\n**Enable logging**:\n```bash\nexport FLASHINFER_LOGLEVEL=3\npython my_script.py\n```\n\n**What to check in logs**:\n- ✅ Tensor shapes match expected dimensions\n- ✅ All tensors are on CUDA (not CPU)\n- ✅ Tensor strides are reasonable\n- ✅ `is_contiguous=True` (if required)\n\n**Common causes**:\n- Wrong tensor dimensions\n- CPU tensor passed to GPU kernel\n- Incorrect stride patterns\n\n### Error 2: NaN or Inf Values\n\n**Error Message**:\n```\nRuntimeError: Function ... returned nan or inf\n```\n\n**Enable statistics logging**:\n```bash\nexport FLASHINFER_LOGLEVEL=5        # Level 5 shows nan_count, inf_count\npython my_script.py\n```\n\n**What to check in logs**:\n```\nTensor(\n  ...\n  min=-1234567.000000     # ❌ Suspiciously large\n  max=9876543.000000      # ❌ Suspiciously large\n  mean=nan                # ❌ NaN detected\n  nan_count=128           # ❌ 128 NaN values!\n  inf_count=0\n)\n```\n\n**Common causes**:\n- Division by zero in previous operation\n- Numerical overflow/underflow\n- Uninitialized memory\n\n### Error 3: Out of Memory\n\n**Error Message**:\n```\nRuntimeError: CUDA out of memory\n```\n\n**Enable logging**:\n```bash\nexport FLASHINFER_LOGLEVEL=3\npython my_script.py\n```\n\n**What to check in logs**:\n- ✅ Tensor shapes (are they unexpectedly large?)\n- ✅ Batch size\n- ✅ Sequence length\n\nExample:\n```\nTensor(\n  shape=(1024, 8192, 128, 128)  # ❌ Way too large! Should be (1024, 128, 128)?\n  ...\n)\n```\n\n### Error 4: Wrong Dtype\n\n**Error Message**:\n```\nRuntimeError: expected scalar type BFloat16 but found Float16\n```\n\n**Enable logging**:\n```bash\nexport FLASHINFER_LOGLEVEL=3\npython my_script.py\n```\n\n**What to check in logs**:\n```\nTensor(\n  dtype=torch.float16     # ❌ Should be torch.bfloat16\n  ...\n)\n```\n\n## Step 4: Multi-Process Debugging\n\nWhen running with multiple GPUs/processes, use `%i` pattern:\n\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=debug_rank_%i.txt    # %i = process ID\n\ntorchrun --nproc_per_node=4 my_script.py\n```\n\nThis creates separate logs:\n- `debug_rank_12345.txt` (process 12345)\n- `debug_rank_12346.txt` (process 12346)\n- `debug_rank_12347.txt` (process 12347)\n- `debug_rank_12348.txt` (process 12348)\n\nNow you can debug each rank independently.\n\n## Step 5: Advanced Debugging with compute-sanitizer\n\nFor harder bugs, combine API logging with CUDA tools:\n\n### Use compute-sanitizer (Memory Checker)\n\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=debug.log\n\ncompute-sanitizer --tool memcheck python my_script.py\n```\n\nOutput shows exact memory errors:\n```\n========= COMPUTE-SANITIZER\n========= Invalid __global__ write of size 4 bytes\n=========     at 0x1234 in ScaleKernel<float>\n=========     by thread (256,0,0) in block (10,0,0)\n=========     Address 0x7f1234567890 is out of bounds\n```\n\nCheck `debug.log` to see what inputs caused this kernel to fail.\n\n### Use cuda-gdb (Debugger)\n\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=debug.log\n\ncuda-gdb --args python my_script.py\n```\n\nIn gdb:\n```\n(cuda-gdb) run\n(cuda-gdb) where     # Show stack trace when it crashes\n```\n\nCheck `debug.log` for the inputs that led to the crash.\n\n## Step 6: Kernel-Level Debugging with printf()\n\nYou can use `printf()` inside CUDA kernels for debugging:\n\n### Basic Usage\n\n```cpp\n__global__ void MyKernel(const float* input, float* output, int n) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Print from one thread to avoid spam\n  if (threadIdx.x == 0 && blockIdx.x == 0) {\n    printf(\"n=%d, input[0]=%f\\n\", n, input[0]);\n  }\n\n  if (idx < n) {\n    output[idx] = input[idx] * 2.0f;\n  }\n}\n```\n\n**Important**: Flush printf buffer after kernel:\n```python\nmy_kernel(input, output)\ntorch.cuda.synchronize()  # ← Flushes printf output\n```\n\n### ⚠️ Warp-Specialized Kernels: Choosing the Right Print Thread\n\n**Problem**: `threadIdx.x == 0` doesn't work for all warps (warp starting at thread 32 won't have thread 0).\n\n**Solution**: Choose one representative thread per specialization group.\n\n```cpp\n__global__ void WarpSpecializedKernel(...) {\n  // Define your group's representative thread\n  // e.g., first thread of each warp: threadIdx.x % 32 == 0\n  // e.g., first thread of each 4-warp group: threadIdx.x % 128 == 0\n\n  if (is_group_representative) {\n    printf(\"Group %d processing\\n\", group_id);\n  }\n}\n```\n\n**Common mistake** ❌:\n```cpp\n// ❌ Only warp 0 will print!\nif (threadIdx.x == 0) {\n  printf(\"Warp %d processing\\n\", threadIdx.x / 32);\n}\n```\n\n### Quick Reference\n\n| Kernel Type | Print Condition | Notes |\n|-------------|-----------------|-------|\n| Simple kernel | `threadIdx.x == 0` | One thread per block |\n| Warp-specialized | One thread per group | Depends on kernel design |\n\n### Other Kernel Debugging Tools\n\n```cpp\n// Assert for invariants\nassert(value >= 0.0f && \"Value must be non-negative\");\n\n// Compile-time checks\nstatic_assert(BLOCK_SIZE % 32 == 0, \"BLOCK_SIZE must be multiple of warp size\");\n```\n\n## Environment Variables Reference\n\n| Variable | Values | Description |\n|----------|--------|-------------|\n| `FLASHINFER_LOGLEVEL` | `0` | No logging (default) |\n|  | `1` | Function names only |\n|  | `3` | Inputs/outputs with metadata |\n|  | `5` | + Tensor statistics (min/max/mean/nan/inf) |\n| `FLASHINFER_LOGDEST` | `stdout` | Log to console (default) |\n|  | `stderr` | Log to stderr |\n|  | `<path>` | Log to file |\n|  | `log_%i.txt` | Multi-process: %i = process ID |\n\n## Best Practices\n\n### 1. Always Start with Level 3\n\n```bash\nexport FLASHINFER_LOGLEVEL=3\n```\n\nLevel 3 provides tensor metadata (shape, dtype, device) without overwhelming output.\n\n### 2. Use Level 5 for Numerical Issues\n\n```bash\nexport FLASHINFER_LOGLEVEL=5\n```\n\nOnly use level 5 when debugging NaN/Inf problems (adds statistics).\n\n### 3. Log to File for Crashes\n\n```bash\nexport FLASHINFER_LOGDEST=crash_log.txt\n```\n\nConsole output may be lost when program crashes. File logs persist.\n\n### 4. Compare Before/After\n\nEnable logging and compare:\n- Last successful API call (inputs logged, outputs logged) ✅\n- First failed API call (inputs logged, no outputs) ❌ ← This is where it crashed!\n\n### 5. Disable Logging in Production\n\n```bash\nunset FLASHINFER_LOGLEVEL   # or export FLASHINFER_LOGLEVEL=0\n```\n\nLogging has zero overhead when disabled (decorator returns original function).\n\n## Troubleshooting\n\n### No Logs Appearing\n\n**Problem**: Set `FLASHINFER_LOGLEVEL=3` but no logs appear\n\n**Solutions**:\n1. **Check if API has the decorator**: Not all FlashInfer APIs have `@flashinfer_api` yet (work in progress)\n\n2. **Verify environment variable**:\n   ```bash\n   echo $FLASHINFER_LOGLEVEL    # Should print \"3\"\n   ```\n\n3. **Check log destination**:\n   ```bash\n   echo $FLASHINFER_LOGDEST     # Should print path or \"stdout\"\n   ```\n\n### Too Much Output\n\n**Problem**: Level 5 produces too much output\n\n**Solution**: Use level 3 instead:\n```bash\nexport FLASHINFER_LOGLEVEL=3   # Skip tensor statistics\n```\n\n### Statistics Skipped in CUDA Graph\n\n**Warning**: `[statistics skipped: CUDA graph capture in progress]`\n\n**What it means**: Level 5 statistics are automatically skipped during CUDA graph capture (to avoid synchronization)\n\n**This is normal**: The framework protects you from graph capture issues.\n\n## Quick Examples\n\n### Debug Shape Mismatch\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=stdout\npython my_script.py\n# Check tensor shapes in output\n```\n\n### Debug NaN/Inf\n```bash\nexport FLASHINFER_LOGLEVEL=5         # Show statistics\nexport FLASHINFER_LOGDEST=debug.log\npython my_script.py\n# Check nan_count and inf_count in debug.log\n```\n\n### Debug Multi-GPU Training\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=rank_%i.log   # Separate log per rank\ntorchrun --nproc_per_node=8 train.py\n# Check rank_*.log files\n```\n\n### Combine with Memory Checker\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=inputs.log\ncompute-sanitizer --tool memcheck python my_script.py\n# inputs.log shows what data caused the memory error\n```\n\n## Example: Full Debug Session\n\n### Your code crashes:\n```python\nimport torch\nfrom flashinfer import batch_decode_with_padded_kv_cache\n\nq = torch.randn(32, 8, 128, dtype=torch.bfloat16, device=\"cuda\")\nkv = torch.randn(1024, 2, 8, 64, dtype=torch.bfloat16, device=\"cuda\")  # Wrong dim!\n\noutput = batch_decode_with_padded_kv_cache(q, kv)  # ❌ Crashes\n```\n\n### Enable logging:\n```bash\nexport FLASHINFER_LOGLEVEL=3\nexport FLASHINFER_LOGDEST=debug.log\npython test.py\n```\n\n### Check debug.log:\n```\n[2025-12-18 10:45:23] FlashInfer API Call: batch_decode_with_padded_kv_cache\nPositional input arguments:\n  arg[0]:\n    Tensor(\n      shape=(32, 8, 128)\n      dtype=torch.bfloat16\n      ...\n    )\n  arg[1]:\n    Tensor(\n      shape=(1024, 2, 8, 64)    # ❌ Found it! Last dim should be 128\n      dtype=torch.bfloat16\n      ...\n    )\n```\n\n### Fix the bug:\n```python\nkv = torch.randn(1024, 2, 8, 128, dtype=torch.bfloat16, device=\"cuda\")  # ✅ Fixed\n```\n\n### Success!\n```bash\npython test.py\n# No crash, outputs logged successfully\n```\n\n## Summary\n\n1. **Enable logging** before the crash:\n   ```bash\n   export FLASHINFER_LOGLEVEL=3\n   export FLASHINFER_LOGDEST=debug.log\n   ```\n\n2. **Run your code** - inputs are logged BEFORE crash\n\n3. **Check the log** - last API call shows what caused the crash\n\n4. **Fix the issue** based on logged input metadata\n\n5. **Disable logging** when done:\n   ```bash\n   export FLASHINFER_LOGLEVEL=0\n   ```\n\n## Related Documentation\n\n- See CLAUDE.md \"API Logging with @flashinfer_api\" for technical details\n- See `flashinfer/api_logging.py` for implementation\n- See CUDA documentation for compute-sanitizer and cuda-gdb\n",
    "github": {
      "stars": 4421,
      "forks": 622,
      "owner": "flashinfer-ai",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/145061914?v=4",
      "license": "Apache-2.0",
      "lastCommit": "2026-01-04T05:33:55Z",
      "createdAt": "2023-07-22T00:29:04Z",
      "topics": [
        "attention",
        "cuda",
        "distributed-inference",
        "gpu",
        "jit",
        "large-large-models",
        "llm-inference",
        "moe",
        "nvidia",
        "pytorch"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:28.160Z"
  },
  {
    "id": "claude-agent-ts-sdk",
    "source": {
      "type": "github-search",
      "repo": "szweibel/claude-skills",
      "path": "claude-agent-ts-sdk/SKILL.md.backup",
      "url": "https://github.com/szweibel/claude-skills/blob/main/claude-agent-ts-sdk/SKILL.md.backup"
    },
    "frontmatter": {
      "name": "claude-agent-ts-sdk",
      "description": "Build Claude agents using TypeScript with the @anthropic-ai/claude-agent-sdk. Use this skill when implementing conversational agents, building tools for agents, setting up streaming responses, or debugging agent implementations. Covers the tool wrapping pattern, SDK initialization, agent architecture, and best practices.",
      "license": "MIT",
      "category": "development-tools"
    },
    "content": "# Claude Agent TypeScript SDK\n\n## Overview\n\nBuild production-ready Claude agents using TypeScript and the `@anthropic-ai/claude-agent-sdk`. This skill provides battle-tested patterns from successful implementations, focusing on the tool wrapping approach (not full MCP servers) for creating modular, composable agent tools.\n\n## When to Use This Skill\n\nUse this skill when:\n- Implementing a new Claude agent in TypeScript\n- Creating tools for agent use\n- Setting up streaming agent responses\n- Debugging agent SDK implementations\n- Converting between MCP server and tool wrapping approaches\n- Building CLI agents, web server agents, or plugin-based agents\n- Working with subprocess tools, file system tools, or API wrappers\n\n## Authentication\n\nThe SDK automatically uses Claude Code's authentication - no setup required. Your agent works seamlessly within Claude Code without any authentication configuration.\n\n## Core Architecture Patterns\n\n### Pattern 1: CLI/Specialized Agent\n\n**Best for:** Command-line tools, specialized workflows, batch processing\n\n```typescript\n// src/agent.ts\nimport { query, createSdkMcpServer } from '@anthropic-ai/claude-agent-sdk';\nimport { myTools } from './tools/index.js';\n\nconst SYSTEM_PROMPT = `You are a specialized agent that...\n[Detailed instructions about capabilities and tools]`;\n\nexport async function runAgent(userPrompt: string) {\n  // Create MCP server at query time (not module level)\n  const server = createSdkMcpServer({\n    name: 'my-agent',\n    version: '1.0.0',\n    tools: myTools,\n  });\n\n  // Query with streaming\n  const messages = query({\n    prompt: userPrompt,\n    options: {\n      systemPrompt: SYSTEM_PROMPT,\n      permissionMode: 'bypassPermissions',\n      mcpServers: {\n        'my-agent': server,\n      },\n    },\n  });\n\n  // Process stream\n  for await (const event of messages) {\n    if (event.type === 'assistant') {\n      for (const content of event.message.content) {\n        if (content.type === 'text') {\n          process.stdout.write(content.text);\n        }\n      }\n    }\n  }\n}\n```\n\n### Pattern 2: Web Server Agent with SSE\n\n**Best for:** Web applications, API servers, real-time UIs\n\n```typescript\n// src/routes/agent.ts\nimport { query, createSdkMcpServer } from '@anthropic-ai/claude-agent-sdk';\n\napp.post('/agent/stream', async (req, res) => {\n  res.setHeader('Content-Type', 'text/event-stream');\n  res.setHeader('Cache-Control', 'no-cache');\n  res.setHeader('Connection', 'keep-alive');\n\n  const server = createSdkMcpServer({\n    name: 'web-agent',\n    version: '1.0.0',\n    tools: createContextualTools(req.session),\n  });\n\n  const messages = query({\n    prompt: req.body.prompt,\n    options: {\n      systemPrompt: SYSTEM_PROMPT,\n      mcpServers: { 'web-agent': server },\n    },\n  });\n\n  for await (const event of messages) {\n    res.write(`data: ${JSON.stringify(event)}\\n\\n`);\n  }\n\n  res.end();\n});\n```\n\n### Pattern 3: Plugin/Framework Agent\n\n**Best for:** Extensible systems, plugin architectures, reusable frameworks\n\n```typescript\n// src/engine.ts\nexport class AgentEngine {\n  constructor(private plugin: Plugin) {}\n\n  async stream(prompt: string): Promise<void> {\n    const mcpServers: Record<string, any> = {};\n\n    if (this.plugin.tools.length > 0) {\n      const serverName = `${this.plugin.name}-tools`;\n      mcpServers[serverName] = createSdkMcpServer({\n        name: serverName,\n        version: this.plugin.version ?? '1.0.0',\n        tools: this.plugin.tools,\n      });\n    }\n\n    const messages = query({\n      prompt,\n      options: {\n        systemPrompt: this.plugin.systemPrompt,\n        permissionMode: 'bypassPermissions',\n        allowedTools: this.plugin.allowedTools,\n        mcpServers,\n      },\n    });\n\n    for await (const event of messages) {\n      await this.plugin.handleEvent(event);\n    }\n  }\n}\n```\n\n### Pattern 4: Monorepo Full-Stack Application\n\n**Best for:** Full-stack apps with frontend + backend, complex web applications\n\nSite Studio is an example of this pattern using npm workspaces.\n\n**Project Structure:**\n```\nmy-app/\n├── package.json           # Root workspace config\n├── packages/\n│   ├── backend/\n│   │   ├── package.json\n│   │   ├── src/\n│   │   │   ├── index.ts   # Express server\n│   │   │   ├── agent.ts   # Agent logic\n│   │   │   └── tools/     # Tool definitions\n│   │   └── dist/          # Build output\n│   └── frontend/\n│       ├── package.json\n│       ├── src/\n│       └── dist/\n```\n\n**Root package.json:**\n```json\n{\n  \"name\": \"my-app\",\n  \"private\": true,\n  \"workspaces\": [\"packages/*\"],\n  \"scripts\": {\n    \"dev\": \"concurrently \\\"npm run dev:backend\\\" \\\"npm run dev:frontend\\\"\",\n    \"dev:backend\": \"npm run dev --workspace=packages/backend\",\n    \"dev:frontend\": \"npm run dev --workspace=packages/frontend\",\n    \"build\": \"npm run build --workspace=packages/backend && npm run build --workspace=packages/frontend\"\n  },\n  \"devDependencies\": {\n    \"concurrently\": \"^9.1.2\"\n  }\n}\n```\n\n**Backend package.json:**\n```json\n{\n  \"name\": \"@my-app/backend\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"tsc && node dist/index.js\",\n    \"build\": \"tsc\"\n  },\n  \"dependencies\": {\n    \"@anthropic-ai/claude-agent-sdk\": \"^0.1.14\",\n    \"express\": \"^4.21.2\",\n    \"zod\": \"^3.24.1\"\n  }\n}\n```\n\n**Backend agent.ts:**\n```typescript\nimport { query, createSdkMcpServer } from '@anthropic-ai/claude-agent-sdk';\nimport { createFileTools } from './tools/file-tools.js';\n\nexport async function runAgent(\n  prompt: string,\n  projectPath: string,\n  sessionId?: string\n): Promise<AsyncIterable<any>> {\n  const tools = createFileTools(projectPath);\n\n  const server = createSdkMcpServer({\n    name: 'my-app',\n    version: '1.0.0',\n    tools,\n  });\n\n  const queryOptions: any = {\n    permissionMode: 'bypassPermissions',  // Required for standalone servers!\n    systemPrompt: SYSTEM_PROMPT,\n    mcpServers: { 'my-app': server },\n  };\n\n  if (sessionId) {\n    queryOptions.resume = sessionId;\n  }\n\n  return query({ prompt, options: queryOptions });\n}\n```\n\n**Backend index.ts (Express):**\n```typescript\nimport express from 'express';\nimport { runAgent } from './agent.js';\n\nconst app = express();\napp.use(express.json());\n\napp.post('/api/query', async (req, res) => {\n  const { prompt, projectId, sessionId } = req.body;\n  const projectPath = path.join(PROJECTS_DIR, projectId);\n\n  // Set up SSE\n  res.setHeader('Content-Type', 'text/event-stream');\n  res.setHeader('Cache-Control', 'no-cache');\n  res.setHeader('Connection', 'keep-alive');\n\n  try {\n    const stream = await runAgent(prompt, projectPath, sessionId);\n\n    for await (const event of stream) {\n      res.write(`data: ${JSON.stringify(event)}\\n\\n`);\n    }\n\n    res.write('data: [DONE]\\n\\n');\n    res.end();\n  } catch (error: any) {\n    res.write(`data: ${JSON.stringify({ error: error.message })}\\n\\n`);\n    res.end();\n  }\n});\n\napp.listen(3001, () => {\n  console.log('Backend running on http://localhost:3001');\n});\n```\n\n**Benefits of Monorepo Pattern:**\n- Share TypeScript configurations\n- Single `npm install` for all dependencies\n- Coordinated development with `concurrently`\n- Type safety across packages\n- Unified build and deploy processes\n\n## Tool Wrapping Pattern (Preferred)\n\n**Philosophy:** Create portable, composable tools as modules. Only wrap in MCP servers when needed for query execution.\n\n### Basic Tool Definition\n\n```typescript\n// src/tools/my-tool.ts\nimport { tool } from '@anthropic-ai/claude-agent-sdk';\nimport { z } from 'zod';\n\nexport const myTool = tool(\n  'my_tool_name',\n  'Clear description of what this tool does',\n  z.object({\n    param1: z.string().describe('What param1 is for'),\n    param2: z.number().optional().describe('Optional parameter'),\n  }).shape,\n  async (params) => {\n    // Implementation\n    const result = await doSomething(params.param1, params.param2);\n\n    return {\n      content: [{\n        type: 'text',\n        text: JSON.stringify(result, null, 2),\n      }],\n    };\n  }\n);\n```\n\n### Tool Composition Pattern\n\n```typescript\n// src/tools/index.ts\nimport { fileTools } from './file-tools.js';\nimport { apiTools } from './api-tools.js';\nimport { searchTool } from './search-tool.js';\n\n// Export individual tools for selective use\nexport { fileTools, apiTools, searchTool };\n\n// Export combined array for convenience\nexport const allTools = [\n  ...fileTools,\n  ...apiTools,\n  searchTool,\n];\n```\n\n### Tool Factories (Context-Aware Tools)\n\n```typescript\n// src/tools/file-tools.ts\nimport { tool } from '@anthropic-ai/claude-agent-sdk';\nimport { z } from 'zod';\n\nexport function createFileTools(projectPath: string) {\n  const readFile = tool(\n    'read_file',\n    'Read a file from the project',\n    z.object({\n      path: z.string().describe('Relative path from project root'),\n    }).shape,\n    async ({ path }) => {\n      const fullPath = join(projectPath, path);\n      const content = await fs.readFile(fullPath, 'utf-8');\n      return {\n        content: [{ type: 'text', text: content }],\n      };\n    }\n  );\n\n  const writeFile = tool(\n    'write_file',\n    'Write content to a file',\n    z.object({\n      path: z.string().describe('Relative path from project root'),\n      content: z.string().describe('File content'),\n    }).shape,\n    async ({ path, content }) => {\n      const fullPath = join(projectPath, path);\n      await fs.writeFile(fullPath, content, 'utf-8');\n      return {\n        content: [{ type: 'text', text: `Wrote to ${path}` }],\n      };\n    }\n  );\n\n  return [readFile, writeFile];\n}\n\n// Usage in agent:\nconst tools = createFileTools('/path/to/project');\nconst server = createSdkMcpServer({ name: 'files', version: '1.0.0', tools });\n```\n\n### Subprocess Wrapper Tools\n\n```typescript\n// src/tools/python-tools.ts\nimport { tool } from '@anthropic-ai/claude-agent-sdk';\nimport { spawn } from 'child_process';\nimport { z } from 'zod';\n\nexport const runPythonScript = tool(\n  'run_python_analysis',\n  'Run Python data analysis script',\n  z.object({\n    query: z.string().describe('Search query for analysis'),\n    limit: z.number().optional().describe('Max results'),\n  }).shape,\n  async ({ query, limit = 10 }) => {\n    return new Promise((resolve, reject) => {\n      const pythonProcess = spawn('python3', [\n        'scripts/analyze.py',\n        '--query', query,\n        '--limit', String(limit),\n      ]);\n\n      let stdout = '';\n      let stderr = '';\n\n      pythonProcess.stdout.on('data', (data) => {\n        stdout += data.toString();\n      });\n\n      pythonProcess.stderr.on('data', (data) => {\n        stderr += data.toString();\n      });\n\n      pythonProcess.on('close', (code) => {\n        if (code !== 0) {\n          reject(new Error(`Python script failed: ${stderr}`));\n          return;\n        }\n\n        try {\n          const result = JSON.parse(stdout);\n          resolve({\n            content: [{\n              type: 'text',\n              text: JSON.stringify(result, null, 2),\n            }],\n          });\n        } catch (e) {\n          reject(new Error(`Failed to parse output: ${stdout}`));\n        }\n      });\n    });\n  }\n);\n```\n\n### Dynamic Tool Creation\n\n```typescript\n// src/tools/dynamic-tools.ts\nimport { tool } from '@anthropic-ai/claude-agent-sdk';\nimport { z } from 'zod';\n\nexport function createDynamicTools(config: ToolConfig[]) {\n  return config.map((toolConfig) => {\n    // Build Zod schema from config\n    const schemaShape: Record<string, z.ZodType> = {};\n    for (const param of toolConfig.parameters) {\n      let zodType: z.ZodType = z.string();\n\n      if (param.type === 'number') zodType = z.number();\n      if (param.type === 'boolean') zodType = z.boolean();\n      if (param.optional) zodType = zodType.optional();\n\n      zodType = zodType.describe(param.description);\n      schemaShape[param.name] = zodType;\n    }\n\n    return tool(\n      toolConfig.name,\n      toolConfig.description,\n      z.object(schemaShape).shape,\n      async (params) => {\n        // Execute configured command\n        const result = await executeCommand(toolConfig.command, params);\n        return {\n          content: [{ type: 'text', text: JSON.stringify(result) }],\n        };\n      }\n    );\n  });\n}\n```\n\n## Project Setup Checklist\n\n### 1. Package Configuration\n\n```json\n// package.json\n{\n  \"name\": \"my-agent\",\n  \"version\": \"1.0.0\",\n  \"type\": \"module\",\n  \"engines\": {\n    \"node\": \">=18.0.0\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node dist/index.js\",\n    \"dev\": \"tsx watch src/index.ts\",\n    \"typecheck\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"@anthropic-ai/claude-agent-sdk\": \"^0.1.14\",\n    \"zod\": \"^3.23.8\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.11.0\",\n    \"tsx\": \"^4.7.0\",\n    \"typescript\": \"^5.3.3\"\n  }\n}\n```\n\n### 2. TypeScript Configuration\n\n```json\n// tsconfig.json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n### 3. Environment Setup\n\n```bash\n# .env.example (optional - for your own environment variables)\nNODE_ENV=development\n```\n\n```typescript\n// .gitignore\nnode_modules/\ndist/\n.env\n*.log\n.DS_Store\n```\n\n### 4. Directory Structure\n\n```\nmy-agent/\n├── src/\n│   ├── index.ts              # Entry point\n│   ├── agent.ts              # Agent implementation\n│   ├── tools/\n│   │   ├── index.ts          # Tool exports\n│   │   ├── file-tools.ts     # File operations\n│   │   └── api-tools.ts      # API calls\n│   ├── prompts/\n│   │   └── system.ts         # System prompts\n│   └── types/\n│       └── index.ts          # Type definitions\n├── scripts/                   # Python/shell scripts called by tools\n├── package.json\n├── tsconfig.json\n├── .env\n└── .env.example\n```\n\n## System Prompts\n\n### Structure\n\nSystem prompts should be 100-170+ lines with:\n1. Role and capabilities\n2. Detailed tool descriptions and when to use them\n3. Workflow instructions\n4. Output formatting requirements\n5. Error handling guidelines\n\n```typescript\n// src/prompts/system.ts\nexport const SYSTEM_PROMPT = `You are a specialized agent with the following capabilities:\n\n## Role\n[Clear description of the agent's purpose and responsibilities]\n\n## Available Tools\n\n### tool_name_1\n**Purpose:** [What it does]\n**When to use:** [Specific scenarios]\n**Parameters:**\n- param1: [Description]\n- param2: [Description]\n**Example usage:** [Concrete example]\n\n### tool_name_2\n[Same structure...]\n\n## Workflow\n\n1. First, [initial step with tool usage]\n2. Then, [subsequent steps]\n3. Finally, [completion steps]\n\n## Output Format\n\nAlways structure your responses as:\n- [Format specification]\n- [Examples]\n\n## Error Handling\n\nIf a tool fails:\n- [Recovery strategies]\n- [Alternative approaches]\n\n## Important Notes\n\n- [Critical constraints]\n- [Edge cases to handle]\n`;\n```\n\n## Streaming and Event Handling\n\n### Tool Execution Event Flow\n\nWhen tools are executed, events follow this specific pattern that mirrors the Anthropic Messages API:\n\n**1. `assistant` event with `tool_use` block:**\n\nClaude requests to use a tool. The event contains:\n\n```typescript\n{\n  type: 'assistant',\n  message: {\n    role: 'assistant',\n    content: [{\n      type: 'tool_use',\n      id: 'toolu_01AvhNWuX3aeWaaouT5t6D73',  // Unique ID for this tool call\n      name: 'mcp__server__tool_name',\n      input: { param1: 'value' }\n    }]\n  }\n}\n```\n\n**2. `user` event with `tool_result` block:**\n\nThe system/environment provides the tool result. **Note: This is a `user` event, not an `assistant` event!**\n\n```typescript\n{\n  type: 'user',\n  message: {\n    role: 'user',\n    content: [{\n      type: 'tool_result',\n      tool_use_id: 'toolu_01AvhNWuX3aeWaaouT5t6D73',  // References the tool_use.id\n      content: [\n        { type: 'text', text: '{\"result\": \"data\"}' }\n      ],\n      is_error: false\n    }]\n  }\n}\n```\n\n**3. `assistant` event with text response:**\n\nClaude processes the result and responds to the user.\n\n```typescript\n{\n  type: 'assistant',\n  message: {\n    role: 'assistant',\n    content: [{\n      type: 'text',\n      text: 'Based on the results...'\n    }]\n  }\n}\n```\n\n**Key Points:**\n- **Tool results come in `user` events**, not `assistant` events\n- Match results using `tool_result.tool_use_id` ↔ `tool_use.id`\n- Tool output is in the `content` array (usually `content[0].text`)\n- This structure mirrors how you'd interact with the Anthropic API directly\n- From Claude's perspective: it requests a tool (`assistant`), the world responds (`user`), then it continues (`assistant`)\n\n**Example: Tracking Tool Execution**\n\n```typescript\nconst toolExecutions = new Map();\n\nfor await (const event of messages) {\n  if (event.type === 'assistant' && event.message?.content) {\n    for (const block of event.message.content) {\n      if (block.type === 'tool_use') {\n        // Store tool execution with its ID\n        toolExecutions.set(block.id, {\n          name: block.name,\n          input: block.input,\n          status: 'running'\n        });\n      }\n    }\n  }\n\n  if (event.type === 'user' && event.message?.content) {\n    for (const block of event.message.content) {\n      if (block.type === 'tool_result') {\n        // Match result with the tool call\n        const tool = toolExecutions.get(block.tool_use_id);\n        if (tool) {\n          tool.status = block.is_error ? 'error' : 'success';\n          tool.output = block.content.map(c => c.text).join('\\n');\n        }\n      }\n    }\n  }\n}\n```\n\n### Pattern 1: Simple Text Streaming\n\n```typescript\nfor await (const event of messages) {\n  if (event.type === 'assistant') {\n    for (const content of event.message.content) {\n      if (content.type === 'text') {\n        process.stdout.write(content.text);\n      }\n    }\n  }\n}\n```\n\n### Pattern 2: Structured Event Processing\n\n```typescript\nfor await (const event of messages) {\n  switch (event.type) {\n    case 'assistant':\n      // Handle assistant messages (including tool_use blocks)\n      for (const block of event.message.content) {\n        if (block.type === 'text') {\n          await handleTextContent(block.text);\n        } else if (block.type === 'tool_use') {\n          await handleToolUse(block);\n        }\n      }\n      break;\n\n    case 'user':\n      // Handle tool results\n      for (const block of event.message.content) {\n        if (block.type === 'tool_result') {\n          await handleToolResult(block);\n        }\n      }\n      break;\n\n    case 'error':\n      await handleError(event.error);\n      break;\n  }\n}\n```\n\n### Pattern 3: Web SSE Streaming\n\n```typescript\nfor await (const event of messages) {\n  res.write(`data: ${JSON.stringify(event)}\\n\\n`);\n\n  if (event.type === 'assistant') {\n    // Optionally send custom events\n    res.write(`event: message\\ndata: ${JSON.stringify({\n      role: 'assistant',\n      content: event.message.content\n    })}\\n\\n`);\n  }\n}\n```\n\n### Pattern 4: Web UI with Separate Text Sections (CRITICAL)\n\n**Problem:** When building web UIs that display conversation with interleaved text and tools, a common mistake is accumulating ALL text into one string. This causes text from different conversation sections to merge incorrectly.\n\n**❌ WRONG - Text accumulates forever:**\n```typescript\nlet fullTextResponse = '';  // DON'T DO THIS\n\nfor await (const event of messages) {\n  if (event.type === 'assistant' && event.message?.content) {\n    for (const block of event.message.content) {\n      if (block.type === 'text') {\n        fullTextResponse += block.text;  // ❌ Never resets!\n        // Display fullTextResponse\n      }\n    }\n  }\n}\n```\n\nThis creates: `\"Text1 Text2 Text3\"` all merged together, even when tools appear between them.\n\n**✅ CORRECT - Section-based text management:**\n```typescript\nlet currentSectionText = '';  // Resets after each tool group\nlet contentBlocks = [];  // Structured: [text, tools, text, tools, text]\nlet processedToolIds = new Set();\n\nfor await (const event of messages) {\n  if (event.type === 'assistant' && event.message?.content) {\n    for (const block of event.message.content) {\n      if (block.type === 'text') {\n        // Accumulate text for CURRENT section only\n        currentSectionText += block.text;\n\n        // Update or create text block for this section\n        let textBlockIndex = contentBlocks.findIndex((b, i) => {\n          if (b.type === 'text') {\n            // Find last text block (no tools after it)\n            const hasToolsAfter = contentBlocks.slice(i + 1).some(cb => cb.type === 'tools');\n            return !hasToolsAfter;\n          }\n          return false;\n        });\n\n        if (textBlockIndex >= 0) {\n          contentBlocks[textBlockIndex].text = currentSectionText;\n        } else {\n          contentBlocks.push({ type: 'text', text: currentSectionText });\n        }\n\n      } else if (block.type === 'tool_use') {\n        // Only process if not duplicate\n        if (!processedToolIds.has(block.id)) {\n          processedToolIds.add(block.id);\n\n          // Finalize current text section\n          if (currentSectionText.trim()) {\n            const lastBlock = contentBlocks[contentBlocks.length - 1];\n            if (lastBlock?.type === 'text') {\n              lastBlock.text = currentSectionText;\n            }\n          }\n\n          // RESET for next section - this is the key!\n          currentSectionText = '';\n\n          // Add tool to tools block\n          // (implementation depends on your UI structure)\n        }\n      }\n    }\n  }\n}\n```\n\n**Result:** Properly separated structure:\n```javascript\ncontentBlocks = [\n  { type: 'text', text: 'I will scaffold your site...' },\n  { type: 'tools', tools: [scaffoldTool] },\n  { type: 'text', text: 'Now I will read the files...' },\n  { type: 'tools', tools: [readTool1, readTool2] },\n  { type: 'text', text: 'I have updated your files...' }\n]\n```\n\n**Key Principles:**\n1. **Reset `currentSectionText` after tools** - Don't accumulate across tool boundaries\n2. **Deduplicate tools** - Use a Set to track processed tool IDs\n3. **Separate text and tool blocks** - Create structured sections, not one giant text blob\n4. **Update the current text block** - Don't create a new text block on every streaming chunk\n\n**Reference:** See `/home/zweb/apps/obsidian-agent-plugin/src/main.ts` lines 915-1010 for a working implementation of this pattern.\n\n## Error Handling\n\n### Tool Error Handling\n\n```typescript\nexport const myTool = tool(\n  'my_tool',\n  'Description',\n  schema,\n  async (params) => {\n    try {\n      const result = await riskyOperation(params);\n      return {\n        content: [{ type: 'text', text: JSON.stringify(result) }],\n      };\n    } catch (error) {\n      return {\n        content: [{\n          type: 'text',\n          text: JSON.stringify({\n            error: true,\n            message: error.message,\n            suggestion: 'Try adjusting parameters or check prerequisites',\n          }),\n        }],\n        isError: true,\n      };\n    }\n  }\n);\n```\n\n### Stream Error Handling\n\n```typescript\ntry {\n  for await (const event of messages) {\n    // Process events\n  }\n} catch (error) {\n  if (error.name === 'AbortError') {\n    console.log('Stream aborted by user');\n  } else if (error.status === 429) {\n    console.error('Rate limit exceeded. Please wait and retry.');\n  } else {\n    console.error('Agent error:', error.message);\n  }\n}\n```\n\n## Best Practices\n\n### DO\n\n✅ **Export tools as both individual exports and arrays**\n```typescript\nexport const tool1 = tool(...);\nexport const tool2 = tool(...);\nexport const myTools = [tool1, tool2];\n```\n\n✅ **Use Zod schemas with detailed descriptions**\n```typescript\nz.object({\n  query: z.string().describe('The search query to execute'),\n  limit: z.number().optional().describe('Maximum number of results (default: 10)'),\n})\n```\n\n✅ **Create MCP servers at query time, not module level**\n```typescript\n// GOOD: In function scope\nasync function runAgent() {\n  const server = createSdkMcpServer({ tools });\n  const messages = query({ options: { mcpServers: { 'name': server } } });\n}\n```\n\n✅ **Use closures for context-aware tools**\n```typescript\nexport function createTools(context: Context) {\n  return [\n    tool('name', 'desc', schema, async (params) => {\n      // Has access to context via closure\n      return processWithContext(context, params);\n    })\n  ];\n}\n```\n\n✅ **Include detailed system prompts (100+ lines)**\n\n✅ **Set `type: \"module\"` in package.json**\n\n✅ **Authentication handled automatically by Claude Code**\n\n### DON'T\n\n❌ **Don't create full MCP servers for simple tools**\n```typescript\n// AVOID: Creating MCP server infrastructure\nconst server = new MCPServer(...);\nserver.registerTool(...);\n// Use tool() wrapper instead\n```\n\n❌ **Don't use global state in tools**\n```typescript\n// BAD\nlet globalContext;\nexport const tool = tool('name', 'desc', schema, async () => {\n  return useGlobalContext(globalContext); // Brittle!\n});\n```\n\n❌ **Don't mix module types**\n```typescript\n// package.json should have \"type\": \"module\"\n// Don't mix require() and import\n```\n\n❌ **Don't skip Zod validation**\n```typescript\n// BAD: No schema validation\ntool('name', 'desc', {}, async (params: any) => { ... });\n```\n\n### Permission Modes\n\n**CRITICAL: Standalone servers must use `bypassPermissions`**\n\nWhen building standalone applications (Express servers, CLI tools), always use `'bypassPermissions'`:\n\n```typescript\n// ✅ CORRECT for standalone servers\nconst messages = query({\n  prompt,\n  options: {\n    permissionMode: 'bypassPermissions',  // Always use this!\n    systemPrompt,\n    mcpServers,\n  },\n});\n```\n\n**Why?** The `'interactive'` mode requires Claude Code to be running to show permission prompts. Standalone servers run independently and cannot display interactive prompts, leading to \"Claude Code process exited with code 1\" errors.\n\n```typescript\n// ❌ WRONG for standalone servers\npermissionMode: mode === 'execute' ? 'bypassPermissions' : 'interactive',\n// This will crash when mode is 'plan'!\n```\n\n**When to use each mode:**\n- `'bypassPermissions'` - Standalone servers, CLI tools, background agents\n- `'interactive'` - Only when running inside Claude Code with UI for approvals\n- `'allowedTools'` - When you want to restrict tool access programmatically\n\n## Common Patterns\n\n### CSV Processing Agent\n\n```typescript\nimport { tool } from '@anthropic-ai/claude-agent-sdk';\nimport { z } from 'zod';\nimport { parse } from 'csv-parse/sync';\nimport { stringify } from 'csv-stringify/sync';\n\nexport const readCsv = tool(\n  'read_csv',\n  'Read and parse CSV file',\n  z.object({\n    path: z.string(),\n  }).shape,\n  async ({ path }) => {\n    const content = await fs.readFile(path, 'utf-8');\n    const records = parse(content, { columns: true });\n    return {\n      content: [{\n        type: 'text',\n        text: JSON.stringify(records, null, 2),\n      }],\n    };\n  }\n);\n\nexport const writeCsv = tool(\n  'write_csv',\n  'Write data to CSV file',\n  z.object({\n    path: z.string(),\n    data: z.array(z.record(z.any())),\n  }).shape,\n  async ({ path, data }) => {\n    const csv = stringify(data, { header: true });\n    await fs.writeFile(path, csv, 'utf-8');\n    return {\n      content: [{\n        type: 'text',\n        text: `Wrote ${data.length} records to ${path}`,\n      }],\n    };\n  }\n);\n```\n\n### API Integration Tools\n\n```typescript\nexport const searchApi = tool(\n  'search_api',\n  'Search external API',\n  z.object({\n    query: z.string(),\n    filters: z.record(z.string()).optional(),\n  }).shape,\n  async ({ query, filters }) => {\n    const params = new URLSearchParams({ q: query, ...filters });\n    const response = await fetch(`https://api.example.com/search?${params}`);\n\n    if (!response.ok) {\n      return {\n        content: [{\n          type: 'text',\n          text: JSON.stringify({ error: `API error: ${response.statusText}` }),\n        }],\n        isError: true,\n      };\n    }\n\n    const data = await response.json();\n    return {\n      content: [{\n        type: 'text',\n        text: JSON.stringify(data, null, 2),\n      }],\n    };\n  }\n);\n```\n\n## Debugging Tips\n\n### 1. Enable Verbose Logging\n\n```typescript\nconst messages = query({\n  prompt,\n  options: {\n    systemPrompt,\n    mcpServers,\n    // Add debugging\n    onToolCall: (toolName, params) => {\n      console.log(`[TOOL CALL] ${toolName}`, params);\n    },\n    onToolResult: (toolName, result) => {\n      console.log(`[TOOL RESULT] ${toolName}`, result);\n    },\n  },\n});\n```\n\n### 2. Validate Tool Definitions\n\n```typescript\n// Add validation helper\nfunction validateTools(tools: any[]) {\n  for (const t of tools) {\n    if (!t.name) console.error('Tool missing name:', t);\n    if (!t.description) console.error('Tool missing description:', t);\n    if (!t.inputSchema) console.error('Tool missing schema:', t);\n  }\n}\n```\n\n### 3. Test Tools Independently\n\n```typescript\n// Test tools before using in agent\nasync function testTool() {\n  const result = await myTool.execute({ param1: 'test' });\n  console.log('Tool result:', result);\n}\n```\n\n## Resources\n\nThis skill references the following bundled resources:\n\n### references/\n- `api-reference.md` - Official SDK API documentation\n- `working-examples.md` - Complete examples from production implementations\n- `troubleshooting.md` - Common issues and solutions\n\n### assets/\n- `project-template/` - Starter template for new agent projects\n- `tsconfig-template.json` - Recommended TypeScript configuration\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "szweibel",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/1495183?v=4",
      "lastCommit": "2025-12-09T20:17:12Z",
      "createdAt": "2025-11-06T19:00:55Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:28.986Z"
  },
  {
    "id": "typescript-hook-writer",
    "source": {
      "type": "github-search",
      "repo": "pr-pm/prpm",
      "path": ".claude/skills/typescript-hook-writer/skill.md",
      "url": "https://github.com/pr-pm/prpm/blob/main/.claude/skills/typescript-hook-writer/skill.md"
    },
    "frontmatter": {
      "name": "typescript-hook-writer",
      "description": "Expert guidance for developing Claude Code hooks in TypeScript with shared utilities, esbuild compilation, and Vitest testing - distributes compiled JS while maintaining TypeScript development experience"
    },
    "content": "# TypeScript Hook Writer\n\nUse this skill when developing Claude Code hooks in TypeScript. This skill ensures you maintain type safety, shared utilities, proper build pipeline, and comprehensive testing while distributing single-file JavaScript bundles to users.\n\n## When to Use This Skill\n\n- Creating new TypeScript hooks for Claude Code\n- Setting up the hooks development environment\n- Adding shared utilities for hooks\n- Writing tests for hooks with Vitest\n- Building hooks for distribution\n- Publishing TypeScript-based hooks as PRPM packages\n\n## Why TypeScript for Hooks?\n\n**Advantages over bash:**\n- Type safety catches errors at compile time\n- Shared utility functions reduce code duplication\n- Better IDE support with autocomplete and refactoring\n- Easier to test with Vitest\n- More readable for complex logic\n- Strong validation with TypeScript interfaces\n\n**Trade-offs:**\n- Requires build step (esbuild)\n- Slightly larger bundle size (~2-3KB vs bash)\n- Users still just need Node.js (no TypeScript dependency)\n\n**When to use TypeScript hooks:**\n- Complex input validation or pattern matching\n- Hooks that share common logic\n- Hooks requiring automated testing\n- Teams familiar with TypeScript\n\n**When to stick with bash:**\n- Simple one-off hooks (< 20 lines)\n- Hooks that just call other CLI tools\n- Extreme performance requirements (though difference is negligible)\n\n## Project Structure\n\n```\npackages/hooks/\n├── package.json              # Build tooling (esbuild, tsx, vitest)\n├── tsconfig.json             # TypeScript configuration\n├── vitest.config.ts          # Test configuration\n├── scripts/\n│   └── build-all-hooks.ts    # Build script (compiles all hooks)\n├── shared/\n│   ├── types.ts              # Shared TypeScript interfaces\n│   ├── hook-utils.ts         # Shared utility functions\n│   └── hook-utils.test.ts    # Tests for shared utilities\n\n# Each hook lives in .claude/hooks/\n.claude/hooks/\n├── my-hook/\n│   ├── hook.json             # Hook configuration\n│   ├── README.md             # Documentation\n│   ├── src/\n│   │   ├── hook.ts           # TypeScript source (development)\n│   │   ├── hook.test.ts      # Vitest tests\n│   │   ├── hook-utils.ts     # Copied shared utilities\n│   │   └── types.ts          # Copied shared types\n│   └── dist/\n│       └── hook.js           # Compiled bundle (distributed)\n```\n\n## Setup: packages/hooks Infrastructure\n\n### 1. package.json\n\n```json\n{\n  \"name\": \"@prpm/hooks\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Build system and shared utilities for PRPM Claude Code hooks\",\n  \"private\": true,\n  \"type\": \"module\",\n  \"scripts\": {\n    \"build\": \"tsx scripts/build-all-hooks.ts\",\n    \"build:watch\": \"tsx scripts/build-all-hooks.ts --watch\",\n    \"test\": \"vitest\",\n    \"test:watch\": \"vitest --watch\",\n    \"test:run\": \"vitest run\",\n    \"test:coverage\": \"vitest run --coverage\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.10.0\",\n    \"esbuild\": \"^0.19.8\",\n    \"tsx\": \"^4.7.0\",\n    \"typescript\": \"^5.3.3\",\n    \"vitest\": \"^1.0.4\"\n  }\n}\n```\n\n### 2. tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n    \"lib\": [\"ES2022\"],\n    \"moduleResolution\": \"node\",\n    \"esModuleInterop\": true,\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"resolveJsonModule\": true,\n    \"types\": [\"node\"]\n  },\n  \"include\": [\"**/*.ts\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n### 3. vitest.config.ts\n\n```typescript\nimport { defineConfig } from 'vitest/config';\n\nexport default defineConfig({\n  test: {\n    globals: true,\n    environment: 'node',\n    include: ['**/*.test.ts'],\n    coverage: {\n      provider: 'v8',\n      reporter: ['text', 'json', 'html'],\n      exclude: [\n        'node_modules/',\n        'dist/',\n        '**/*.test.ts',\n        '**/scripts/**',\n      ],\n    },\n  },\n});\n```\n\n### 4. Build Script: scripts/build-all-hooks.ts\n\n```typescript\n#!/usr/bin/env tsx\n/**\n * Build script for compiling all Claude Code hooks to standalone JavaScript bundles\n */\n\nimport { build, BuildOptions } from 'esbuild';\nimport { readdirSync, statSync, existsSync, mkdirSync } from 'fs';\nimport { join, dirname } from 'path';\nimport { fileURLToPath } from 'url';\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\n\n// Root directory is app/ which is 3 levels up from packages/hooks/scripts/\nconst ROOT_DIR = join(__dirname, '../../..');\nconst HOOKS_DIR = join(ROOT_DIR, '.claude/hooks');\n\ninterface HookInfo {\n  name: string;\n  srcPath: string;\n  distPath: string;\n}\n\n/**\n * Find all hooks with TypeScript source files\n */\nfunction findHooks(): HookInfo[] {\n  const hooks: HookInfo[] = [];\n\n  if (!existsSync(HOOKS_DIR)) {\n    console.error(`Hooks directory not found: ${HOOKS_DIR}`);\n    return hooks;\n  }\n\n  const entries = readdirSync(HOOKS_DIR);\n\n  for (const entry of entries) {\n    const hookPath = join(HOOKS_DIR, entry);\n\n    // Skip non-directories and special directories\n    if (!statSync(hookPath).isDirectory() || entry === 'shared') {\n      continue;\n    }\n\n    const srcPath = join(hookPath, 'src/hook.ts');\n    const distPath = join(hookPath, 'dist/hook.js');\n\n    if (existsSync(srcPath)) {\n      hooks.push({\n        name: entry,\n        srcPath,\n        distPath,\n      });\n    }\n  }\n\n  return hooks;\n}\n\n/**\n * Build a single hook\n */\nasync function buildHook(hook: HookInfo): Promise<void> {\n  const buildOptions: BuildOptions = {\n    entryPoints: [hook.srcPath],\n    bundle: true,\n    platform: 'node',\n    target: 'node18',\n    outfile: hook.distPath,\n    format: 'cjs',\n    minify: false, // Keep readable for debugging\n    sourcemap: false,\n    logLevel: 'error',\n    banner: {\n      js: '#!/usr/bin/env node',\n    },\n  };\n\n  try {\n    // Ensure dist directory exists\n    const distDir = dirname(hook.distPath);\n    if (!existsSync(distDir)) {\n      mkdirSync(distDir, { recursive: true });\n    }\n\n    await build(buildOptions);\n    console.log(`✓ Built ${hook.name}`);\n\n    // Make the output file executable\n    const { chmodSync } = await import('fs');\n    chmodSync(hook.distPath, 0o755);\n  } catch (error) {\n    console.error(`✗ Failed to build ${hook.name}:`, error);\n    throw error;\n  }\n}\n\n/**\n * Build all hooks\n */\nasync function buildAll(watch: boolean = false): Promise<void> {\n  console.log('🔨 Building PRPM Claude Code hooks...\\n');\n\n  const hooks = findHooks();\n\n  if (hooks.length === 0) {\n    console.log('No hooks found to build.');\n    return;\n  }\n\n  console.log(`Found ${hooks.length} hooks:\\n`);\n\n  // Build all hooks in parallel\n  try {\n    await Promise.all(hooks.map(hook => buildHook(hook)));\n    console.log(`\\n✓ Built ${hooks.length} hooks successfully`);\n  } catch (error) {\n    console.error('\\n✗ Build failed');\n    process.exit(1);\n  }\n\n  if (watch) {\n    console.log('\\n👀 Watching for changes...');\n    console.log('⚠️  Watch mode not yet implemented. Run `npm run build` after changes.');\n  }\n}\n\n// Parse command line args\nconst args = process.argv.slice(2);\nconst watch = args.includes('--watch') || args.includes('-w');\n\n// Run build\nbuildAll(watch).catch(error => {\n  console.error('Build error:', error);\n  process.exit(1);\n});\n```\n\n## Shared Utilities Pattern\n\n### Why Copy Instead of Import?\n\nEach hook gets its own copy of shared utilities rather than importing from a shared package:\n\n**Benefits:**\n- Each hook is a standalone single-file bundle\n- No external dependencies at runtime\n- Simpler distribution (just one .js file)\n- No module resolution issues\n- Each hook can be updated independently\n\n**Trade-off:**\n- Slight code duplication (~1-2KB per hook)\n- Changes to shared utilities require rebuilding all hooks\n\n### shared/types.ts\n\nDefine TypeScript interfaces for hook input, exit codes, and options:\n\n```typescript\n/**\n * Shared TypeScript types for Claude Code hooks\n */\n\nexport interface HookInput {\n  session_id?: string;\n  transcript_path?: string;\n  current_dir?: string;\n  input?: {\n    file_path?: string;\n    command?: string;\n    content?: string;\n    old_string?: string;\n    new_string?: string;\n    [key: string]: any;\n  };\n  message?: string;\n  tool?: string;\n  [key: string]: any;\n}\n\nexport enum HookExitCode {\n  Success = 0,    // Continue operation\n  Error = 1,      // Log error but continue\n  Block = 2,      // Block operation (PreToolUse only)\n}\n\nexport interface ExecOptions {\n  skipOnMissing?: boolean;  // Exit successfully if command not found\n  background?: boolean;     // Run in background (don't wait)\n  timeout?: number;         // Timeout in milliseconds\n  env?: Record<string, string>; // Environment variables\n}\n\nexport interface PatternMatch {\n  matched: boolean;\n  pattern?: string;\n}\n```\n\n### shared/hook-utils.ts\n\nCommon utility functions used across hooks:\n\n```typescript\nimport { readFileSync, appendFileSync, existsSync } from 'fs';\nimport { execSync } from 'child_process';\nimport type { HookInput, HookExitCode, ExecOptions, PatternMatch } from './types';\n\n/**\n * Read and parse JSON from stdin\n */\nexport function readStdin(): HookInput {\n  try {\n    const input = readFileSync(0, 'utf-8');\n    return JSON.parse(input);\n  } catch (error) {\n    return {};\n  }\n}\n\n/**\n * Extract file path from hook input\n */\nexport function getFilePath(input: HookInput): string | undefined {\n  return input.input?.file_path;\n}\n\n/**\n * Extract command from hook input\n */\nexport function getCommand(input: HookInput): string | undefined {\n  return input.input?.command;\n}\n\n/**\n * Extract content from hook input\n */\nexport function getContent(input: HookInput): string | undefined {\n  return input.input?.content || input.input?.new_string;\n}\n\n/**\n * Check if file has one of the specified extensions\n */\nexport function hasExtension(filePath: string, extensions: string[]): boolean {\n  return extensions.some(ext => filePath.endsWith(ext));\n}\n\n/**\n * Check if a command exists in PATH\n */\nexport function commandExists(command: string): boolean {\n  try {\n    execSync(`command -v ${command}`, { stdio: 'ignore' });\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Execute a command with options\n */\nexport function execCommand(\n  command: string,\n  args: string[],\n  options: ExecOptions = {}\n): void {\n  // Check if command exists if skipOnMissing is true\n  if (options.skipOnMissing && !commandExists(command)) {\n    return;\n  }\n\n  const fullCommand = `${command} ${args.map(arg => `\"${arg}\"`).join(' ')}`;\n\n  if (options.background) {\n    // Run in background - don't wait for completion\n    execSync(`(${fullCommand} &)`, {\n      stdio: 'ignore',\n      timeout: options.timeout,\n      env: { ...process.env, ...options.env },\n    });\n  } else {\n    // Run synchronously\n    execSync(fullCommand, {\n      stdio: 'inherit',\n      timeout: options.timeout,\n      env: { ...process.env, ...options.env },\n    });\n  }\n}\n\n/**\n * Match file path against glob patterns\n */\nexport function matchesPattern(filePath: string, patterns: string[]): PatternMatch {\n  for (const pattern of patterns) {\n    // Convert glob pattern to regex\n    const regexPattern = pattern\n      .replace(/\\./g, '\\\\.')\n      .replace(/\\*/g, '.*')\n      .replace(/\\?/g, '.');\n\n    if (new RegExp(`^${regexPattern}$`).test(filePath)) {\n      return { matched: true, pattern };\n    }\n  }\n\n  return { matched: false };\n}\n\n/**\n * Append line to log file\n */\nexport function appendToLog(logFile: string, line: string): void {\n  try {\n    appendFileSync(logFile, line + '\\n', 'utf-8');\n  } catch {\n    // Fail silently\n  }\n}\n\n/**\n * Get current timestamp in YYYY-MM-DD HH:MM:SS format\n */\nexport function getTimestamp(): string {\n  return new Date().toISOString().replace('T', ' ').substring(0, 19);\n}\n\n/**\n * Log error message to stderr\n */\nexport function logError(message: string): void {\n  console.error(message);\n}\n\n/**\n * Log warning message to stderr\n */\nexport function logWarning(message: string): void {\n  console.error(message);\n}\n\n/**\n * Exit hook with specified code\n */\nexport function exitHook(code: HookExitCode): never {\n  process.exit(code);\n}\n\n// Re-export HookExitCode for convenience\nexport { HookExitCode } from './types';\n```\n\n## Creating a TypeScript Hook\n\n### Step 1: Create Hook Directory Structure\n\n```bash\nmkdir -p .claude/hooks/my-hook/src\nmkdir -p .claude/hooks/my-hook/dist\n```\n\n### Step 2: Copy Shared Utilities\n\nCopy `shared/types.ts` and `shared/hook-utils.ts` into the hook's `src/` directory:\n\n```bash\ncp packages/hooks/shared/types.ts .claude/hooks/my-hook/src/\ncp packages/hooks/shared/hook-utils.ts .claude/hooks/my-hook/src/\n```\n\n**Why copy instead of symlink?** Each hook becomes a standalone bundle when compiled. The build process bundles utilities into the final .js file.\n\n### Step 3: Write Hook Implementation\n\n`.claude/hooks/my-hook/src/hook.ts`:\n\n```typescript\n#!/usr/bin/env tsx\n/**\n * My Hook\n * Description of what this hook does\n */\n\nimport {\n  readStdin,\n  getFilePath,\n  hasExtension,\n  execCommand,\n  logError,\n  logWarning,\n  exitHook,\n  HookExitCode,\n} from './hook-utils';\n\nasync function main() {\n  // Read input from stdin\n  const input = readStdin();\n\n  // Extract file path\n  const filePath = getFilePath(input);\n  if (!filePath) {\n    exitHook(HookExitCode.Success);\n  }\n\n  // Validate file extension\n  const supportedExtensions = ['.ts', '.tsx', '.js', '.jsx'];\n  if (!hasExtension(filePath, supportedExtensions)) {\n    exitHook(HookExitCode.Success);\n  }\n\n  // Perform hook action\n  try {\n    execCommand('prettier', ['--write', filePath], {\n      skipOnMissing: true,\n      background: true,\n    });\n\n    exitHook(HookExitCode.Success);\n  } catch (error) {\n    logError(`Failed to format ${filePath}: ${error}`);\n    exitHook(HookExitCode.Error);\n  }\n}\n\nmain().catch(() => {\n  exitHook(HookExitCode.Success); // Don't block on errors\n});\n```\n\n### Step 4: Create hook.json Configuration\n\n`.claude/hooks/my-hook/hook.json`:\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Edit|Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"node .claude/hooks/my-hook/dist/hook.js\",\n            \"timeout\": 5000\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**Important:** Reference `dist/hook.js` (compiled), not `src/hook.ts` (source).\n\n### Step 4.5: Advanced Hook Configuration (Optional)\n\nAll hook types support optional fields for controlling execution behavior:\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [{\n      \"matcher\": \"Edit|Write\",\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"node .claude/hooks/my-hook/dist/hook.js\",\n        \"timeout\": 5000,\n        \"continue\": true,              // Whether Claude continues after hook (default: true)\n        \"stopReason\": \"string\",        // Message shown when continue is false\n        \"suppressOutput\": false,       // Hide stdout from transcript (default: false)\n        \"systemMessage\": \"string\"      // Warning message shown to user\n      }]\n    }]\n  }\n}\n```\n\n#### `continue` (boolean, default: true)\n\nControls whether Claude continues after hook execution.\n\n**When to use `false`:**\n- Security hooks that must block operations\n- Validation hooks that found critical errors\n- Hooks that require user intervention\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"node .claude/hooks/security-validator/dist/hook.js\",\n  \"continue\": false,\n  \"stopReason\": \"Security validation failed. Please review the detected issues before proceeding.\"\n}\n```\n\n**Exit code interaction:**\n- If hook exits with `HookExitCode.Block` (2): `continue` is ignored, operation is blocked\n- If hook exits with `HookExitCode.Success` (0) or `HookExitCode.Error` (1): `continue` field determines behavior\n\n#### `stopReason` (string)\n\nMessage displayed to user when `continue: false`. Should explain why execution stopped and what action is needed.\n\n```json\n{\n  \"continue\": false,\n  \"stopReason\": \"Pre-commit checks failed. Fix linting errors and try again.\"\n}\n```\n\n#### `suppressOutput` (boolean, default: false)\n\nHides hook stdout from transcript mode (Ctrl-R). Stderr is always shown.\n\n**When to use `true`:**\n- Hooks that produce verbose output\n- Debugging logs not useful to users\n- Noisy background operations\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"node .claude/hooks/cloud-sync/dist/hook.js\",\n  \"suppressOutput\": true  // Don't show sync progress in transcript\n}\n```\n\n**Note:** Always show critical errors via stderr (use `logError()`), as stderr is never suppressed.\n\n#### `systemMessage` (string)\n\nWarning or info message shown to user when hook executes. Useful for non-blocking warnings.\n\n**TypeScript example:**\n\n```typescript\n// In your hook.ts\nif (outdatedDeps.length > 0) {\n  logWarning(`Found ${outdatedDeps.length} outdated dependencies`);\n  // systemMessage in hook.json will also show to user\n}\n```\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"node .claude/hooks/dependency-checker/dist/hook.js\",\n  \"systemMessage\": \"⚠️  Some dependencies are outdated. Consider running 'npm update'.\"\n}\n```\n\n**Difference from `stopReason`:**\n- `systemMessage`: Informational, Claude continues\n- `stopReason`: Critical, requires `continue: false`\n\n### Step 5: Build the Hook\n\n```bash\ncd packages/hooks\npnpm build\n```\n\nOutput:\n```\n🔨 Building PRPM Claude Code hooks...\n\nFound 1 hooks:\n\n✓ Built my-hook\n\n✓ Built 1 hooks successfully\n```\n\nThe compiled hook is now at `.claude/hooks/my-hook/dist/hook.js` (~2-3KB single file).\n\n## Testing TypeScript Hooks\n\n### Step 1: Create Test File\n\n`.claude/hooks/my-hook/src/hook.test.ts`:\n\n```typescript\nimport { describe, it, expect, vi, beforeEach } from 'vitest';\n\n// Mock the hook-utils module\nvi.mock('./hook-utils', () => ({\n  readStdin: vi.fn(),\n  getFilePath: vi.fn(),\n  hasExtension: vi.fn(),\n  execCommand: vi.fn(),\n  logError: vi.fn(),\n  exitHook: vi.fn((code: number) => {\n    throw new Error(`EXIT_${code}`);\n  }),\n  HookExitCode: {\n    Success: 0,\n    Error: 1,\n    Block: 2,\n  },\n}));\n\ndescribe('my-hook', () => {\n  beforeEach(() => {\n    vi.clearAllMocks();\n  });\n\n  it('should exit successfully if no file path is provided', async () => {\n    const { readStdin, getFilePath, exitHook, HookExitCode } = await import('./hook-utils');\n\n    vi.mocked(readStdin).mockReturnValue({});\n    vi.mocked(getFilePath).mockReturnValue(undefined);\n\n    try {\n      await import('./hook');\n    } catch (error: any) {\n      expect(error.message).toBe('EXIT_0');\n    }\n\n    expect(exitHook).toHaveBeenCalledWith(HookExitCode.Success);\n  });\n\n  it('should format supported file types', async () => {\n    const { readStdin, getFilePath, hasExtension, execCommand, exitHook, HookExitCode } = await import('./hook-utils');\n\n    vi.mocked(readStdin).mockReturnValue({\n      input: { file_path: '/path/to/file.ts' },\n    });\n    vi.mocked(getFilePath).mockReturnValue('/path/to/file.ts');\n    vi.mocked(hasExtension).mockReturnValue(true);\n\n    try {\n      await import('./hook');\n    } catch (error: any) {\n      expect(error.message).toBe('EXIT_0');\n    }\n\n    expect(execCommand).toHaveBeenCalledWith(\n      'prettier',\n      ['--write', '/path/to/file.ts'],\n      expect.objectContaining({ background: true })\n    );\n  });\n\n  it('should skip unsupported file types', async () => {\n    const { readStdin, getFilePath, hasExtension, execCommand, exitHook } = await import('./hook-utils');\n\n    vi.mocked(readStdin).mockReturnValue({\n      input: { file_path: '/path/to/file.py' },\n    });\n    vi.mocked(getFilePath).mockReturnValue('/path/to/file.py');\n    vi.mocked(hasExtension).mockReturnValue(false);\n\n    try {\n      await import('./hook');\n    } catch (error: any) {\n      // Expected exit\n    }\n\n    expect(execCommand).not.toHaveBeenCalled();\n  });\n});\n```\n\n### Step 2: Run Tests\n\n```bash\ncd packages/hooks\npnpm test:run\n```\n\nOutput:\n```\n✓ .claude/hooks/my-hook/src/hook.test.ts (3 tests) 5ms\n\nTest Files  1 passed (1)\nTests  3 passed (3)\nDuration  182ms\n```\n\n### Step 3: Run Tests with Coverage\n\n```bash\npnpm test:coverage\n```\n\nCoverage report shows which code paths are tested.\n\n## Build Workflow: TypeScript → JavaScript\n\n### Development vs Distribution\n\n**Development files (NOT distributed):**\n- `.claude/hooks/my-hook/src/hook.ts` - TypeScript source\n- `.claude/hooks/my-hook/src/hook.test.ts` - Tests\n- `.claude/hooks/my-hook/src/types.ts` - Type definitions\n- `.claude/hooks/my-hook/src/hook-utils.ts` - Utilities\n\n**Distribution files (what users get):**\n- `.claude/hooks/my-hook/dist/hook.js` - Compiled JavaScript bundle (~2-3KB)\n- `.claude/hooks/my-hook/hook.json` - Hook configuration\n- `.claude/hooks/my-hook/README.md` - Documentation\n\n### Build Process\n\nThe build script (`packages/hooks/scripts/build-all-hooks.ts`) does the following:\n\n1. **Scans** `.claude/hooks/` for directories with `src/hook.ts`\n2. **Compiles** each hook with esbuild:\n   - Bundles all imports into single file\n   - Converts TypeScript to JavaScript\n   - Targets Node.js 18+\n   - Outputs CommonJS format\n   - Adds `#!/usr/bin/env node` shebang\n3. **Outputs** to `dist/hook.js` in each hook directory\n4. **Sets permissions** to make file executable (`chmod +x`)\n\n### When to Build\n\n**Automatic build:**\n- Publishing with `prpm publish` - `prepublishOnly` script builds automatically (if configured)\n\n**Manual build for:**\n- Testing hook locally before committing\n- Debugging compiled output\n- Verifying build succeeds before pushing\n\n**How to build manually:**\n\n```bash\n# Build all hooks once\ncd packages/hooks\npnpm build\n\n# Output:\n# 🔨 Building PRPM Claude Code hooks...\n# Found 7 hooks:\n# ✓ Built prettier-on-save\n# ✓ Built command-logger\n# ...\n# ✓ Built 7 hooks successfully\n```\n\n**Build output structure:**\n\n```\n.claude/hooks/my-hook/\n├── src/\n│   ├── hook.ts           ← TypeScript source (input)\n│   ├── hook-utils.ts\n│   └── types.ts\n└── dist/\n    └── hook.js           ← Compiled JavaScript (output)\n```\n\n**What gets bundled:**\n\nesbuild traces all imports and bundles them into a single `dist/hook.js`:\n\n```typescript\n// src/hook.ts imports these\nimport { readStdin, getFilePath } from './hook-utils';\nimport { HookExitCode } from './types';\n\n// dist/hook.js contains:\n// - All code from hook.ts\n// - All code from hook-utils.ts\n// - All type definitions (compiled to runtime checks)\n// - No external dependencies\n// - Total size: ~2-3KB\n```\n\n**Why this approach:**\n\n- ✅ Users don't need TypeScript or build tools\n- ✅ Single-file distribution is simple\n- ✅ No runtime dependencies (just Node.js)\n- ✅ Hooks load instantly (no module resolution)\n- ✅ Each hook is independent\n\n## Publishing TypeScript Hooks as PRPM Packages\n\n### Step 1: Build Hooks\n\n**IMPORTANT:** Always build before updating prpm.json or publishing:\n\n```bash\ncd packages/hooks\npnpm build\n```\n\nVerify dist files exist:\n\n```bash\nls -lh .claude/hooks/*/dist/hook.js\n# Should show compiled hooks with ~2-3KB size each\n```\n\n### Step 2: Update prpm.json\n\nAdd the hook to the root `prpm.json`:\n\n```json\n{\n  \"packages\": [\n    {\n      \"name\": \"my-hook\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Brief description of what the hook does\",\n      \"format\": \"claude\",\n      \"subtype\": \"hook\",\n      \"tags\": [\"formatting\", \"automation\", \"typescript\"],\n      \"files\": [\n        \".claude/hooks/my-hook/hook.json\",\n        \".claude/hooks/my-hook/dist/hook.js\",\n        \".claude/hooks/my-hook/README.md\"\n      ]\n    }\n  ]\n}\n```\n\n**Important files array:**\n- `hook.json` - Hook configuration\n- `dist/hook.js` - Compiled JavaScript (NOT src/hook.ts)\n- `README.md` - Documentation\n\n**Do NOT include:**\n- `src/` directory (source code)\n- `*.test.ts` files\n- `node_modules/`\n- Development files\n\n### Step 2: Create README.md\n\n`.claude/hooks/my-hook/README.md`:\n\n```markdown\n# My Hook\n\nBrief description of what this hook does.\n\n## What It Does\n\n- Automatically formats code after Claude edits files\n- Supports TypeScript, JavaScript, JSON, and Markdown\n- Runs in background (non-blocking)\n- Gracefully skips if Prettier not installed\n\n## Installation\n\n```bash\nprpm install @prpm/my-hook\n```\n\n## Requirements\n\n- Node.js 18+ (already required for Claude Code)\n- Prettier (optional): `npm install -g prettier`\n\n## Configuration\n\nThis hook activates on `PostToolUse` for `Edit` and `Write` tools.\n\nTo customize supported file extensions, fork and modify the source.\n\n## Examples\n\nWhen Claude writes a TypeScript file:\n```\nClaude: I'll create a new component...\n[Hook auto-formats component.tsx with Prettier]\n```\n\n## Troubleshooting\n\n**Hook not running?**\n- Check `.claude/settings.json` includes the hook\n- Verify `dist/hook.js` exists and is executable\n- Check transcript (Ctrl-R) for hook errors\n\n**Format not applying?**\n- Ensure Prettier is installed: `prettier --version`\n- Check Prettier config in project (`.prettierrc`)\n```\n\n### Step 3: Set Up Automatic Build Before Publishing\n\n**Good news:** PRPM now supports `prepublishOnly` scripts! Add this to your prpm.json:\n\n```json\n{\n  \"name\": \"prpm-packages\",\n  \"license\": \"MIT\",\n  \"repository\": \"https://github.com/username/repo\",\n  \"scripts\": {\n    \"prepublishOnly\": \"cd packages/hooks && npm run build\"\n  },\n  \"packages\": [\n    // ... your packages\n  ]\n}\n```\n\n**What happens:**\n- When you run `prpm publish`, the `prepublishOnly` script runs automatically\n- Hooks are compiled from TypeScript to JavaScript\n- If the build fails, publish is aborted (prevents publishing broken code)\n- If the build succeeds, publishing continues with up-to-date dist files\n\n**Manual build (for local testing):**\n\n```bash\ncd packages/hooks\nnpm run build\n```\n\nVerify `dist/hook.js` files exist:\n\n```bash\nls -lh .claude/hooks/*/dist/hook.js\n# Should show compiled hooks with ~2-3KB size each\n```\n\n### Step 4: Publish\n\n```bash\n# From project root\nprpm publish\n```\n\n**What happens automatically:**\n1. `prepublishOnly` script runs: `cd packages/hooks && npm run build`\n2. All hooks are compiled to dist/hook.js\n3. Packages are published with up-to-date compiled files\n\n**Users will receive:**\n- `hook.json` - Configuration\n- `dist/hook.js` - Single-file executable (~2-3KB)\n- `README.md` - Documentation\n\n**Users do NOT get:**\n- TypeScript source (src/ directory)\n- Build tooling\n- Tests\n\n**Why this matters:** The prepublishOnly script prevents publishing stale dist files. If you modify a hook's TypeScript source but forget to build, the build happens automatically before publish.\n\n## Best Practices\n\n### 1. Keep Hooks Fast\n\nTarget < 100ms execution time. Use background execution for slow operations:\n\n```typescript\n// BAD - blocks for 5 seconds\nexecCommand('npm', ['test']);\n\n// GOOD - runs in background\nexecCommand('npm', ['test'], { background: true });\n```\n\n### 2. Fail Gracefully\n\nNever crash. Handle missing tools:\n\n```typescript\nexecCommand('prettier', ['--write', filePath], {\n  skipOnMissing: true,  // Exit successfully if prettier not found\n  background: true,\n});\n```\n\n### 3. Use Type Guards\n\nValidate input shape:\n\n```typescript\nfunction isValidInput(input: HookInput): boolean {\n  return !!(input.input?.file_path && typeof input.input.file_path === 'string');\n}\n\nif (!isValidInput(input)) {\n  exitHook(HookExitCode.Success);\n}\n```\n\n### 4. Copy Shared Utilities\n\nAlways copy (not import) shared utilities into each hook's `src/` directory:\n\n```bash\ncp packages/hooks/shared/{types.ts,hook-utils.ts} .claude/hooks/my-hook/src/\n```\n\nThis ensures standalone compilation.\n\n### 5. Test Edge Cases\n\nTest with edge cases:\n\n```typescript\nit('should handle files with spaces', async () => {\n  const input = { input: { file_path: '/path/my file.ts' } };\n  // ...\n});\n\nit('should handle Unicode filenames', async () => {\n  const input = { input: { file_path: '/path/文件.ts' } };\n  // ...\n});\n\nit('should handle missing input fields', async () => {\n  const input = { input: {} };\n  // ...\n});\n```\n\n### 6. Use Descriptive Exit Codes\n\n```typescript\n// Success - continue operation\nexitHook(HookExitCode.Success);\n\n// Block - prevent operation (PreToolUse only)\nexitHook(HookExitCode.Block);\n\n// Error - log but continue\nexitHook(HookExitCode.Error);\n```\n\n### 7. Log to stderr\n\n```typescript\n// WRONG - pollutes transcript\nconsole.log('Processing file...');\n\n// RIGHT - logs to stderr\nlogError('⚠️  Warning: something happened');\nlogWarning('ℹ️  Info: skipping file');\n```\n\n## Common Patterns\n\n### Pattern: File Extension Filter\n\n```typescript\nconst supportedExtensions = ['.ts', '.tsx', '.js', '.jsx'];\n\nif (!hasExtension(filePath, supportedExtensions)) {\n  exitHook(HookExitCode.Success);\n}\n```\n\n### Pattern: Sensitive File Blocker\n\n```typescript\nconst blockedPatterns = ['.env', '.env.*', '*.pem', '*.key', '*credentials*'];\n\nconst match = matchesPattern(filePath, blockedPatterns);\nif (match.matched) {\n  logError(`⛔ Blocked: Cannot modify sensitive file '${filePath}'`);\n  logError(`   Pattern: ${match.pattern}`);\n  exitHook(HookExitCode.Block);\n}\n```\n\n### Pattern: Command Logger\n\n```typescript\nimport { join } from 'path';\nimport { homedir } from 'os';\n\nconst command = getCommand(input);\nif (!command) {\n  exitHook(HookExitCode.Success);\n}\n\nconst logFile = join(homedir(), '.claude-commands.log');\nconst logLine = `[${getTimestamp()}] ${command}`;\n\nappendToLog(logFile, logLine);\nexitHook(HookExitCode.Success);\n```\n\n### Pattern: Content Validator\n\n```typescript\nconst content = getContent(input);\nif (!content) {\n  exitHook(HookExitCode.Success);\n}\n\nconst dangerousPatterns = [\n  /password\\s*=\\s*[\"'][^\"']+[\"']/i,\n  /api[_-]?key\\s*=\\s*[\"'][^\"']+[\"']/i,\n  /AKIA[0-9A-Z]{16}/, // AWS access key\n];\n\nfor (const pattern of dangerousPatterns) {\n  if (pattern.test(content)) {\n    logWarning(`⚠️  Warning: Potential credential detected in ${filePath}`);\n    logWarning(`   Pattern matched: ${pattern}`);\n    break;\n  }\n}\n\nexitHook(HookExitCode.Success);\n```\n\n## Debugging TypeScript Hooks\n\n### 1. Test Compilation\n\n```bash\ncd packages/hooks\npnpm build\n```\n\nCheck for TypeScript errors.\n\n### 2. Test Execution Manually\n\n```bash\necho '{\"input\":{\"file_path\":\"/tmp/test.ts\"}}' | node .claude/hooks/my-hook/dist/hook.js\necho $?  # Check exit code\n```\n\n### 3. Check Hook Registration\n\nVerify hook appears in `.claude/settings.json`:\n\n```bash\ncat .claude/settings.json | jq '.hooks'\n```\n\n### 4. View Transcript\n\nRun Claude Code and check transcript (Ctrl-R) for hook execution:\n\n```\nPostToolUse hook: my-hook\n  command: node .claude/hooks/my-hook/dist/hook.js\n  exit: 0\n  duration: 47ms\n```\n\n### 5. Add Debug Logging\n\nTemporarily add debug output:\n\n```typescript\nlogError(`[DEBUG] Processing file: ${filePath}`);\nlogError(`[DEBUG] Extensions: ${JSON.stringify(supportedExtensions)}`);\n```\n\n## Migration: Bash to TypeScript\n\nConverting an existing bash hook to TypeScript:\n\n### Before (bash)\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\nINPUT=$(cat)\nFILE=$(echo \"$INPUT\" | jq -r '.input.file_path // empty')\n\nif [[ -z \"$FILE\" ]]; then\n  exit 0\nfi\n\nif ! command -v prettier &>/dev/null; then\n  exit 0\nfi\n\nprettier --write \"$FILE\" &\nexit 0\n```\n\n### After (TypeScript)\n\n```typescript\n#!/usr/bin/env tsx\nimport {\n  readStdin,\n  getFilePath,\n  execCommand,\n  exitHook,\n  HookExitCode,\n} from './hook-utils';\n\nasync function main() {\n  const input = readStdin();\n  const filePath = getFilePath(input);\n\n  if (!filePath) {\n    exitHook(HookExitCode.Success);\n  }\n\n  execCommand('prettier', ['--write', filePath], {\n    skipOnMissing: true,\n    background: true,\n  });\n\n  exitHook(HookExitCode.Success);\n}\n\nmain().catch(() => exitHook(HookExitCode.Success));\n```\n\n**Benefits:**\n- Type safety for `input` object\n- Shared utilities reduce code\n- Easier to test\n- Better IDE support\n\n## Quick Reference\n\n### Build Commands\n\n```bash\npnpm build              # Build all hooks once\npnpm build:watch        # Watch mode (not yet implemented)\npnpm test               # Run tests in watch mode\npnpm test:run           # Run tests once\npnpm test:coverage      # Run tests with coverage\n```\n\n### Hook Configuration Fields\n\n**Required:**\n- `type` - \"command\" or \"prompt\"\n- `command` - Path to compiled hook (e.g., \"node .claude/hooks/my-hook/dist/hook.js\")\n\n**Optional:**\n- `timeout` - Max execution time in ms (default: 60000)\n- `continue` - Continue after hook? (default: true)\n- `stopReason` - Message when continue=false\n- `suppressOutput` - Hide stdout from transcript (default: false)\n- `systemMessage` - Warning message to user\n\n### Exit Codes (HookExitCode enum)\n\n```typescript\nHookExitCode.Success = 0    // Continue operation\nHookExitCode.Error = 1      // Log error but continue\nHookExitCode.Block = 2      // Block operation (PreToolUse only)\n```\n\n### Hook Structure Checklist\n\n- [ ] Created `.claude/hooks/my-hook/src/hook.ts`\n- [ ] Copied `types.ts` and `hook-utils.ts` to `src/`\n- [ ] Created `hook.json` referencing `dist/hook.js`\n- [ ] Created `README.md` with installation and usage\n- [ ] Created `hook.test.ts` with test coverage\n- [ ] Built hook with `pnpm build`\n- [ ] Verified `dist/hook.js` exists and is executable\n- [ ] Added to `prpm.json` with correct files array\n- [ ] Tested manually with sample JSON input\n- [ ] Tested in real Claude Code session\n\n### prpm.json Files Array\n\n**Include:**\n- `.claude/hooks/my-hook/hook.json`\n- `.claude/hooks/my-hook/dist/hook.js`\n- `.claude/hooks/my-hook/README.md`\n\n**Exclude:**\n- `src/` directory\n- `*.test.ts` files\n- `node_modules/`\n- Development files\n\n### Common Utilities\n\n```typescript\nreadStdin()                          // Parse stdin JSON\ngetFilePath(input)                   // Extract file path\ngetCommand(input)                    // Extract command\ngetContent(input)                    // Extract content\nhasExtension(path, ['.ts', '.js'])   // Check extension\nmatchesPattern(path, ['*.env'])      // Glob matching\ncommandExists('prettier')            // Check command exists\nexecCommand('cmd', ['arg'], opts)    // Execute command\nappendToLog(file, line)              // Append to log\ngetTimestamp()                       // Current timestamp\nlogError(msg)                        // Log to stderr\nlogWarning(msg)                      // Log warning\nexitHook(HookExitCode.Success)       // Exit with code\n```\n\n## Resources\n\n- [Claude Code Hooks Docs](https://code.claude.com/docs/en/hooks)\n- [TypeScript Handbook](https://www.typescriptlang.org/docs/)\n- [esbuild Documentation](https://esbuild.github.io/)\n- [Vitest Documentation](https://vitest.dev/)\n- [PRPM Publishing Guide](https://prpm.dev/docs/publishing)\n",
    "github": {
      "stars": 72,
      "forks": 11,
      "owner": "pr-pm",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/239213145?v=4",
      "license": "MIT",
      "lastCommit": "2026-01-04T12:42:33Z",
      "createdAt": "2025-10-10T08:43:24Z",
      "topics": [
        "claude",
        "claude-code",
        "cursor",
        "cursor-ai-edit",
        "cursorrules",
        "package-manager",
        "prompts"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:30.338Z"
  },
  {
    "id": "create-config",
    "source": {
      "type": "github-search",
      "repo": "msalganik/sports-poetry-demo",
      "path": ".claude/skills/create_config/SKILL.md.old",
      "url": "https://github.com/msalganik/sports-poetry-demo/blob/main/.claude/skills/create_config/SKILL.md.old"
    },
    "frontmatter": {
      "name": "create-config",
      "description": "Create and manage configuration files for sports poetry generation with complete parameter collection"
    },
    "content": "Create and manage configuration files for sports poetry generation with complete parameter collection.\n\n## Description\n\nThis skill provides an interactive interface for creating configuration files for the sports poetry multi-agent workflow. It collects all required configuration parameters through natural language conversation and uses the `config_builder.py` Python API to validate and generate timestamped configuration files in the `output/configs/` directory.\n\n**This skill ALWAYS creates TWO files:**\n1. **config_{timestamp}.json** - The configuration file used by the orchestrator\n2. **generate_config_{timestamp}.py** - An executable Python script that can reproduce this configuration\n\nBoth files use the same timestamp and are saved to `output/configs/`.\n\n## When to Use This Skill\n\n- User wants to create a new configuration file\n- User asks to \"set up configuration\" or \"configure the demo\"\n- User provides sports and generation preferences\n- User needs help understanding configuration options\n\n## Parameters Collected\n\n### Required Parameters\n1. **sports** (list of 3-5 strings) - Sport names for poem generation\n2. **generation_mode** (\"template\" | \"llm\") - How poems should be generated\n\n### Conditional Parameters (only for LLM mode)\n3. **llm_provider** (\"together\" | \"huggingface\") - Which API provider to use\n4. **llm_model** (string) - Model identifier for the chosen provider\n\n### Optional Parameters (with smart defaults)\n5. **retry_enabled** (boolean) - Whether to retry failed agents (default: true)\n\n### Auto-Generated Parameters\n6. **session_id** (string) - Unique identifier, auto-generated from timestamp\n7. **timestamp** (string) - ISO 8601 timestamp, auto-generated\n\n## Conversation Flow\n\nThe skill follows a structured flow to collect all necessary information:\n\n### Step 1: Sports Selection (REQUIRED)\n\n**Prompt:**\n```\nI'll help you create a sports poetry configuration.\n\n**Sports Selection** (3-5 sports required)\n\nPlease specify which sports you'd like poems about.\n\nYou can:\n• List them directly: \"basketball, soccer, tennis\"\n• Use a category: \"winter sports\", \"ball sports\", \"water sports\"\n• Mix approaches: \"hockey and other winter sports\"\n\nWhat sports would you like?\n```\n\n**Validation:**\n- Must have 3-5 sports\n- No duplicates\n- No empty strings\n- Normalize to lowercase and trim whitespace\n\n**Error Handling:**\n```\nIf < 3 sports:\n  \"You provided {count} sports, but we need 3-5.\n   Would you like to add {3-count} more?\n   Suggestions: {suggest compatible sports}\"\n\nIf > 5 sports:\n  \"You provided {count} sports, but the limit is 5.\n   Please choose your top 5 sports from: {list}\"\n\nIf duplicates found:\n  \"I noticed '{sport}' appears {count} times.\n   Did you mean to include it once?\"\n```\n\n**Category Interpretation:**\n- \"winter sports\" → hockey, skiing, snowboarding, figure skating, curling (user picks 3-5)\n- \"ball sports\" → basketball, soccer, tennis, volleyball, baseball (user picks 3-5)\n- \"water sports\" → swimming, diving, water polo, surfing (user picks 3-5)\n- \"Olympic sports\" → too broad, ask user to narrow down\n\n### Step 2: Generation Mode (REQUIRED)\n\n**Prompt:**\n```\n**Generation Mode**\n\nHow should the poems be generated?\n\n1. **template** (recommended for testing)\n   • Pre-written poems for common sports\n   • Very fast (< 1 second total)\n   • Works offline, no API key needed\n   • Deterministic (same output every time)\n   • Best for: testing, demos, quick iterations\n\n2. **llm** (recommended for production)\n   • AI-generated unique poems for each sport\n   • Slower (~3-5 seconds per sport)\n   • Requires API key (free tiers available)\n   • Creative and varies each run\n   • Best for: final output, variety, creativity\n\nWhich mode would you like? [template/llm]\nDefault: template\n```\n\n**Validation:**\n- Must be \"template\" or \"llm\"\n- Case-insensitive matching\n- Handle typos: \"tempate\", \"templates\", \"ai\", \"llms\", \"gpt\"\n\n**Default Behavior:**\n- If user says \"use defaults\" or \"quick\": use template\n- If user doesn't specify: ask explicitly\n\n### Step 3: LLM Provider (CONDITIONAL - only if mode=llm)\n\n**Prompt:**\n```\n**LLM Provider** (for AI-generated poems)\n\nWhich API provider should generate the poems?\n\n1. **together** (recommended)\n   • Together.ai API\n   • Free tier: Llama-3.3-70B (high quality, no cost)\n   • Fast response times\n   • Setup: Sign up at https://together.ai/\n   • Get API key: https://api.together.xyz/settings/api-keys\n\n2. **huggingface**\n   • HuggingFace Inference API\n   • Free tier: Llama-3-8B\n   • Good for experimentation\n   • Setup: Sign up at https://huggingface.co/\n   • Get token: https://huggingface.co/settings/tokens\n\nWhich provider? [together/huggingface]\nDefault: together\n```\n\n**Validation:**\n- Must be \"together\" or \"huggingface\"\n- Handle variations: \"together.ai\", \"hf\", \"hugging face\"\n\n### Step 4: LLM Model (CONDITIONAL - only if mode=llm)\n\n**Prompt:**\n```\n**LLM Model**\n\nWhich model should be used?\n\nFor Together.ai (free tier models):\n• **meta-llama/Llama-3.3-70B-Instruct-Turbo-Free** (recommended)\n  - Highest quality, 70B parameters\n  - Best for creative poetry\n\n• meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-Free\n  - Faster, 8B parameters\n  - Good for quick iterations\n\n• meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo-Free\n  - Balanced option\n\nFor HuggingFace:\n• **meta-llama/Meta-Llama-3-8B-Instruct** (recommended)\n\nYou can also specify a custom model name.\n\nEnter model name or press Enter for default:\n```\n\n**Defaults:**\n- Together.ai: `meta-llama/Llama-3.3-70B-Instruct-Turbo-Free`\n- HuggingFace: `meta-llama/Meta-Llama-3-8B-Instruct`\n\n**Validation:**\n- Accept any string (custom models allowed)\n- If empty/default: use provider-specific default\n\n### Step 5: API Key Check (CONDITIONAL - only if mode=llm)\n\n**Action:** Check if appropriate API key is set in environment or claude.local.md\n\n**Step 5a: Check Environment Variables**\n\n**For Together.ai:**\n```python\nimport os\napi_key = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n**For HuggingFace:**\n```python\nimport os\napi_key = os.environ.get(\"HUGGINGFACE_API_TOKEN\")\n```\n\n**Step 5b: If not in environment, check .claude/claude.local.md**\n\nIf API key not found in environment variables, check for project-level claude.local.md:\n\n```python\nfrom pathlib import Path\nimport re\n\n# Check project .claude directory for claude.local.md\nlocal_config_path = Path.cwd() / \".claude\" / \"claude.local.md\"\n\napi_key_from_file = None\n\nif local_config_path.exists():\n    content = local_config_path.read_text()\n    # Look for API key patterns\n    if provider == \"together\":\n        match = re.search(r'TOGETHER_API_KEY[\"\\s:=]+([a-zA-Z0-9_-]+)', content)\n    else:  # huggingface\n        match = re.search(r'HUGGINGFACE_API_TOKEN[\"\\s:=]+([a-zA-Z0-9_-]+)', content)\n\n    if match:\n        api_key_from_file = match.group(1)\n```\n\n**If API key found in .claude/claude.local.md:**\n```\n✓ API key found in .claude/claude.local.md\n  Provider: {provider}\n  Key length: {len(api_key)} characters\n\nNote: The API key is stored in .claude/claude.local.md but not set as an environment variable.\nThe orchestrator will need to read it from this file or you can set it in your environment:\n  export {PROVIDER_UPPERCASE}_API_KEY=\"{api_key_from_file}\"\n\nContinue with LLM mode? [yes/no]\n```\n\n**If API key NOT found in environment or .claude/claude.local.md:**\n```\n⚠️  **API Key Required**\n\nTo use LLM mode with {provider}, you need to set an API key.\n\n{provider_uppercase}_API_KEY environment variable not found.\nAlso checked .claude/claude.local.md\n\n**Setup Instructions:**\n\nFor Together.ai:\n  1. Sign up: https://together.ai/\n  2. Get API key: https://api.together.xyz/settings/api-keys\n  3. Set in terminal:\n     export TOGETHER_API_KEY=\"your-key-here\"\n\n  Or add to .claude/claude.local.md:\n     TOGETHER_API_KEY: your-key-here\n\nFor HuggingFace:\n  1. Sign up: https://huggingface.co/\n  2. Get token: https://huggingface.co/settings/tokens\n  3. Set in terminal:\n     export HUGGINGFACE_API_TOKEN=\"your-token-here\"\n\n  Or add to .claude/claude.local.md:\n     HUGGINGFACE_API_TOKEN: your-token-here\n\n**What would you like to do?**\n1. I've set the API key - continue with LLM mode\n2. Switch to template mode (no API key needed)\n3. Cancel and set up API key later\n\nChoose [1/2/3]:\n```\n\n**If API key IS found in environment:**\n```\n✓ API key found for {provider}\n  Source: Environment variable\n  Key length: {len(api_key)} characters\n```\n\n### Step 6: Retry Behavior (OPTIONAL - always ask with default)\n\n**Prompt:**\n```\n**Retry Behavior** (optional)\n\nShould failed poetry agents be retried automatically?\n\n• **yes** (recommended)\n  - If a sport's poem generation fails, retry once\n  - Increases success rate\n  - Minimal additional time (~3-5 seconds per retry)\n\n• **no**\n  - If a sport fails, skip it and continue\n  - Faster completion\n  - May result in incomplete output\n\nEnable automatic retry? [yes/no]\nDefault: yes\n```\n\n**Validation:**\n- Accept: yes/no, true/false, y/n, 1/0, enable/disable\n- Default to true if not specified\n\n### Step 7: Configuration Summary (REQUIRED)\n\n**ALWAYS Display before creating file:**\n```\n**Configuration Summary**\n\n📋 Sports:          {sport1}, {sport2}, {sport3} ({count} sports)\n⚙️  Generation:      {mode}\n{if mode=llm:}\n🤖 LLM Provider:    {provider}\n🧠 LLM Model:       {model}\n🔑 API Key:         ✓ Set ({key_length} chars)\n{endif}\n🔄 Retry Enabled:   {retry_enabled}\n🆔 Session ID:      {session_id} (auto-generated)\n⏰ Timestamp:       {timestamp} (auto-generated)\n\n📁 Output files:    output/configs/config_{timestamp}.json\n                    output/configs/generate_config_{timestamp}.py\n{if mode=llm:}\n⏱️  Estimated time:  ~{count * 4} seconds ({count} sports × ~4s each)\n💰 API cost:        Free (using free tier model)\n{else:}\n⏱️  Estimated time:  < 1 second\n{endif}\n\nProceed with this configuration? [yes/no/edit]\n```\n\n**Actions:**\n- `yes`: Create both configuration files (JSON + generator script)\n- `no`: Cancel\n- `edit`: Ask which parameter to change and restart from that step\n\n### Step 8: File Creation\n\n**CRITICAL: This step MUST create TWO files:**\n1. **config_{timestamp}.json** - The configuration file\n2. **generate_config_{timestamp}.py** - The executable generator script\n\n**BOTH files are REQUIRED for every configuration. Do NOT skip the generator script.**\n\n**Using config_builder.py to create both config JSON and generator script:**\n\n```python\nfrom config_builder import ConfigBuilder, ConfigValidationError\nfrom pathlib import Path\nfrom datetime import datetime\nimport os\nimport stat\n\ntry:\n    # Always start from default config\n    builder = ConfigBuilder.load_default()\n\n    # Apply user-specified changes\n    builder.with_sports(sports_list)\n\n    if mode != \"template\":  # Only change if different from default\n        builder.with_generation_mode(mode)\n\n    if mode == \"llm\":\n        if provider != \"together\":  # Only change if different from default\n            builder.with_llm_provider(provider)\n        if model != \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\":  # Only change if different\n            builder.with_llm_model(model)\n\n    if not retry_enabled:  # Only change if different from default (true)\n        builder.with_retry(retry_enabled)\n\n    # Create output/configs directory if it doesn't exist\n    configs_dir = Path(\"output/configs\")\n    configs_dir.mkdir(parents=True, exist_ok=True)\n\n    # Generate timestamped filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    config_filename = f\"config_{timestamp}.json\"\n    config_path = configs_dir / config_filename\n\n    # Generator script filename (matching timestamp)\n    generator_filename = f\"generate_config_{timestamp}.py\"\n    generator_path = configs_dir / generator_filename\n\n    # Save configuration to output/configs directory\n    builder.save(str(config_path))\n\n    # Generate the Python script that created this config\n    script_content = f'''#!/usr/bin/env python3\n\"\"\"\nConfiguration Generator Script\nGenerated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\nOutput: {config_path}\n\nThis script was created by the create_config skill to generate\nthe sports poetry configuration with the following parameters:\n- Sports: {\", \".join(sports_list)}\n- Generation Mode: {mode}\n{f\"- Provider: {provider}\" if mode == \"llm\" else \"\"}\n{f\"- Model: {model}\" if mode == \"llm\" else \"\"}\n- Retry Enabled: {retry_enabled}\n\nThis script provides reproducibility and auditability for the configuration.\nYou can re-run it to regenerate the same configuration with a new timestamp.\n\"\"\"\n\nfrom config_builder import ConfigBuilder\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Load default configuration\nbuilder = ConfigBuilder.load_default()\n\n# Apply configuration settings\nbuilder.with_sports({sports_list!r})\n\nif {mode!r} != \"template\":\n    builder.with_generation_mode({mode!r})\n\nif {mode!r} == \"llm\":\n    if {provider!r} != \"together\":\n        builder.with_llm_provider({provider!r})\n    if {model!r} != \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\":\n        builder.with_llm_model({model!r})\n\nif not {retry_enabled!r}:\n    builder.with_retry({retry_enabled!r})\n\n# Create output directory\nconfigs_dir = Path(\"output/configs\")\nconfigs_dir.mkdir(parents=True, exist_ok=True)\n\n# Generate new timestamped filename\nnew_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nnew_config_path = configs_dir / f\"config_{{new_timestamp}}.json\"\n\n# Save configuration\nbuilder.save(str(new_config_path))\n\nprint(f\"✓ Configuration saved to: {{new_config_path}}\")\nprint(f\"\\\\nNext step: Run the orchestrator\")\nprint(f\"  python3 orchestrator.py --config {{new_config_path}}\")\n'''\n\n    # Write generator script to file\n    generator_path.write_text(script_content)\n\n    # Make script executable (chmod +x)\n    st = os.stat(generator_path)\n    os.chmod(generator_path, st.st_mode | stat.S_IEXEC | stat.S_IXGRP | stat.S_IXOTH)\n\n    print(f\"✓ Successfully created configuration files:\")\n    print(f\"  Config:    {config_path}\")\n    print(f\"  Generator: {generator_path}\")\n    print(f\"\\nThe generator script provides reproducibility - you can re-run it\")\n    print(f\"to create the same configuration with a new timestamp.\")\n    print(f\"\\nNext step: Run the orchestrator with this config\")\n    print(f\"  cd sports_poetry_demo\")\n    print(f\"  python3 orchestrator.py --config {config_path}\")\n\nexcept FileNotFoundError as e:\n    print(f\"❌ Error: {e}\")\n    print(f\"The config.default.json file is missing from the repository.\")\nexcept ConfigValidationError as e:\n    print(f\"❌ Configuration error: {e}\")\n    print(f\"Please try again or check your inputs.\")\nexcept Exception as e:\n    print(f\"❌ Unexpected error: {e}\")\n    print(f\"Please report this issue.\")\n```\n\n**Generator Script Features:**\n\n1. **Matching Timestamp**: Script uses same timestamp as config file\n   - `config_20251103_222333.json`\n   - `generate_config_20251103_222333.py`\n\n2. **Self-Documenting Header**: Includes all configuration details in docstring\n   - Generation timestamp\n   - Output file path\n   - All parameter values\n   - Purpose and usage notes\n\n3. **Executable**: Script is automatically made executable (`chmod +x`)\n   - Can be run directly: `./generate_config_20251103_222333.py`\n   - Or with Python: `python3 generate_config_20251103_222333.py`\n\n4. **Reproducible**: Re-running the script creates a new config with fresh timestamp\n   - Same settings, new timestamp\n   - Useful for batch operations or CI/CD\n\n5. **Version Controlled**: Scripts are gitignored (see below) but can be committed if needed for auditability\n\n## Usage Examples\n\n### Example 1: Quick Start (Minimal Input)\n\n```\nUser: \"Create a config for basketball, soccer, and tennis\"\n\nSkill: Great! I'll create a configuration starting from defaults:\n\n       Default settings:\n       • Sports: basketball, soccer, tennis (will use default)\n       • Mode: template (default)\n       • Retry: enabled (default)\n\n       You're using the default sports. Keep all defaults? [yes/no]\n\nUser: \"yes\"\n\nSkill: ✓ Successfully created configuration files:\n         Config:    output/configs/config_20251101_184500.json\n         Generator: output/configs/generate_config_20251101_184500.py\n\n       The generator script provides reproducibility - you can re-run it\n       to create the same configuration with a new timestamp.\n\n       Next step: python3 orchestrator.py --config output/configs/config_20251101_184500.json\n```\n\n### Example 2: LLM Mode (Full Flow)\n\n```\nUser: \"Set up a config using LLM mode\"\n\nSkill: **Sports Selection**\n       Which sports? (3-5 sports)\n\nUser: \"winter sports\"\n\nSkill: \"Winter sports\" could include: hockey, skiing, snowboarding,\n       figure skating, curling\n\n       Please choose 3-5 from this list:\n\nUser: \"hockey, skiing, snowboarding\"\n\nSkill: ✓ 3 sports selected\n\n       **Generation Mode**\n       You specified LLM mode.\n\n       **LLM Provider**\n       Which provider? [together/huggingface]\n\nUser: \"together\"\n\nSkill: **LLM Model**\n       Using default: meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n       Change? [yes/no]\n\nUser: \"no\"\n\nSkill: **API Key Check**\n       ✓ TOGETHER_API_KEY found (64 chars)\n\n       **Retry Behavior**\n       Enable retry? [yes/no] (default: yes)\n\nUser: \"yes\"\n\nSkill: **Summary**\n       📋 Sports: hockey, skiing, snowboarding\n       🤖 Provider: together\n       🧠 Model: Llama-3.3-70B-Instruct-Turbo-Free\n       🔄 Retry: enabled\n\n       Proceed? [yes/no]\n\nUser: \"yes\"\n\nSkill: ✓ Successfully created configuration files:\n         Config:    output/configs/config_20251101_184500.json\n         Generator: output/configs/generate_config_20251101_184500.py\n\n       The generator script provides reproducibility - you can re-run it\n       to create the same configuration with a new timestamp.\n```\n\n### Example 3: Error Recovery\n\n```\nUser: \"Create config for basketball, soccer\"\n\nSkill: ❌ Error: Only 2 sports provided (need 3-5)\n\n       You entered: basketball, soccer\n\n       Suggestions to add:\n       • tennis (complements ball sports)\n       • volleyball (another ball sport)\n       • baseball (similar skill set)\n\n       Please add 1 more sport:\n\nUser: \"tennis\"\n\nSkill: ✓ Updated to: basketball, soccer, tennis\n\n       [Continues with generation mode question...]\n```\n\n### Example 4: API Key Missing\n\n```\nUser: \"Create config with swimming, hockey, volleyball using LLM\"\n\nSkill: **LLM Provider**\n       [together selected]\n\n       **API Key Check**\n       ❌ TOGETHER_API_KEY not found\n\n       Setup instructions:\n       1. https://together.ai/ → Sign up\n       2. Get key: https://api.together.xyz/settings/api-keys\n       3. export TOGETHER_API_KEY=\"your-key\"\n\n       Options:\n       1. I've set it - continue\n       2. Use template mode instead\n       3. Cancel\n\nUser: \"2\"\n\nSkill: Switching to template mode...\n\n       **Summary**\n       📋 Sports: swimming, hockey, volleyball\n       ⚙️  Mode: template\n       🔄 Retry: enabled\n\n       Proceed? [yes]\n\nUser: \"yes\"\n\nSkill: ✓ Successfully created configuration files:\n         Config:    output/configs/config_20251101_184500.json\n         Generator: output/configs/generate_config_20251101_184500.py\n\n       Switched to template mode (no API key required).\n```\n\n## Error Handling\n\n### Sports Validation Errors\n\n**Too few sports:**\n```\n❌ Error: Need 3-5 sports (you provided {count})\n   Current: {list}\n   Please add {needed} more sport(s)\n```\n\n**Too many sports:**\n```\n❌ Error: Maximum 5 sports (you provided {count})\n   Current: {list}\n   Please choose your top 5\n```\n\n**Duplicates:**\n```\n❌ Error: Duplicate sport found\n   '{sport}' appears {count} times\n   Please provide unique sports\n```\n\n**Empty/Invalid:**\n```\n❌ Error: Invalid sport name detected\n   All sports must be non-empty strings\n   Please check your input\n```\n\n### Generation Mode Errors\n\n**Invalid mode:**\n```\n❌ Error: Invalid mode '{input}'\n   Valid options: 'template' or 'llm'\n   Did you mean 'llm'? [yes/no]\n```\n\n### LLM Configuration Errors\n\n**Invalid provider:**\n```\n❌ Error: Invalid provider '{input}'\n   Valid options: 'together' or 'huggingface'\n   Would you like to use 'together' (recommended)? [yes/no]\n```\n\n**Missing API key:**\n```\n❌ Error: API key required for LLM mode\n   {PROVIDER}_API_KEY not set\n   See setup instructions above\n```\n\n### File Creation Errors\n\n**Permission denied:**\n```\n❌ Error: Cannot write to output/configs/\n   Permission denied\n   Check directory permissions or try a different location\n```\n\n**File exists:**\n```\nNote: Each config is saved with a unique timestamp, so conflicts are rare.\nIf a config file already exists at the exact same second, a random suffix will be added.\n```\n\n## Sport Category Mappings\n\n### Predefined Categories\n\n**Winter Sports:**\n- hockey\n- skiing\n- snowboarding\n- figure skating\n- curling\n- ice skating\n- bobsled\n\n**Ball Sports:**\n- basketball\n- soccer\n- tennis\n- volleyball\n- baseball\n- football\n- golf\n- rugby\n\n**Water Sports:**\n- swimming\n- diving\n- water polo\n- surfing\n- rowing\n- sailing\n\n**Track & Field:**\n- running\n- sprinting\n- marathon\n- hurdles\n- long jump\n- high jump\n- javelin\n\n**Combat Sports:**\n- boxing\n- wrestling\n- judo\n- karate\n- taekwondo\n- fencing\n\n**Racquet Sports:**\n- tennis\n- badminton\n- squash\n- table tennis\n\n**Team Sports:**\n- basketball\n- soccer\n- volleyball\n- football\n- hockey\n- baseball\n- rugby\n\n**Individual Sports:**\n- tennis\n- golf\n- swimming\n- gymnastics\n- track and field\n\n## CRITICAL REQUIREMENT: Always Create Both Files\n\n**When executing this skill, you MUST create BOTH files:**\n1. `output/configs/config_{timestamp}.json`\n2. `output/configs/generate_config_{timestamp}.py` (and make it executable with chmod +x)\n\n**Failure to create both files is considered an incomplete execution of this skill.**\n\nThe generator script provides reproducibility and auditability. Without it, users cannot easily recreate configurations or track how they were generated.\n\n## Dependencies\n\nThis skill requires:\n- `config_builder.py` - Python API for config generation and validation\n- Python 3.7+ (for the config_builder module)\n\n## Notes\n\n- Config files are saved to `output/configs/config_{timestamp}.json`\n- Generator scripts are saved to `output/configs/generate_config_{timestamp}.py`\n- Timestamp format: `YYYYMMDD_HHMMSS` (shared between config and generator script)\n- Each config gets a unique timestamped filename to prevent overwrites\n- Generator scripts are automatically made executable (`chmod +x`)\n- API keys are validated but never displayed or logged\n- The orchestrator must be run with `--config` flag pointing to the created file\n- All user inputs are normalized (lowercase, trimmed) for consistency\n\n## Version Control and .gitignore\n\nGenerator scripts should typically be gitignored to avoid cluttering the repository with generated code. However, they can be committed if needed for auditability or compliance purposes.\n\n**Add to `.gitignore`:**\n```gitignore\n# Generated configuration scripts (auto-created by create_config skill)\noutput/configs/generate_config_*.py\n```\n\n**Rationale:**\n- ✓ Generator scripts are reproducible from the config JSON\n- ✓ Prevents repository bloat with auto-generated files\n- ✓ Config JSON files remain committed for session reproducibility\n- ✓ Scripts can still be manually committed when needed (e.g., audit requirements)\n\n**When to commit generator scripts:**\n- Compliance/audit requirements need code provenance\n- Debugging configuration generation issues\n- Documenting complex configuration workflows\n- CI/CD pipelines that need reproducible config generation\n",
    "github": {
      "stars": 2,
      "forks": 0,
      "owner": "msalganik",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/142116?v=4",
      "lastCommit": "2025-11-16T14:53:26Z",
      "createdAt": "2025-11-01T17:16:17Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:31.161Z"
  },
  {
    "id": "your-skill-name",
    "source": {
      "type": "github-search",
      "repo": "tryinhard1080/ClaudeClodeCommander",
      "path": "skills/Skill.md.template",
      "url": "https://github.com/tryinhard1080/ClaudeClodeCommander/blob/main/skills/Skill.md.template"
    },
    "frontmatter": {
      "name": "Your Skill Name",
      "description": "Clear description of what this Skill does and when to use it (200 chars max)",
      "version": "1.0.0",
      "dependencies": "python>=3.8, package>=version"
    },
    "content": "# Your Skill Name\n\n## Overview\n\nProvide a detailed overview of this Skill. Explain:\n- What problem it solves\n- When Claude should use it\n- What workflows it supports\n- How it helps users accomplish their goals\n\nThis section helps Claude understand the context and purpose of the Skill.\n\n## Instructions\n\n### Step-by-Step Process\n\n1. **First Step**\n   - Detailed instructions for the first action\n   - Include any specific requirements or constraints\n   - Provide examples if helpful\n\n2. **Second Step**\n   - Continue with clear, actionable instructions\n   - Be specific about what Claude should do\n   - Include decision points if applicable\n\n3. **Additional Steps**\n   - Add as many steps as needed\n   - Keep each step focused and clear\n\n### Guidelines and Rules\n\n- List any important rules or constraints\n- Define quality standards or requirements\n- Specify formatting preferences\n- Note any edge cases to handle\n\n## Examples\n\n### Example 1: Basic Usage\n\n**Input:**\n```\nDescribe the input scenario\n```\n\n**Expected Output:**\n```\nShow what the desired result looks like\n```\n\n### Example 2: Advanced Usage\n\n**Input:**\n```\nMore complex input scenario\n```\n\n**Expected Output:**\n```\nExpected result for this scenario\n```\n\n## When to Apply\n\nApply this Skill when:\n- Specific scenario 1\n- Specific scenario 2\n- Specific scenario 3\n\nDo NOT use this Skill for:\n- Counter-example 1\n- Counter-example 2\n\n## Resources\n\nReference additional files or resources if needed:\n- See `REFERENCE.md` for detailed specifications\n- Check `resources/` folder for templates and assets\n- Review `examples/` for more use cases\n\n## Technical Details\n\n### Dependencies\n\nList any required packages or tools:\n- Python: pandas>=1.5.0, numpy>=1.20.0\n- JavaScript: lodash>=4.17.21\n\n### File Structure\n\nIf your Skill includes multiple files, document the structure:\n```\nskill-name/\n├── Skill.md (this file)\n├── REFERENCE.md\n├── scripts/\n│   ├── helper.py\n│   └── processor.js\n└── resources/\n    └── templates/\n```\n\n### Scripts\n\nIf your Skill includes executable code, explain:\n- What each script does\n- When it should be run\n- What inputs it expects\n- What outputs it produces\n\n## Troubleshooting\n\nCommon issues and solutions:\n\n### Issue 1\n**Problem:** Description of the problem\n**Solution:** How to resolve it\n\n### Issue 2\n**Problem:** Another common issue\n**Solution:** Resolution steps\n\n## Version History\n\n- v1.0.0 (YYYY-MM-DD) - Initial implementation\n- v1.1.0 (YYYY-MM-DD) - Added feature X\n- v1.2.0 (YYYY-MM-DD) - Improved performance for Y\n\n## Notes\n\nAny additional notes, tips, or best practices:\n- Important consideration 1\n- Best practice 2\n- Tip for optimal usage 3\n",
    "github": {
      "stars": 1,
      "forks": 0,
      "owner": "tryinhard1080",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/79526771?v=4",
      "lastCommit": "2025-11-05T11:54:53Z",
      "createdAt": "2025-10-21T13:21:25Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:33.675Z"
  },
  {
    "id": "claude",
    "source": {
      "type": "github-search",
      "repo": "oneshot2001/OmniSight-dev",
      "path": ".claude/omnisight-skill.md",
      "url": "https://github.com/oneshot2001/OmniSight-dev/blob/main/.claude/omnisight-skill.md"
    },
    "frontmatter": {},
    "content": "# OMNISIGHT Development Skill\n\n> **Claude Skill for OMNISIGHT ACAP Development**\n> Version: 1.0\n> Last Updated: November 3, 2025\n\n## Skill Purpose\n\nThis skill guides Claude through OMNISIGHT-specific development workflows, including ACAP packaging, camera deployment, troubleshooting, and phase management.\n\n## Core Principles\n\n### 1. Camera Compatibility First\n**ALWAYS** check camera compatibility before recommending packages:\n\n| Camera Model | ARTPEC | Python 3 | Required Package |\n|--------------|--------|----------|------------------|\n| **M4228-LVE** | ARTPEC-8 | ✅ Yes | v0.4.2 (works with any) |\n| **P3285-LVE** | ARTPEC-9 | ❌ No | v0.4.2 (Native only) |\n| **P3265-LVE** | ARTPEC-9 | ❌ No | v0.4.2 (Native only) |\n| **Unknown** | Any | Unknown | v0.4.2 (safest choice) |\n\n**Rule**: When in doubt, recommend Phase 4 V3 (v0.4.2) - it works on ALL cameras.\n\n### 2. Package Verification\nBefore recommending deployment, ALWAYS verify:\n```bash\n# Check package contents\ntar tzf <package.eap> | grep -E \"omnisight|manifest.json|index.html\"\n\n# Must contain:\n# - ./omnisight (executable wrapper)\n# - ./manifest.json (ACAP manifest)\n# - ./html/index.html (web interface)\n# - ./package.conf (package config)\n```\n\n### 3. Log Analysis Protocol\nWhen analyzing deployment issues:\n\n1. **Extract error context** (10 lines before/after error)\n2. **Identify error category**:\n   - Missing file: `No such file or directory`\n   - Missing interpreter: `python3: not found`\n   - Permission: `Permission denied`\n   - Manifest: `Failed to convert string to integer`\n3. **Check package version** in logs (search for upload.cgi)\n4. **Verify file installation** path: `/usr/local/packages/omnisight/`\n5. **Propose specific fix** with verification steps\n\n## Workflow: Deployment Troubleshooting\n\n### Step 1: Analyze Logs\n```bash\n# Search for OMNISIGHT errors\ngrep -i \"omnisight\" system_log.txt | grep -E \"ERR|WARN\"\n\n# Check package upload\ngrep -i \"upload.cgi\" system_log.txt | grep \".eap\"\n\n# Verify service status\ngrep -i \"sdkomnisight.service\" system_log.txt\n```\n\n### Step 2: Identify Root Cause\n\n**Pattern Recognition**:\n\n| Error Pattern | Root Cause | Solution |\n|--------------|------------|----------|\n| `python3: not found` | Missing Python 3 | Deploy v0.4.2 |\n| `No such file or directory` (omnisight) | Wrong package version | Verify tar contents |\n| `Failed to convert string to integer for appId` | Manifest appId is string | Fix to numeric |\n| `Couldn't find page` | runMode: \"never\" or service crashed | Change to \"respawn\" |\n| No \"Open\" button | Missing settingPage or service not running | Check manifest + service |\n\n### Step 3: Provide Solution\n\n**Template**:\n```markdown\n## 🔴 Problem Identified\n\n[Clear description of what went wrong]\n\n**Evidence from logs**:\n```\n[Relevant log excerpts]\n```\n\n## ✅ Solution\n\n**Deploy**: [Specific package version and location]\n\n**Steps**:\n1. [Action 1]\n2. [Action 2]\n3. [Action 3]\n\n**Expected Result**:\n✅ [What should happen]\n\n**Verification**:\n- [ ] Check system log for errors\n- [ ] Verify service running\n- [ ] Confirm \"Open\" button appears\n- [ ] Test web interface\n```\n\n## Workflow: Building Packages\n\n### Decision Tree\n\n```\nIs this for testing/development?\n├─ YES → Use build-stub.sh (fast, no camera needed)\n└─ NO → Is Python available on target camera?\n    ├─ UNKNOWN → Use build-phase4-v3-native-eap.sh (universal)\n    ├─ NO → Use build-phase4-v3-native-eap.sh (native)\n    └─ YES → Use build-phase4-v3-native-eap.sh (works on both)\n```\n\n**Default Recommendation**: `./scripts/build-phase4-v3-native-eap.sh`\n\n### Build Verification Checklist\n\nAfter building, ALWAYS verify:\n```bash\n# 1. Package created\nls -lh output/*.eap\n\n# 2. Package size reasonable (>100KB, <500KB for Phase 4)\ndu -h output/*.eap\n\n# 3. Contains required files\ntar tzf output/*.eap | grep -E \"omnisight|manifest|html/index.html\"\n\n# 4. Manifest is valid JSON\ntar xzf output/*.eap manifest.json -O | python3 -m json.tool\n\n# 5. Wrapper script has shebang\ntar xzf output/*.eap omnisight -O | head -1\n# Should show: #!/bin/sh\n```\n\n## Workflow: Documentation Updates\n\n### When to Create New Documentation\n\n**Critical Issues** (always document):\n- Deployment failures that took >30 minutes to diagnose\n- Camera-specific incompatibilities discovered\n- Package bugs affecting multiple users\n- Breaking changes in ACAP SDK\n\n**Location Mapping**:\n```\nCritical deployment fix → docs/troubleshooting/deployment-fixes/\nUser guide → docs/guides/\nPhase-specific → docs/phase{1-4}/\nACAP platform info → docs/acap/\nArchitecture changes → docs/development/\n```\n\n### Documentation Template\n\n**For Deployment Fixes**:\n```markdown\n# [Camera Model] [Issue Description]\n\n**Date**: [YYYY-MM-DD]\n**Camera Model**: [Model]\n**Package Version**: [Version]\n**Issue**: [One-line summary]\n\n## Problem Identified\n\n[Detailed description with log evidence]\n\n## Root Cause\n\n[Technical explanation]\n\n## Solution\n\n[Step-by-step fix]\n\n## Prevention\n\n[How to avoid this in future]\n\n## Related Documentation\n\n- [Link to related docs]\n```\n\n## Workflow: Phase Management\n\n### Phase Status Tracking\n\n**Phase 1** (Stub Implementation - 60% complete):\n- Focus: Simulation and testing without hardware\n- Deliverables: Stub implementations, demo mode\n- Build: `./scripts/build-stub.sh`\n\n**Phase 2** (API Server - 100% complete):\n- Focus: Flask REST API with IPC to C core\n- Deliverables: Working API endpoints, React dashboard\n- Build: `./scripts/deprecated/build-phase2-eap.sh` (requires Python)\n\n**Phase 3** (Hardware Integration - In Progress):\n- Focus: VDO API, Larod API, MQTT swarm\n- Deliverables: Real camera integration\n- Requirements: Physical camera required\n\n**Phase 4** (Claude Flow + Universal Packaging - 100% complete):\n- Focus: Distributed AI coordination + Universal builds\n- Deliverables: v0.4.2 native package\n- Build: `./scripts/build-phase4-v3-native-eap.sh` ✅\n\n### Phase Transition Checklist\n\nBefore moving to next phase:\n- [ ] All phase deliverables complete\n- [ ] Tests passing\n- [ ] Documentation updated\n- [ ] Package built and tested\n- [ ] Known issues documented\n- [ ] Next phase prerequisites verified\n\n## Common Patterns\n\n### 1. Quick Diagnosis\n\nWhen user reports \"no Open button\":\n```bash\n# Check these in order:\n1. What package version was deployed? (check logs for upload.cgi)\n2. Is service running? (check systemd status)\n3. Does manifest have settingPage? (check manifest.json)\n4. Are files in /usr/local/packages/omnisight/?\n5. Any errors in system log?\n```\n\n### 2. Package Recommendation\n\nWhen user asks \"which package should I use?\":\n```markdown\n**Recommended Package**: Phase 4 V3 (v0.4.2)\n\n**Location**: `packages/current/OMNISIGHT_-_Precognitive_Security_042_aarch64.eap`\n\n**Why**: Works on all cameras regardless of Python availability\n\n**Camera Compatibility**: Universal ✅\n- ARTPEC-8 with Python: ✅ Works\n- ARTPEC-8 without Python: ✅ Works\n- ARTPEC-9 with Python: ✅ Works\n- ARTPEC-9 without Python: ✅ Works\n```\n\n### 3. Build Script Selection\n\nWhen user asks \"how to build?\":\n```markdown\n**For deployment**: `./scripts/build-phase4-v3-native-eap.sh`\n- Universal compatibility\n- No Python dependency\n- Includes web interface\n- Ready for production\n\n**For testing**: `./scripts/build-stub.sh`\n- Fast compilation\n- No camera needed\n- Demo mode included\n- Development only\n```\n\n## Error Recovery Patterns\n\n### Pattern: Python Not Found\n\n**Symptoms**:\n```\npython3: not found\nexit code 127\n```\n\n**Immediate Action**:\n```markdown\nThis camera doesn't have Python 3 installed.\n\n✅ **Solution**: Deploy v0.4.2 (Native build)\n\nPackage: `packages/current/OMNISIGHT_-_Precognitive_Security_042_aarch64.eap`\n\nThis version works without Python.\n```\n\n### Pattern: No Such File or Directory (omnisight)\n\n**Symptoms**:\n```\nFailed at step EXEC spawning /usr/local/packages/omnisight/omnisight: No such file or directory\n```\n\n**Diagnosis Steps**:\n1. Check what package was uploaded (version number in logs)\n2. Verify package contents: `tar tzf <package.eap> | grep omnisight`\n3. Compare deployed version vs recommended version\n\n**Likely Causes**:\n- Wrong package deployed (check version number)\n- Corrupted package (re-download)\n- Incomplete installation (reinstall)\n\n### Pattern: No \"Open\" Button\n\n**Diagnosis Checklist**:\n```markdown\n1. **Is service running?**\n   - Check toggle status\n   - Check system log for crashes\n\n2. **Is settingPage configured?**\n   - manifest.json must have: \"settingPage\": \"index.html\"\n\n3. **Does index.html exist?**\n   - Should be in /usr/local/packages/omnisight/html/\n\n4. **Is package correct version?**\n   - v0.4.2 is recommended\n   - Check upload.cgi logs for version\n\n5. **Is runMode correct?**\n   - Should be \"respawn\" for web servers\n   - \"never\" won't serve pages\n```\n\n## Quick Reference Commands\n\n### Package Management\n```bash\n# List package contents\ntar tzf <package.eap>\n\n# Extract specific file\ntar xzf <package.eap> manifest.json -O\n\n# Check package size\ndu -h <package.eap>\n\n# Verify manifest JSON\ntar xzf <package.eap> manifest.json -O | python3 -m json.tool\n```\n\n### Camera Operations (SSH)\n```bash\n# Install package\nacapctl install /tmp/<package.eap>\n\n# Start/stop service\nacapctl start omnisight\nacapctl stop omnisight\n\n# Check status\nacapctl status omnisight\n\n# View logs\ntail -100 /var/log/messages | grep omnisight\n\n# Uninstall\nacapctl uninstall omnisight\n```\n\n### Log Analysis\n```bash\n# Find OMNISIGHT errors\ngrep -i \"omnisight\" system_log.txt | grep ERR\n\n# Find package upload\ngrep -i \"upload.cgi\" system_log.txt | grep \".eap\"\n\n# Find Python errors\ngrep -i \"python3: not found\" system_log.txt\n\n# Service failures\ngrep -i \"sdkomnisight.service\" system_log.txt\n```\n\n## Communication Style\n\n### When Providing Solutions\n\n**DO**:\n- ✅ Provide specific package version and location\n- ✅ Show evidence from logs\n- ✅ Include verification steps\n- ✅ Explain why the issue occurred\n- ✅ Link to relevant documentation\n\n**DON'T**:\n- ❌ Say \"try this\" without explanation\n- ❌ Recommend deprecated packages\n- ❌ Skip verification steps\n- ❌ Ignore log evidence\n- ❌ Give generic answers\n\n### Template for Responses\n\n```markdown\n## 🔴 [Problem Summary]\n\nI've analyzed the [logs/screenshot/issue] and identified:\n\n**Evidence**:\n[Log excerpts or specific observations]\n\n## ✅ Solution\n\n**[Recommended Action]**\n\n[Specific steps with commands/paths]\n\n## Why This Works\n\n[Brief technical explanation]\n\n## Verification\n\nAfter deployment:\n- [ ] [Check 1]\n- [ ] [Check 2]\n- [ ] [Check 3]\n\n## Documentation\n\n[Link to relevant docs for details]\n```\n\n## Critical Rules\n\n### 1. Never Recommend Deprecated Packages\n\n**Deprecated** (Don't recommend):\n- ❌ build-phase2-eap.sh (Python required)\n- ❌ build-phase4-eap.sh (Python required)\n- ❌ build-phase4-fixed-eap.sh (Python required)\n- ❌ build-phase4-v2-eap.sh (Python required)\n\n**Current** (Always recommend):\n- ✅ build-phase4-v3-native-eap.sh (Universal)\n- ✅ build-stub.sh (Testing only)\n\n### 2. Always Verify Package Version\n\nBefore recommending deployment:\n```bash\n# User reports issue → First check what they deployed\ngrep \"upload.cgi\" logs.txt | grep \".eap\"\n# Extract version number from filename\n```\n\n### 3. Camera Compatibility Matrix is Gospel\n\n**Never guess** about Python availability. Reference the compatibility matrix:\n- M4228-LVE (ARTPEC-8): Has Python 3 ✅\n- P3285-LVE (ARTPEC-9): No Python 3 ❌\n- P3265-LVE (ARTPEC-9): No Python 3 ❌\n- Unknown: Assume no Python 3 (safest)\n\n### 4. Documentation Paths Matter\n\nAfter folder reorganization, ALWAYS use new paths:\n```\nOLD: PHASE4_PYTHON_DEPENDENCY_ISSUE.md\nNEW: docs/troubleshooting/deployment-fixes/PHASE4_PYTHON_DEPENDENCY_ISSUE.md\n\nOLD: QUICK_TROUBLESHOOTING.md\nNEW: docs/troubleshooting/QUICK_TROUBLESHOOTING.md\n\nOLD: ACAP_PACKAGING.md\nNEW: docs/acap/ACAP_PACKAGING.md\n```\n\n## Success Metrics\n\n### Deployment Success\n\n**Green Light Indicators**:\n```markdown\n✅ Service status: Running\n✅ System log: No errors for 5+ minutes\n✅ \"Open\" button: Visible in camera web UI\n✅ Web interface: Loads and shows v0.4.2\n✅ Server type: Shows \"Native\" (not Python)\n```\n\n### Build Success\n\n**Green Light Indicators**:\n```markdown\n✅ Package created: output/*.eap exists\n✅ Package size: 200-400KB (Phase 4 range)\n✅ Contains files: omnisight, manifest.json, html/\n✅ Manifest valid: JSON parses correctly\n✅ Wrapper script: Has #!/bin/sh shebang\n```\n\n## Troubleshooting Decision Tree\n\n```\nUser reports issue\n├─ No \"Open\" button?\n│  ├─ Check service running → If not running, check logs\n│  ├─ Check package version → If v0.4.0 or v0.4.1, recommend v0.4.2\n│  └─ Check manifest → Verify settingPage configured\n│\n├─ Service won't start?\n│  ├─ Check logs for \"python3: not found\" → Deploy v0.4.2\n│  ├─ Check logs for \"No such file\" → Wrong package, deploy v0.4.2\n│  └─ Check permissions → Verify omnisight is executable\n│\n├─ Web interface doesn't load?\n│  ├─ Service running? → If yes, check reverse proxy config\n│  ├─ Check html/index.html exists → Should be in package\n│  └─ Browser console errors? → May need to rebuild package\n│\n└─ Which package to use?\n   └─ ALWAYS recommend v0.4.2 (works on all cameras)\n```\n\n---\n\n## Skill Activation\n\nThis skill is active for all OMNISIGHT-related tasks including:\n- Package building\n- Deployment troubleshooting\n- Log analysis\n- Documentation updates\n- Camera compatibility questions\n- Build script selection\n- Phase management\n\n**Last Validated**: November 3, 2025 (P3265-LVE deployment fix)\n",
    "github": {
      "stars": 1,
      "forks": 0,
      "owner": "oneshot2001",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/24663843?v=4",
      "lastCommit": "2025-11-05T15:36:38Z",
      "createdAt": "2025-10-20T14:14:50Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:35.332Z"
  },
  {
    "id": "prompts",
    "source": {
      "type": "github-search",
      "repo": "dmccreary/microsims",
      "path": "docs/prompts/ooda-loop-from-skill.md",
      "url": "https://github.com/dmccreary/microsims/blob/main/docs/prompts/ooda-loop-from-skill.md"
    },
    "frontmatter": {},
    "content": "\n▗ ▗   ▖ ▖  Claude Code v2.0.36\n           Sonnet 4.5 · Claude Pro\n  ▘▘ ▝▝    /Users/danmccreary/Documents/ws/microsims\n\n> Use the microsim-p5 skill to create a new microsim called 'ooda' that is \nbased around the OODA_loop page on Wikipedia.  \nhttps://en.wikipedia.org/wiki/OODA_loop \n\n⏺ I'll help you create a new MicroSim about the OODA loop. Let me first fetch information\n  about the OODA loop from Wikipedia, then use the microsim-p5 skill to create the\n  simulation.\n\n⏺ Fetch(https://en.wikipedia.org/wiki/OODA_loop)\n  ⎿  Error: Request failed with status code 403\n\n⏺ Let me try a web search instead to gather information about the OODA loop:\n\n⏺ Web Search(\"OODA loop Observe Orient Decide Act John Boyd decision making\")\n  ⎿  Did 1 search in 20s                                                                   \n\n⏺ Perfect! Now I have good information about the OODA loop. Let me invoke the microsim-p5\n  skill to create an interactive educational simulation:\n\n> The \"microsim-p5\" skill is running \n\n> The \"microsim-p5\" skill is running \n\n⏺ Perfect! I now have all the information I need to create an OODA Loop MicroSim. Let me\n  create a comprehensive, educational simulation that demonstrates how the OODA loop works\n  and why speed through the loop provides competitive advantage.\n\n⏺ Bash(mkdir -p /Users/danmccreary/Documents/ws/microsims/docs/sims/ooda)\n  ⎿  (No content)\n\n⏺ Write(docs/sims/ooda/ooda.js)\n  ⎿  Wrote 225 lines to docs/sims/ooda/ooda.js                            \n     // OODA Loop MicroSim\n     // Demonstrates the Observe-Orient-Decide-Act decision-making cycle\n     // Created by: Claude and Dan McCreary\n     // Concept developed by: Colonel John Boyd, USAF\n     // Canvas dimensions - REQUIRED structure\n     let canvasWidth = 800;              // Initial width (responsive)\n     let drawHeight = 500;               // Drawing/simulation area height\n     let controlHeight = 120;            // Controls area height\n     let canvasHeight = drawHeight + controlHeight;\n     … +215 lines (ctrl+o to expand)\n\n⏺ Write(docs/sims/ooda/main.html)\n  ⎿  Wrote 22 lines to docs/sims/ooda/main.html                                   \n     <!DOCTYPE html>\n     <html lang=\"en\">\n     <head>\n         <meta charset=\"UTF-8\">\n         <title>OODA Loop MicroSim using P5.js 1.11.10</title>\n         <script src=\"https://cdn.jsdelivr.net/npm/p5@1.11.10/lib/p5.js\"></script>\n         <style>\n             body {\n                 margin: 0px;\n                 padding: 0px;\n     … +12 lines (ctrl+o to expand)\n\n⏺ Write(docs/sims/ooda/index.md)\n  ⎿  Wrote 167 lines to docs/sims/ooda/index.md                                            \n     ---\n     title: OODA Loop\n     description: An interactive simulation of the OODA Loop (Observe, Orient, Decide, Act)\n     decision-making cycle, demonstrating how speed through the loop provides competitive\n     advantage.\n     image: /sims/ooda/ooda.png\n     og:image: /sims/ooda/ooda.png\n     twitter:image: /sims/ooda/ooda.png\n     social:\n        cards: false\n     ---\n     … +157 lines (ctrl+o to expand)\n\n⏺ Read(src/microsim-schema/microsim-schema.json)\n  ⎿  Read 823 lines\n\n⏺ Write(docs/sims/ooda/metadata.json)\n  ⎿  Wrote 497 lines to docs/sims/ooda/metadata.json\n     {\n       \"microsim\": {\n         \"dublinCore\": {\n           \"title\": \"OODA Loop\",\n           \"creator\": [\"Claude\", \"Dan McCreary\"],\n           \"subject\": [\n             \"Decision Making\",\n             \"Strategy\",\n             \"Systems Thinking\",\n             \"Military Science\",\n     … +487 lines (ctrl+o to expand)\n\n⏺ Congratulations! Your OODA Loop MicroSim has been successfully created! 🎉\n\n  What Was Created\n\n  I've generated a complete educational MicroSim in /docs/sims/ooda/ with the following\n  files:\n\n  1. ooda.js - Interactive p5.js simulation showing two agents progressing through the OODA\n  decision cycle\n  2. main.html - HTML wrapper with p5.js CDN link\n  3. index.md - Comprehensive documentation with lesson plans and educational content\n  4. metadata.json - Complete Dublin Core metadata conforming to your schema\n\n  Key Features\n\n  The simulation demonstrates:\n  - Two agents (Blue and Red) moving through circular OODA loops\n  - Four color-coded phases: Observe (blue), Orient (green), Decide (yellow), Act (red)\n  - Speed controls for each agent (0.2 to 3.0)\n  - Cycle counters showing completed decision loops\n  - Start/Pause and Reset buttons\n  - Responsive width design\n\n  Testing Your MicroSim\n\n  Option 1: Test with MkDocs (Recommended)\n\n  cd /Users/danmccreary/Documents/ws/microsims\n  mkdocs serve\n  Then visit: http://localhost:8000/sims/ooda/\n\n  Option 2: Test JavaScript in p5.js Editor\n\n  1. Copy the contents of /docs/sims/ooda/ooda.js\n  2. Paste into https://editor.p5js.org/\n  3. Click Run to test the simulation\n\n  Option 3: Open HTML Directly\n\n  open /Users/danmccreary/Documents/ws/microsims/docs/sims/ooda/main.html\n\n  Next Steps\n\n  1. Create Screenshot: Capture a screenshot (1200×630px recommended) and save as\n  /docs/sims/ooda/ooda.png for social media previews\n  2. Add to Navigation: Edit mkdocs.yml to add the OODA Loop to your site navigation:\n  - OODA Loop: sims/ooda/index.md\n  3. Validate Metadata:\n  python src/validate-yaml-file.py docs/sims/ooda/index.md\n  4. Regenerate Gallery:\n  python src/mk-gallery.py\n  5. Deploy to Production:\n  mkdocs gh-deploy\n\n  The simulation effectively demonstrates Colonel John Boyd's OODA Loop concept, showing how\n   speed through the decision cycle provides competitive advantage - a key insight\n  applicable to business, military strategy, cybersecurity, and personal decision-making!\n\n> update the mkdocs.yml nav to include this new MicroSim \n\n⏺ Read(mkdocs.yml)\n  ⎿  Read 208 lines\n\n⏺ Update(mkdocs.yml)\n  ⎿  Updated mkdocs.yml with 1 addition                           \n       116        - Named Colors: sims/named-colors/index.md\n       117        - NeoPixel: sims/neopixel/index.md\n       118        - Neural Network: sims/neural-network/index.md\n       119 +      - OODA Loop: sims/ooda/index.md\n       120        - Particle Fields: sims/particle-fields/index.md\n       121        - Pendulum: sims/pendulum/index.md\n       122        - Polygon: sims/polygon/index.md\n\n⏺ Perfect! I've successfully added the OODA Loop to the navigation in mkdocs.yml at line\n  119, positioned alphabetically between \"Neural Network\" and \"Particle Fields\".\n\n  You can now test the navigation by running:\n\n  mkdocs serve\n\n  Then visit http://localhost:8000 and look for \"OODA Loop\" in the MicroSims section of the\n  navigation menu. The link should take you to the new simulation at\n  http://localhost:8000/sims/ooda/\n\n  When you're ready to deploy to production, run:\n\n  mkdocs gh-deploy\n\n> how many tokens did you use and how long did it take to generate this microsim? \n\n⏺ Based on the token usage tracking throughout our conversation:\n\n  Token Usage\n\n  Total tokens used: ~33,000 tokens\n\n  Breaking it down:\n  - Starting point (after your request): 20,654 tokens\n  - Current point (after updating mkdocs.yml): 53,625 tokens\n  - Difference: 32,971 tokens\n\n  This includes:\n  - Web search for OODA Loop information (~1,000 tokens)\n  - Loading the microsim-p5 skill instructions (~7,000 tokens)\n  - Generating all 4 files:\n    - ooda.js (~2,000 tokens)\n    - main.html (~500 tokens)\n    - index.md (~2,500 tokens)\n    - metadata.json (~4,000 tokens)\n  - Reading the metadata schema (~12,000 tokens)\n  - Reading and updating mkdocs.yml (~400 tokens)\n  - Todo list management and responses (~3,500 tokens)\n\n  Time Elapsed\n\n  Unfortunately, I don't have access to wall-clock time or timestamps, so I cannot tell you\n  the actual elapsed time. The system doesn't provide me with timing information - only\n  token counts.\n\n  However, the generation was quite efficient because I:\n  1. Created all files in sequence without needing corrections\n  2. Used the existing schema and templates effectively\n  3. Didn't require multiple iterations or debugging\n\n  If you're interested in performance metrics, you might be able to check the timestamps in\n  your Claude Code interface or the file creation times:\n\n  ls -la /Users/danmccreary/Documents/ws/microsims/docs/sims/ooda/\n\n  This will show when each file was created!\n\n\n\n\n\n\n\n",
    "github": {
      "stars": 7,
      "forks": 5,
      "owner": "dmccreary",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/674055?v=4",
      "license": "NOASSERTION",
      "lastCommit": "2025-12-15T00:23:55Z",
      "createdAt": "2023-11-21T20:12:32Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:38.304Z"
  },
  {
    "id": "cursor-internals",
    "source": {
      "type": "github-search",
      "repo": "dav-ell/blink",
      "path": ".cursor/rules/skills/cursor-internals/SKILL.mdc",
      "url": "https://github.com/dav-ell/blink/blob/main/.cursor/rules/skills/cursor-internals/SKILL.mdc"
    },
    "frontmatter": {
      "alwaysApply": "false"
    },
    "content": "# Cursor Internals Skill\n\n## Purpose\nSpecialized knowledge of Cursor IDE's internal architecture, authentication system, API structure, and cursor-agent CLI behavior discovered through reverse engineering.\n\n## When to Use\n- Integrating with Cursor's backend API\n- Understanding Cursor's authentication flow\n- Discovering and testing API endpoints\n- Troubleshooting authentication issues\n- Finding protobuf message definitions\n- Building tools that interact with Cursor's services\n- Working with cursor-agent CLI\n\n**Note:** For database operations, schema details, and complete data structures, see the **cursor-db skill**.\n\n## Quick Start\n\n### Authentication Overview\nCursor uses **Auth0 OAuth** for authentication with JWT bearer tokens.\n\n**Key URLs**:\n- Auth: `https://authentication.cursor.sh`\n- API: `https://api2.cursor.sh`\n- Telemetry: `https://api3.cursor.sh`\n\n**Required Headers**:\n```http\nAuthorization: Bearer <jwt_token>\nContent-Type: application/json\nX-Cursor-User-Id: auth0|user_<identifier>\nX-Cursor-Client-Version: 2.0.69\n```\n\n### Capturing Your Token\nTokens are stored in-memory only, not on disk. Capture via HTTPS interception:\n\n```bash\ncd ~/Documents/github/blink/tools\n./capture_cursor_auth.sh\n```\n\nTool will guide you through:\n1. Installing mitmproxy\n2. Installing SSL certificate\n3. Proxying Cursor traffic\n4. Extracting token from logs\n\nToken location after capture: `tools/.cursor_token` (git-ignored)\n\n## Architecture Deep Dive\n\n### Dual Protocol System\n\nCursor's API uses **two different protocols** depending on endpoint complexity:\n\n#### 1. JSON Endpoints (Simple Queries)\nWork with plain HTTP/JSON - accessible via curl/requests.\n\n**Working Endpoints**:\n- `/aiserver.v1.AiService/AvailableModels` - List 40+ AI models\n- `/aiserver.v1.AiService/GetDefaultModel` - Get default model\n- `/aiserver.v1.AiService/CheckFeaturesStatus` - Feature flags\n- `/aiserver.v1.AiService/KnowledgeBaseList` - Knowledge bases\n- `/aiserver.v1.AiService/CheckNumberConfig` - Usage quotas\n- `/aiserver.v1.AiService/AvailableDocs` - Documentation sources\n\n**Characteristics**:\n- Accept: `application/json`\n- Simple request/response\n- POST method with empty body `{}`\n- No streaming\n\n#### 2. Protobuf Endpoints (Complex Operations)\nRequire Protocol Buffers encoding via gRPC.\n\n**Endpoints** (Not accessible via plain JSON):\n- `/aiserver.v1.AiService/GetCompletionStream` - Chat/completions\n- `/aiserver.v1.ToolCallEventService/SubmitToolCallEvents` - Tool calls\n- `/aiserver.v1.AnalyticsService/Batch` - Analytics\n- `/aiserver.v1.AiService/CheckQueuePosition` - Queue status\n\n**Characteristics**:\n- Content-Type: `application/proto`\n- Binary protobuf encoding\n- Streaming support (SSE)\n- Connect-RPC protocol\n- User-Agent: `connect-es/1.6.1`\n\n### Network Stack\n\n```\nCursor IDE (Electron)\n    ↓\nconnect-es (gRPC-Web Client)\n    ↓ \nHTTP/2 + Protocol Buffers\n    ↓\napi2.cursor.sh (AWS us-east-1)\n    ↓\nAI Providers (Anthropic, OpenAI, etc.)\n```\n\n## Database\n\n**For complete database documentation, see the [cursor-db skill](../cursor-db/SKILL.mdc).**\n\n### Quick Reference\n\n**Location:** `~/Library/Application Support/Cursor/User/globalStorage/state.vscdb`\n\n**Key Patterns:**\n- `composerData:{uuid}` - Chat metadata\n- `bubbleId:{composer}:{uuid}` - Individual messages\n\n**Important:** Database operations require understanding the complete 69+ field bubble structure. See cursor-db skill for:\n- Complete schemas\n- Required fields for IDE compatibility\n- Query patterns\n- Write best practices\n- Troubleshooting guides\n\n## Machine Identifiers\n\nCursor tracks three machine identifiers:\n\n**Storage location**: `~/Library/Application Support/Cursor/User/storage.json`\n\n**IDs**:\n- `telemetry.machineId` - Machine identifier\n- `telemetry.devDeviceId` - Device UUID\n- `telemetry.macMachineId` - Mac-specific ID\n\n**Extraction script**:\n```bash\ncd ~/Documents/github/blink/tools\npython3 extract_cursor_auth.py\n```\n\nOutput: `cursor_auth_data.json` with all IDs\n\n## Authentication System\n\n### Token Structure\n\n**Format**: JWT (JSON Web Token)\n**Algorithm**: HS256\n\n**Payload**:\n```json\n{\n  \"sub\": \"auth0|user_<identifier>\",\n  \"time\": \"timestamp\",\n  \"randomness\": \"uuid\",\n  \"exp\": 1767531038,\n  \"iss\": \"https://authentication.cursor.sh\",\n  \"scope\": \"openid profile email offline_access\",\n  \"aud\": \"https://cursor.com\",\n  \"type\": \"session\"\n}\n```\n\n**Expiration**: Typically 53-60 days\n\n### Token Capture Process\n\n1. **Install mitmproxy**: `brew install mitmproxy`\n2. **Generate certificate**: Run mitmproxy once\n3. **Install cert**: Add to macOS System Keychain\n4. **Proxy Cursor**: Set `HTTP_PROXY` and `HTTPS_PROXY`\n5. **Use Cursor**: Chat feature generates API calls\n6. **Extract token**: From Authorization header in logs\n\n**Automated script**: `tools/capture_cursor_auth.sh`\n\n### Token Storage\n\n**Local storage**:\n- NOT in plain text files\n- NOT in database\n- NOT in keychain\n- Only in memory during runtime\n\n**After capture**:\n- Stored in `tools/.cursor_token` (git-ignored)\n- Use environment variables:\n  ```bash\n  export CURSOR_AUTH_TOKEN=\"$(cat tools/.cursor_token)\"\n  export CURSOR_USER_ID=\"auth0|user_<your_id>\"\n  ```\n\n## Working with the API\n\n### Python Example\n\n```python\nimport requests\nimport os\n\nclass CursorAPI:\n    def __init__(self):\n        self.base_url = \"https://api2.cursor.sh\"\n        self.token = os.getenv('CURSOR_AUTH_TOKEN')\n        self.user_id = os.getenv('CURSOR_USER_ID')\n        \n    def _headers(self):\n        return {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\",\n            \"X-Cursor-User-Id\": self.user_id,\n            \"X-Cursor-Client-Version\": \"2.0.69\"\n        }\n    \n    def list_models(self):\n        \"\"\"Get all available AI models\"\"\"\n        response = requests.post(\n            f\"{self.base_url}/aiserver.v1.AiService/AvailableModels\",\n            headers=self._headers(),\n            json={}\n        )\n        return response.json()\n    \n    def check_features(self):\n        \"\"\"Check enabled features\"\"\"\n        response = requests.post(\n            f\"{self.base_url}/aiserver.v1.AiService/CheckFeaturesStatus\",\n            headers=self._headers(),\n            json={}\n        )\n        return response.json()\n    \n    def get_quotas(self):\n        \"\"\"Check usage quotas\"\"\"\n        response = requests.post(\n            f\"{self.base_url}/aiserver.v1.AiService/CheckNumberConfig\",\n            headers=self._headers(),\n            json={}\n        )\n        return response.json()\n\n# Usage\napi = CursorAPI()\nmodels = api.list_models()\nprint(f\"Found {len(models['models'])} models\")\n```\n\n## Available AI Models\n\nCursor provides access to 40+ AI models:\n\n**Cursor's Own**:\n- `composer-1` - Cursor's agentic coding model (200k context)\n- `default` - General model\n\n**Claude (Anthropic)** - 8+ variants:\n- `claude-4.5-sonnet` - Latest model (200k context)\n- `claude-4.5-sonnet-thinking` - With reasoning\n- `claude-4.5-haiku` - Fast and cheap\n- `claude-4.1-opus` - Most powerful\n- Plus thinking and legacy variants\n\n**GPT-5 (OpenAI)** - 15+ variants:\n- `gpt-5` - Latest flagship\n- `gpt-5-codex` - Coding specialist\n- `gpt-5-codex-high` - High reasoning\n- `gpt-5-fast` - Priority processing (2x cost)\n- Multiple reasoning levels (low/medium/high)\n\n**Reasoning Models**:\n- `o3` - Deep reasoning\n- `o3-pro` - Most complex reasoning\n\n**Google**:\n- `gemini-2.5-pro` - 1M context\n- `gemini-2.5-flash` - Fast variant\n\n**Others**:\n- `grok-4` (xAI)\n- `grok-code-fast-1` - Free during promo\n- `deepseek-r1`, `deepseek-v3.1`\n- `kimi-k2-instruct`\n\n## Cursor-Agent CLI Deep Dive\n\n### Understanding cursor-agent Behavior\n\n**Installation:**\n```bash\ncurl https://cursor.com/install -fsS | bash\n```\n\n**Command Types:**\n\n1. **create-chat** - Generates chat ID only\n```bash\ncursor-agent create-chat\n# Returns: uuid (e.g., 3f1a6a8c-58d1-4fbe-81f7-1ad946d9c84e)\n# NOTE: Does NOT create database entry - entry created on first message\n```\n\n2. **Sending Messages** - Uses --resume flag\n```bash\ncursor-agent --print --force --resume <chat_id> \"Your prompt\"\n# Automatically includes ALL chat history\n# Writes both user and assistant messages to database\n# Creates composerData entry if it doesn't exist\n```\n\n### Critical Discovery: Database Entry Creation\n\n**Important:** `cursor-agent create-chat` ONLY generates a UUID. The actual database entry is created when:\n1. First message is sent via cursor-agent\n2. OR manually created via direct database write\n\n**Implication for REST APIs:**\n- After calling `/agent/create-chat`, the chat doesn't exist in database yet\n- Sending a message to a \"created\" chat will fail with 404\n- Solution: Auto-create composerData entry on first message if missing\n\n**For complete database structure requirements and investigation tools, see the [cursor-db skill](../cursor-db/SKILL.mdc).**\n\n## Quick Reference\n\n### Environment Setup\n\n```bash\n# Set credentials\nexport CURSOR_AUTH_TOKEN=\"<your_token>\"\nexport CURSOR_USER_ID=\"auth0|user_<your_id>\"\n\n# Or load from file\nexport CURSOR_AUTH_TOKEN=\"$(cat tools/.cursor_token)\"\n```\n\n### API Call Template\n\n```bash\ncurl -X POST https://api2.cursor.sh/aiserver.v1.AiService/<EndpointName> \\\n  -H \"Authorization: Bearer $CURSOR_AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Cursor-User-Id: $CURSOR_USER_ID\" \\\n  -H \"X-Cursor-Client-Version: 2.0.69\" \\\n  -d '{}'\n```\n\n\n### Status Codes\n\n- `200` - Success\n- `401` - Unauthorized (bad token)\n- `403` - Forbidden (expired token)\n- `404` - Not found (or protobuf required, or chat doesn't exist)\n- `429` - Rate limited\n- `503` - Service unavailable\n\n## Progressive Disclosure\n\n### Basic (Start Here)\n- Cursor uses OAuth/Auth0 authentication\n- Two API types: JSON (simple) and Protobuf (chat)\n- Token capture via mitmproxy\n- 40+ AI models available\n\n### Intermediate\n- Connect-RPC protocol for protobuf\n- SQLite database structure\n- Machine identifiers\n- Working code examples\n\n### Advanced\n- Protobuf definition discovery\n- gRPC reflection attempts\n- Binary traffic decoding\n- Custom gRPC client implementation\n\n## External Resources\n\n- **Connect-RPC**: https://connectrpc.com/\n- **Protocol Buffers**: https://protobuf.dev/\n- **gRPC**: https://grpc.io/\n- **mitmproxy**: https://mitmproxy.org/\n- **Auth0**: https://auth0.com/docs\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "dav-ell",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/17986781?v=4",
      "lastCommit": "2025-12-30T14:28:10Z",
      "createdAt": "2025-11-11T18:22:09Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:40.885Z"
  },
  {
    "id": "beyond-mcp-v2",
    "source": {
      "type": "github-search",
      "repo": "basher83/lunar-claude",
      "path": "docs/research/beyond-mcp-v2/04-skill.md",
      "url": "https://github.com/basher83/lunar-claude/blob/main/docs/research/beyond-mcp-v2/04-skill.md"
    },
    "frontmatter": {},
    "content": "# Skill Approach Analysis\n\n## 1. Skill Description Comparison\n\n### Kalshi-Markets Skill (beyond-mcp)\n**Description:** \"Access Kalshi prediction market data including market prices, orderbooks, trades, events, and series information. Use when the user asks about prediction markets, Kalshi markets, betting odds, market prices, or needs to search or analyze prediction market data.\"\n\n**Trigger keywords:** prediction markets, Kalshi markets, betting odds, market prices, search prediction market data, analyze prediction market data\n\n**Capability visibility:** HIGH\n- Explicit list of capabilities in description (market prices, orderbooks, trades, events, series)\n- 10 scripts named directly in SKILL.md with \"When to use\" for each\n- Clear progressive disclosure pattern with script names visible upfront\n\n**Lines in SKILL.md:** 71 (compact but comprehensive)\n\n### Claude-Mem Search Skill (current)\n**Description:** \"Search claude-mem persistent memory for past sessions, observations, bugs fixed, features implemented, decisions made, code changes, and previous work. Use when answering questions about history, finding past decisions, or researching previous implementations.\"\n\n**Trigger keywords:** past sessions, observations, bugs fixed, features implemented, decisions made, code changes, previous work, history, past decisions, previous implementations\n\n**Capability visibility:** MEDIUM-HIGH\n- Good list of capabilities in description\n- \"When to Use This Skill\" section lists 8 specific user questions\n- BUT: Operations are hidden behind links - not immediately visible\n- Quick Decision Guide requires loading skill first\n\n**Lines in SKILL.md:** 96 (more verbose, more indirection)\n\n### Comparison\n**Winner for discoverability:** Kalshi-Markets\n\n**Key differences:**\n1. **Frontmatter description length:**\n   - Kalshi: 3 sentences, very concrete (market prices, orderbooks, trades)\n   - Claude-mem: 2 sentences, somewhat abstract (observations, decisions, code changes)\n\n2. **Trigger keyword specificity:**\n   - Kalshi: Domain-specific terms (Kalshi, prediction markets, betting odds)\n   - Claude-mem: Generic dev terms (bugs, features, history, decisions)\n\n3. **Capability surfacing:**\n   - Kalshi: ALL 10 scripts listed directly in SKILL.md with \"When to use\"\n   - Claude-mem: 9 operations hidden behind links in operations/ subdirectory\n\n4. **Progressive disclosure balance:**\n   - Kalshi: Script names + purpose visible; implementation hidden\n   - Claude-mem: Operation names + links visible; details hidden behind two layers\n\n**Missing from claude-mem:**\n1. Direct listing of all capabilities in main SKILL.md (not behind links)\n2. Concrete examples in description (e.g., \"search for bug #1234\" vs abstract \"bugs fixed\")\n3. Simpler mental model (10 scripts vs 9 operations + workflows + formatting)\n4. More specific domain triggers (claude-mem, persistent memory, session history)\n\n## 2. Implementation Pattern Analysis\n\n**Kalshi pattern:**\n- SKILL.md structure:\n  - Frontmatter with rich description and triggers\n  - Direct list of 10 scripts with one-line purposes\n  - Architecture section explaining self-contained pattern\n  - Quick start with command examples\n- Scripts approach:\n  - Each script is 150-200 lines, self-contained\n  - Embedded HTTP client in every script\n  - All support --help and --json\n  - No dependencies between scripts\n- Progressive disclosure:\n  - First layer (SKILL.md): Script names and purposes (71 lines)\n  - Second layer (scripts/): Full implementation (150-200 lines each)\n  - Total: ~1500 lines across 10 files, load individually\n\n**Claude-mem pattern:**\n- SKILL.md structure:\n  - Frontmatter with description\n  - \"When to Use\" section with 8 examples\n  - \"Quick Decision Guide\" with links\n  - \"Available Operations\" with 10 links\n  - \"Common Workflows\" link\n  - \"Response Formatting\" link\n  - Technical notes, performance tips, error handling\n- Operations docs:\n  - 9 operation files (1.4-6.2k each)\n  - 2 meta files (workflows, formatting)\n  - Each operation: When to use, curl command, parameters, response structure, use cases, presentation format\n- Progressive disclosure:\n  - First layer (SKILL.md): Operation categories and links (96 lines)\n  - Second layer (operations/*.md): Full instructions (1.4-6.2k per file)\n  - Total: ~30k total characters across 12 files, unclear which to load first\n\n**Better for autonomy:** Kalshi pattern\n\n**Reasoning:**\n1. **Cognitive load:** 10 scripts with clear names vs 9 operations + 2 meta files + decision trees\n2. **Action clarity:** \"Run status.py\" vs \"Read operations/recent-context.md then run curl command\"\n3. **Self-contained execution:** Scripts have embedded logic; operations require curl + parsing\n4. **Discovery path:** Kalshi shows all tools immediately; claude-mem requires reading links to understand capabilities\n5. **Token efficiency:** Kalshi loads 150-200 lines per task; claude-mem must load SKILL.md (96) + operation file (1.4-6.2k) + possibly formatting.md (6.2k)\n\n## 3. Auto-Invocation Evidence\n\n**README claims:** \"Model-Invoked - Claude autonomously activates based on context\" and \"The skill automatically activates when Claude detects keywords like: prediction markets, Kalshi markets, betting odds, market prices\"\n\n**Evidence found:**\n1. **Claim source:** README.md line 7 and lines 19-23\n2. **Mechanism described:** \"Claude (detects keyword) -> Loads SKILL.md -> Runs relevant script -> Kalshi API\"\n3. **No testing evidence:** No test logs, no metrics, no user reports, no examples of auto-invocation\n4. **No verification code:** No tests that verify auto-invocation works\n5. **Marketing language:** \"Model-Invoked\", \"autonomously activates\", \"automatically activates\" are aspirational claims\n6. **Keyword list provided:** This suggests the author WANTS auto-invocation but doesn't prove it works\n\n**Actual evidence pattern:**\n- This is a DEMONSTRATION/EXAMPLE project in beyond-mcp\n- Claims are based on Claude Code skill design pattern expectations\n- No empirical evidence of autonomous invocation\n- Keywords in description are BEST PRACTICE, not proof of auto-invocation\n\n**Confidence in claim:** LOW\n\n**Reality check:** The skill follows best practices for discoverability (good trigger keywords, clear description), but there's no evidence that Claude Code actually auto-invokes skills reliably. The claims are likely based on:\n1. Understanding of how skill descriptions work\n2. Belief that good keywords = auto-invocation\n3. Marketing the skill approach vs MCP approach\n\n## 4. Root Cause Analysis: Why Claude Doesn't Use claude-mem\n\n### Hypothesis A: Weak Description\n**Evidence:**\n- Description uses generic terms (bugs, features, history) that apply to ANY development work\n- Missing domain-specific triggers like \"claude-mem\", \"persistent memory\", \"session history\"\n- No concrete examples in description (e.g., \"find bug #1234 details\" vs \"bugs fixed\")\n- Trigger keywords compete with native Claude capabilities (Claude already remembers recent conversation)\n\n**Fix effort:** LOW - Rewrite SKILL.md frontmatter description (1-2 hours)\n\n**Likelihood:** HIGH\n\nThis is the most likely root cause. Compare:\n- Kalshi: \"prediction markets, Kalshi markets, betting odds\" (unique, specific domain)\n- Claude-mem: \"bugs fixed, features implemented, decisions made\" (generic, applies to any dev work)\n\nWhen user says \"what bugs did we fix?\", Claude can answer from current session context. No trigger to think \"I need to check persistent memory across sessions.\"\n\n### Hypothesis B: Progressive Disclosure Tradeoff\n**Evidence:**\n- SKILL.md shows 9 operations but hides details behind links\n- User must ask about \"past work\" -> Claude loads SKILL.md -> Claude must read operations/*.md to know what's possible\n- Compare to Kalshi: User asks about \"markets\" -> Claude loads SKILL.md -> Claude sees all 10 scripts immediately\n- claude-mem requires TWO reads (SKILL.md + operation file), Kalshi requires ONE read (SKILL.md lists everything)\n- Token burden: claude-mem forces loading SKILL.md (96 lines) first, then operations (1.4-6.2k), total 1.5-6.3k\n- Token burden: Kalshi loads SKILL.md (71 lines), done - all tools visible\n\n**Fix effort:** MEDIUM - Rebalance what's in SKILL.md vs operations/ (4-8 hours)\n\n**Likelihood:** MEDIUM\n\nThis is a contributing factor. Even if Claude loads the skill, it doesn't immediately know what claude-mem can DO without reading additional files. Kalshi shows all capabilities in 71 lines.\n\n### Hypothesis C: Pattern Limitation\n**Evidence:**\n- Skills are passive documentation, not active agents\n- Claude must CHOOSE to invoke skill tool\n- No evidence that ANY skill reliably auto-invokes (including Kalshi)\n- MCP tools appear in tool list automatically; skills require explicit invocation decision\n- Skill pattern depends on LLM detecting keywords and deciding to load skill\n- This is an LLM routing problem, not a documentation problem\n\n**Fix effort:** HIGH - Change to MCP server pattern or slash command pattern (2-4 weeks)\n\n**Likelihood:** MEDIUM\n\nThis is possible but less likely to be the PRIMARY cause. The skill pattern CAN work if the description is good enough. The issue is likely that claude-mem's description isn't triggering the \"load this skill\" decision.\n\n**Most likely root cause:** A (Weak Description)\n\n**Combined factors:** A + B (Weak description AND too much hidden behind links)\n\nThe description doesn't clearly differentiate claude-mem's persistent memory from Claude's native conversation memory. Users ask \"what did we do?\" and Claude answers from current session context, never thinking to check a persistent database across sessions. Add to this that even IF Claude loads the skill, it must read additional files to understand capabilities, making it less likely to succeed.\n\n## 5. Specific Recommendations for claude-mem\n\n### Quick Wins (Hours)\n\n#### 1. Rewrite SKILL.md frontmatter description with unique triggers\n**Current:**\n```yaml\ndescription: Search claude-mem persistent memory for past sessions, observations, bugs fixed, features implemented, decisions made, code changes, and previous work. Use when answering questions about history, finding past decisions, or researching previous implementations.\n```\n\n**Recommended:**\n```yaml\ndescription: Search claude-mem's persistent cross-session memory database to find work from previous conversations. Access past session summaries, bug fixes, feature implementations, and decisions that are NOT in the current conversation context. Use when user asks about work from days/weeks/months ago, previous sessions, or \"did we already solve this?\". Searches observations, session summaries, and user prompts across the entire project history stored in the PM2-managed database.\n```\n\n**Key improvements:**\n- Add \"claude-mem\" to description (unique trigger)\n- Add \"cross-session memory database\" (differentiates from current conversation)\n- Add \"NOT in current conversation context\" (clear value prop)\n- Add \"days/weeks/months ago\" (temporal trigger)\n- Add \"previous sessions\" (specific trigger phrase)\n- Add \"did we already solve this?\" (concrete user question)\n- Add \"PM2-managed database\" (shows it's a real system, not just Claude's memory)\n\n#### 2. Surface all 9 operations directly in SKILL.md\n**Current pattern:** Links to operations/*.md files\n\n**Recommended pattern:** List all operations with one-line purposes, like Kalshi does\n\n```markdown\n## Available Operations\n\n### Search Operations\n- **observations.md** - Full-text search across all observations (bugs, features, decisions)\n- **sessions.md** - Search session summaries to find what was accomplished when\n- **prompts.md** - Find what users have asked about in past sessions\n\n### Filtered Search\n- **by-type.md** - Filter by observation type (bugfix, feature, refactor, decision, discovery, change)\n- **by-concept.md** - Find observations tagged with specific concepts (problem-solution, how-it-works, gotcha)\n- **by-file.md** - Find all work related to a specific file path across all sessions\n\n### Timeline & Context\n- **recent-context.md** - Get last N sessions with summaries and observations\n- **timeline.md** - Get chronological context around a specific point in time (before/after)\n- **timeline-by-query.md** - Search first, then get timeline context around best match\n\nFor detailed instructions on any operation, read the corresponding file in operations/.\n```\n\n**Benefits:**\n- Shows ALL capabilities in ~15 lines vs requiring link clicking\n- Claude can immediately see what's possible without reading 9 separate files\n- Pattern matches Kalshi's successful approach\n- Still preserves progressive disclosure (details in operations/)\n\n#### 3. Add concrete examples to trigger phrases\n**Current:** \"Use when answering questions about history, finding past decisions\"\n\n**Recommended additions:**\n```markdown\n## Trigger Phrases\n\nUse this skill when you see phrases like:\n- \"Did we already fix this bug?\"\n- \"How did we solve X last time?\"\n- \"What did we do in yesterday's session?\"\n- \"Show me all authentication-related changes\"\n- \"What features did we add last week?\"\n- \"Why did we choose this approach?\" (for decisions made in past sessions)\n- \"What files did we modify when we added X?\"\n```\n\n**Benefits:**\n- Concrete user phrases vs abstract categories\n- Claude can pattern-match actual user language\n- Clear differentiation from current-session questions\n\n### Medium Effort (Days)\n\n#### 4. Adopt embedded curl commands pattern\n**Current pattern:** operations/*.md files show curl commands; Claude must read file then execute command\n\n**Recommended pattern:** Create scripts/ directory with simple wrapper scripts, like Kalshi\n\n```bash\nscripts/\n├── recent.sh          # Wraps curl for recent context\n├── search-obs.sh      # Wraps curl for observation search\n├── search-session.sh  # Wraps curl for session search\n├── by-type.sh         # Wraps curl for type-based search\n└── timeline.sh        # Wraps curl for timeline\n```\n\n**Benefits:**\n- Single action: `./scripts/recent.sh --limit 5` vs read operations/recent-context.md then construct curl\n- Matches Kalshi's successful pattern\n- Reduces cognitive load and steps\n- Each script can include --help and --json like Kalshi\n\n**Downside:**\n- More files to maintain\n- Duplicates some logic from operations/*.md\n\n#### 5. Reduce SKILL.md line count to <80 lines\n**Current:** 96 lines with lots of sections\n\n**Recommended:** Consolidate to match Kalshi's 71-line approach\n\nRemove from main SKILL.md and move to operations/README.md:\n- Common Workflows section (already in operations/common-workflows.md)\n- Response Formatting section (already in operations/formatting.md)\n- Technical Notes (can be in operations/help.md)\n- Performance Tips (can be in operations/help.md)\n- Error Handling (can be in operations/help.md)\n\nKeep in SKILL.md:\n- Description (enhanced)\n- When to Use (enhanced with concrete phrases)\n- Quick Decision Guide (simplified)\n- Available Operations (expanded to show all 9 with purposes)\n- Quick examples\n\n**Benefits:**\n- Faster load time = more likely Claude loads it\n- Clearer signal-to-noise ratio\n- Matches successful pattern\n\n### Large Changes (Weeks)\n\n#### 6. Convert to MCP server pattern\n**Effort:** 2-4 weeks\n\n**Rationale:**\n- MCP tools appear in Claude's tool list automatically\n- No routing/invocation decision required\n- Skills require Claude to decide \"should I load this skill?\"\n- MCP bypasses this decision point\n\n**Tradeoff:**\n- Loses context preservation benefit of skills\n- Requires running server (already have PM2 worker, so not a huge change)\n- Better discovery but loses the skill pattern's conversation flow\n\n**Recommendation:** Try Quick Wins first before this nuclear option\n\n## 6. Agent Autonomy Assessment\n\n**Auto-invoked:** ❌ No (for both Kalshi and claude-mem)\n\n**Why:**\n\n1. **No empirical evidence:** Kalshi README claims \"model-invoked\" and \"autonomously activates\" but provides zero evidence:\n   - No test logs showing autonomous invocation\n   - No metrics on auto-invocation success rate\n   - No user testimonials or examples\n   - No code that verifies auto-invocation behavior\n\n2. **Skills are passive:** The skill pattern is documentation that Claude CAN load, not tools that Claude MUST see. The invocation flow is:\n   - User says something\n   - Claude decides \"should I load a skill?\"\n   - Claude searches available skills\n   - Claude matches description keywords\n   - Claude invokes Skill tool\n   - Claude reads SKILL.md\n   - Claude uses the skill\n\n   This is a 5-step process with a critical decision point at step 2. Compare to MCP:\n   - User says something\n   - Claude sees all MCP tools in tool list\n   - Claude uses appropriate tool\n\n   MCP is 2 steps with no decision point.\n\n3. **Keywords are necessary but not sufficient:** Good keywords in the description (like Kalshi has) make auto-invocation MORE LIKELY but don't guarantee it. They increase the probability that step 2 above (\"should I load a skill?\") results in \"yes.\"\n\n4. **Beyond-MCP is aspirational:** The repository is called \"beyond-mcp\" and is demonstrating alternatives to MCP. The claims about auto-invocation are ASPIRATIONAL (this is how skills SHOULD work) not EMPIRICAL (this is how skills DO work).\n\n**Confidence:** HIGH\n\n**Evidence-based conclusion:**\n- Kalshi skill has BETTER discoverability than claude-mem (stronger keywords, clearer capabilities)\n- This makes it MORE LIKELY to be auto-invoked\n- But there's no evidence EITHER skill reliably auto-invokes\n- The skill pattern depends on LLM routing decisions that are probabilistic, not deterministic\n- claude-mem's weak description makes it LESS LIKELY to win the routing lottery\n- Improving the description per Quick Wins above will increase probability but not guarantee auto-invocation\n\n**Bottom line:** Fix the description to match Kalshi's quality, but don't expect magic. Skills are inherently less discoverable than MCP tools because they require an extra routing decision.\n",
    "github": {
      "stars": 10,
      "forks": 1,
      "owner": "basher83",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/129473091?v=4",
      "license": "MIT",
      "lastCommit": "2026-01-02T22:16:24Z",
      "createdAt": "2025-10-24T02:41:21Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:41.574Z"
  },
  {
    "id": "plans",
    "source": {
      "type": "github-search",
      "repo": "pumped-fn/pumped-fn",
      "path": "plans/2025-10-27-module-authoring-skill.md",
      "url": "https://github.com/pumped-fn/pumped-fn/blob/main/plans/2025-10-27-module-authoring-skill.md"
    },
    "frontmatter": {},
    "content": "# Module Authoring Skill Implementation Plan\n\n> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.\n\n**Goal:** Add module authoring guidance to pumped-fn skill for creating reusable, publishable components with optional dependencies and proper testability.\n\n**Architecture:** Pattern-based reference file (~2-3k words) with 3 core patterns: (1) Reusable Resource with optional dependencies, (2) Extension Package for framework adapters, (3) Composition and exports strategy. Integrates into existing skill structure via Module Authoring Mode detection.\n\n**Tech Stack:** Markdown, bash (grep patterns), pumped-fn API patterns (provide, derive, tag, lazy, resolve, preset)\n\n---\n\n## Task 1: Create references/authoring.md with Pattern 1\n\n**Files:**\n- Create: `.claude/skills/pumped-fn/references/authoring.md`\n\n**Step 1: Create authoring.md with header and Pattern 1**\n\n```markdown\n## Module Authoring\n\n**Purpose:** Patterns for creating reusable, publishable pumped-fn components\n\n**When to use:** Building components for npm distribution, not application code\n\n---\n\n### Pattern 1: Reusable Resource (Configurable with Optional Dependencies)\n\n**What:** Resources that consumers can configure for their needs\n\n**Example:** Logger with multiple backends (console, winston, pino)\n\n**Key principles:**\n- Define interface upfront (hides implementation details)\n- Each backend is an executor returning the interface\n- Configuration via exported tags (not hardcoded)\n- Dynamic imports for optional deps (no side effects)\n- Use `.lazy` modifier + `resolve()` for selection\n- Backends read config via `tag.find(scope)` in factory\n\n**Structure:**\n\n```typescript\nimport { provide, derive, tag, custom } from '@pumped-fn/core-next'\n\n// 1. Interface (contract)\ninterface Logger {\n  log(msg: string): void\n  error(msg: string): void\n}\n\n// 2. Configuration tags\nexport const logConfig = {\n  backend: tag(custom<'console' | 'winston' | 'pino'>(), {\n    label: 'log.backend',\n    default: 'console'\n  }),\n  level: tag(custom<'info' | 'debug' | 'error'>(), {\n    label: 'log.level',\n    default: 'info'\n  })\n}\n\n// 3. Each backend reads config from scope\nconst consoleLogger = provide(({ scope }): Logger => {\n  const level = logConfig.level.find(scope) ?? 'info'\n  return {\n    log: (msg: string) => console.log(`[${level}]`, msg),\n    error: (msg: string) => console.error(`[${level}]`, msg)\n  }\n})\n\nconst winstonLogger = provide(async ({ scope }): Promise<Logger> => {\n  const level = logConfig.level.find(scope) ?? 'info'\n\n  const winston = await import('winston')\n  const winstonLogger = winston.createLogger({\n    level,\n    transports: [new winston.transports.Console()]\n  })\n\n  return {\n    log: (msg: string) => winstonLogger.info(msg),\n    error: (msg: string) => winstonLogger.error(msg)\n  }\n})\n\nconst pinoLogger = provide(async ({ scope }): Promise<Logger> => {\n  const level = logConfig.level.find(scope) ?? 'info'\n\n  const pino = await import('pino')\n  const pinoLogger = pino({ level })\n\n  return {\n    log: (msg: string) => pinoLogger.info(msg),\n    error: (msg: string) => pinoLogger.error(msg)\n  }\n})\n\n// 4. Main logger selects backend via tag + lazy resolve\nexport const logger = derive(\n  {\n    console: consoleLogger.lazy,\n    winston: winstonLogger.lazy,\n    pino: pinoLogger.lazy\n  },\n  async (backends, { scope }): Promise<Logger> => {\n    const backend = logConfig.backend.find(scope) ?? 'console'\n\n    switch (backend) {\n      case 'winston':\n        return await backends.winston.resolve()\n      case 'pino':\n        return await backends.pino.resolve()\n      default:\n        return await backends.console.resolve()\n    }\n  }\n)\n```\n\n**Consumer usage:**\n\n```typescript\nimport { createScope } from '@pumped-fn/core-next'\nimport { logger, logConfig } from '@myorg/pumped-logger'\n\nconst scope = createScope({\n  tags: [\n    logConfig.backend('pino'),\n    logConfig.level('debug')\n  ]\n})\n\nconst log = await scope.resolve(logger)  // Only pino loads\nlog.log('Hello')\n```\n\n**Key takeaways:**\n- Interface defined upfront\n- Backends read config via `tag.find(scope)`\n- Dynamic imports prevent side effects\n- `.lazy` + `resolve()` enable runtime selection\n- Consumer controls everything via scope tags\n```\n\n**Step 2: Verify file created**\n\nRun: `ls -lh .claude/skills/pumped-fn/references/authoring.md`\nExpected: File exists\n\n**Step 3: Count words in Pattern 1**\n\nRun: `wc -w .claude/skills/pumped-fn/references/authoring.md`\nExpected: ~800-900 words\n\n**Step 4: Commit**\n\n```bash\ngit add .claude/skills/pumped-fn/references/authoring.md\ngit commit -m \"feat(skill): add Pattern 1 (Reusable Resource) to authoring reference\"\n```\n\n---\n\n## Task 2: Add Pattern 2 (Extension Package) to authoring.md\n\n**Files:**\n- Modify: `.claude/skills/pumped-fn/references/authoring.md`\n\n**Step 1: Append Pattern 2 to authoring.md**\n\nAdd after Pattern 1:\n\n```markdown\n---\n\n### Pattern 2: Extension Package (Framework Adapters)\n\n**What:** Extensions that integrate pumped-fn with frameworks\n\n**Example:** Generic web server adapter (supports fastify, express, hono)\n\n**Key principles:**\n- Framework as peer dependency\n- Interface for adapter (hide framework specifics)\n- Consumer selects framework via tags\n- Same lazy loading pattern as Pattern 1\n\n**Structure:**\n\n```typescript\nimport { provide, derive, tag, custom } from '@pumped-fn/core-next'\n\n// 1. Server interface\ninterface Server {\n  listen(port: number): Promise<void>\n  route(path: string, handler: RouteHandler): void\n}\n\n// 2. Configuration tags\nexport const serverConfig = {\n  framework: tag(custom<'fastify' | 'express' | 'hono'>(), {\n    label: 'server.framework',\n    default: 'fastify'\n  }),\n  port: tag(custom<number>(), {\n    label: 'server.port',\n    default: 3000\n  })\n}\n\n// 3. Framework adapters (lazy loaded)\nconst fastifyAdapter = provide(async ({ scope }): Promise<Server> => {\n  const port = serverConfig.port.find(scope) ?? 3000\n  const fastify = await import('fastify')\n  const app = fastify.default()\n\n  return {\n    listen: async (p: number) => { await app.listen({ port: p }) },\n    route: (path, handler) => { app.get(path, handler) }\n  }\n})\n\nconst expressAdapter = provide(async ({ scope }): Promise<Server> => {\n  const express = await import('express')\n  const app = express.default()\n\n  return {\n    listen: async (p: number) => {\n      return new Promise((resolve) => app.listen(p, () => resolve()))\n    },\n    route: (path, handler) => { app.get(path, handler) }\n  }\n})\n\n// Similar for honoAdapter...\n\n// 4. Main server selector\nexport const server = derive(\n  {\n    fastify: fastifyAdapter.lazy,\n    express: expressAdapter.lazy,\n    hono: honoAdapter.lazy\n  },\n  async (adapters, { scope }): Promise<Server> => {\n    const framework = serverConfig.framework.find(scope) ?? 'fastify'\n\n    switch (framework) {\n      case 'express':\n        return await adapters.express.resolve()\n      case 'hono':\n        return await adapters.hono.resolve()\n      default:\n        return await adapters.fastify.resolve()\n    }\n  }\n)\n```\n\n**Consumer usage:**\n\n```typescript\nimport { createScope } from '@pumped-fn/core-next'\nimport { server, serverConfig } from '@myorg/pumped-server'\n\nconst scope = createScope({\n  tags: [\n    serverConfig.framework('express'),\n    serverConfig.port(8080)\n  ]\n})\n\nconst srv = await scope.resolve(server)  // Only express loads\nawait srv.listen(8080)\n```\n\n**Key takeaways:**\n- Same pattern as resources, different domain\n- Framework-specific code hidden behind interface\n- Consumer choice via tags\n- Only selected framework loads\n```\n\n**Step 2: Verify content**\n\nRun: `grep -c \"Pattern 2\" .claude/skills/pumped-fn/references/authoring.md`\nExpected: At least 1 match\n\n**Step 3: Count total words**\n\nRun: `wc -w .claude/skills/pumped-fn/references/authoring.md`\nExpected: ~1500-1700 words\n\n**Step 4: Commit**\n\n```bash\ngit add .claude/skills/pumped-fn/references/authoring.md\ngit commit -m \"feat(skill): add Pattern 2 (Extension Package) to authoring reference\"\n```\n\n---\n\n## Task 3: Add Pattern 3 (Composition and Exports) to authoring.md\n\n**Files:**\n- Modify: `.claude/skills/pumped-fn/references/authoring.md`\n\n**Step 1: Append Pattern 3 to authoring.md**\n\nAdd after Pattern 2:\n\n```markdown\n---\n\n### Pattern 3: Composition and Exports\n\n**What:** How to organize exports for reusability without compromising testing\n\n**Key principles:**\n- Export interface (contract for consumers)\n- Export config tags (consumer control)\n- Export main executor (primary API)\n- Export individual backends (REQUIRED for preset() testing)\n- Never export internals/implementation details\n\n**How backends read configuration:**\n\n```typescript\n// Each backend reads tags from scope via controller\nconst pinoLogger = provide(async ({ scope }): Promise<Logger> => {\n  // Read config from scope using tags\n  const level = logConfig.level.find(scope) ?? 'info'\n\n  const pino = await import('pino')\n  const pinoLogger = pino({ level })\n\n  return {\n    log: (msg: string) => pinoLogger.info(msg),\n    error: (msg: string) => pinoLogger.error(msg)\n  }\n})\n\n// Consumer controls config via scope tags\nconst scope = createScope({\n  tags: [\n    logConfig.backend('pino'),\n    logConfig.level('debug')  // Backend reads this\n  ]\n})\n```\n\n**Export structure:**\n\n```typescript\n// index.ts\n\n// 1. Interface (contract)\nexport interface Logger {\n  log(msg: string): void\n  error(msg: string): void\n}\n\n// 2. Configuration tags (consumer control)\nexport const logConfig = {\n  backend: tag(custom<'console' | 'winston' | 'pino'>(), {\n    label: 'log.backend',\n    default: 'console'\n  }),\n  level: tag(custom<'info' | 'debug' | 'error'>(), {\n    label: 'log.level',\n    default: 'info'\n  })\n}\n\n// 3. Main executor (primary API)\nexport const logger = derive(...)\n\n// 4. Individual backends (REQUIRED for testing/override)\nexport const consoleLogger = provide(({ scope }): Logger => {\n  const level = logConfig.level.find(scope) ?? 'info'\n  // ...\n})\nexport const winstonLogger = provide(async ({ scope }): Promise<Logger> => {\n  const level = logConfig.level.find(scope) ?? 'info'\n  // ...\n})\nexport const pinoLogger = provide(async ({ scope }): Promise<Logger> => {\n  const level = logConfig.level.find(scope) ?? 'info'\n  // ...\n})\n```\n\n**Why export backends:**\n- **Required for testing:** Consumers need original executor to use `preset()` for mocking\n- Consumers can test with specific backend directly\n- Consumers can override/extend specific backend\n- Consumers can compose custom variants with subset of backends\n\n**Testing by consumers:**\n\n```typescript\nimport { logger, logConfig, pinoLogger } from '@myorg/pumped-logger'\nimport { preset, derive } from '@pumped-fn/core-next'\n\n// preset requires original executor\nconst testLogger = derive(\n  {\n    pino: preset(pinoLogger, (): Logger => ({\n      log: vi.fn(),\n      error: vi.fn()\n    })).lazy\n  },\n  async (backends) => await backends.pino.resolve()\n)\n\nconst scope = createScope({ tags: [logConfig.backend('pino')] })\nconst log = await scope.resolve(testLogger)\n```\n\n**Composability by consumers:**\n\n```typescript\nimport { consoleLogger, winstonLogger, logConfig } from '@myorg/pumped-logger'\n\n// Create custom logger with only console + winston\nconst customLogger = derive(\n  {\n    console: consoleLogger.lazy,\n    winston: winstonLogger.lazy\n  },\n  async (backends, { scope }) => {\n    const backend = logConfig.backend.find(scope) ?? 'console'\n    return backend === 'winston'\n      ? await backends.winston.resolve()\n      : await backends.console.resolve()\n  }\n)\n```\n\n---\n\n### Checklist: Reusable Component Structure\n\n☐ Interface exported (defines contract)\n☐ Config tags exported (consumer control)\n☐ Backends read config via `tag.find(scope)` in factory\n☐ Main executor exported (primary API)\n☐ **Individual backends exported (REQUIRED for preset() in consumer tests)**\n☐ No implementation details exported\n☐ All exports use interface types (not concrete implementations)\n☐ Consumers can preset() individual components for testing\n☐ Consumers can compose custom variants from exported pieces\n```\n\n**Step 2: Verify Pattern 3 added**\n\nRun: `grep -c \"Pattern 3\" .claude/skills/pumped-fn/references/authoring.md`\nExpected: At least 1 match\n\n**Step 3: Count total words**\n\nRun: `wc -w .claude/skills/pumped-fn/references/authoring.md`\nExpected: ~2400-2800 words (target < 3000)\n\n**Step 4: Commit**\n\n```bash\ngit add .claude/skills/pumped-fn/references/authoring.md\ngit commit -m \"feat(skill): add Pattern 3 (Composition and Exports) to authoring reference\"\n```\n\n---\n\n## Task 4: Update SKILL.md with Module Authoring Mode\n\n**Files:**\n- Modify: `.claude/skills/pumped-fn/SKILL.md`\n\n**Step 1: Add Module Authoring Mode section after Continuous Development**\n\nFind the section that ends with \"Continuous Development Mode\" and add after it (before `## Quick Navigation`):\n\n```markdown\n### Module Authoring Mode (Creating Reusable Components)\n\n**Detection:**\n- User mentions: \"reusable\", \"package\", \"library\", \"module\", \"publish\"\n- User asks: \"How do I make this reusable?\", \"Can this be a package?\"\n- Code patterns: Creating executors meant for npm distribution\n\n**Workflow:**\n1. Load references/authoring.md\n2. Identify pattern type: Reusable Resource vs Extension Package\n3. Apply configurability patterns (interface + tags + lazy loading)\n4. Ensure proper exports structure (interface, tags, main, backends)\n5. Validate composition and testability\n\n**Key requirements for modules:**\n- Configuration via exported tags (not hardcoded)\n- Dynamic imports for optional dependencies (no side effects)\n- All backends exported (required for preset() testing)\n- Interface-first design (hide implementation details)\n```\n\n**Step 2: Verify section added**\n\nRun: `grep -c \"Module Authoring Mode\" .claude/skills/pumped-fn/SKILL.md`\nExpected: 1 match\n\n**Step 3: Commit**\n\n```bash\ngit add .claude/skills/pumped-fn/SKILL.md\ngit commit -m \"feat(skill): add Module Authoring Mode detection to SKILL.md\"\n```\n\n---\n\n## Task 5: Add grep patterns to Quick Navigation\n\n**Files:**\n- Modify: `.claude/skills/pumped-fn/SKILL.md`\n\n**Step 1: Add authoring grep patterns to Quick Navigation section**\n\nFind the `## Quick Navigation` section and add these patterns after the existing ones:\n\n```markdown\n# Finding module authoring guidance\ngrep -l \"Pattern 1.*Reusable Resource\" references/*.md\ngrep -l \"Pattern 2.*Extension Package\" references/*.md\ngrep -l \"Pattern 3.*Composition and Exports\" references/*.md\ngrep -l \"preset.*original executor\" references/*.md\n```\n\n**Step 2: Test grep patterns work**\n\nRun: `grep -l \"Pattern 1.*Reusable Resource\" .claude/skills/pumped-fn/references/*.md`\nExpected: `references/authoring.md`\n\nRun: `grep -l \"Pattern 3.*Composition and Exports\" .claude/skills/pumped-fn/references/*.md`\nExpected: `references/authoring.md`\n\n**Step 3: Commit**\n\n```bash\ngit add .claude/skills/pumped-fn/SKILL.md\ngit commit -m \"feat(skill): add authoring grep patterns to Quick Navigation\"\n```\n\n---\n\n## Task 6: Update Reference Files table in SKILL.md\n\n**Files:**\n- Modify: `.claude/skills/pumped-fn/SKILL.md`\n\n**Step 1: Add authoring.md row to Reference Files table**\n\nFind the `## Reference Files` section table and add this row:\n\n```markdown\n| **authoring.md** | 3 module authoring patterns (Reusable Resource, Extension Package, Composition/Exports) with optional dependencies | ~2600 words | When creating reusable/publishable components, libraries, or extensions |\n```\n\n**Step 2: Verify table updated**\n\nRun: `grep -c \"authoring.md\" .claude/skills/pumped-fn/SKILL.md`\nExpected: At least 1 match\n\n**Step 3: Commit**\n\n```bash\ngit add .claude/skills/pumped-fn/SKILL.md\ngit commit -m \"feat(skill): add authoring.md to Reference Files table\"\n```\n\n---\n\n## Task 7: Final verification\n\n**Files:**\n- Read: `.claude/skills/pumped-fn/SKILL.md`\n- Read: `.claude/skills/pumped-fn/references/authoring.md`\n\n**Step 1: Verify SKILL.md word count still under limit**\n\nRun: `wc -w .claude/skills/pumped-fn/SKILL.md`\nExpected: < 2000 words (should be ~1700-1800 now)\n\n**Step 2: Verify authoring.md word count**\n\nRun: `wc -w .claude/skills/pumped-fn/references/authoring.md`\nExpected: 2400-2800 words (target < 3000)\n\n**Step 3: Verify all grep patterns work**\n\nRun:\n```bash\ngrep -l \"Module Authoring Mode\" .claude/skills/pumped-fn/SKILL.md\ngrep -l \"Pattern 1.*Reusable Resource\" .claude/skills/pumped-fn/references/*.md\ngrep -l \"Pattern 2.*Extension Package\" .claude/skills/pumped-fn/references/*.md\ngrep -l \"Pattern 3.*Composition\" .claude/skills/pumped-fn/references/*.md\n```\n\nExpected: All return correct file paths\n\n**Step 4: Verify file structure**\n\nRun: `ls -lh .claude/skills/pumped-fn/references/`\nExpected: 6 files (anti-patterns, authoring, decision-trees, environments, templates, validation)\n\n**Step 5: Final commit message**\n\n```bash\ngit add -A\ngit commit -m \"feat(skill): complete module authoring skill integration\n\nAdded references/authoring.md with 3 patterns:\n- Pattern 1: Reusable Resource with optional dependencies\n- Pattern 2: Extension Package for framework adapters\n- Pattern 3: Composition and exports for testability\n\nUpdated SKILL.md with:\n- Module Authoring Mode detection\n- Grep patterns for finding authoring content\n- Reference files table entry\n\nTotal: ~2600 words in authoring.md, SKILL.md remains < 2000 words\"\n```\n\n---\n\n## Summary\n\n**Files created:**\n- `.claude/skills/pumped-fn/references/authoring.md` (~2600 words)\n\n**Files modified:**\n- `.claude/skills/pumped-fn/SKILL.md` (added ~150 words)\n\n**Commits:** 7 atomic commits\n\n**Verification:**\n- SKILL.md < 2000 words ✓\n- authoring.md < 3000 words ✓\n- All grep patterns working ✓\n- 3 patterns complete ✓\n- Integration seamless ✓\n",
    "github": {
      "stars": 5,
      "forks": 3,
      "owner": "pumped-fn",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/199755659?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-29T10:19:18Z",
      "createdAt": "2025-02-18T10:57:41Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:42.275Z"
  },
  {
    "id": "skills",
    "source": {
      "type": "github-search",
      "repo": "WalkerVVV/firstmile-deals-pipeline",
      "path": ".claude/skills/analysis_skill.md",
      "url": "https://github.com/WalkerVVV/firstmile-deals-pipeline/blob/main/.claude/skills/analysis_skill.md"
    },
    "frontmatter": {},
    "content": "# Analysis Agent Skill\n\n**Purpose**: Generate shipping profile analyses, rate comparisons, and FirstMile performance reports.\n\n## Core Reference\n→ **See [rules.md](../../rules.md) → Agent-Specific Rules → Analysis Agent**\n→ **See [~/.claude/FIRSTMILE.md](~/.claude/FIRSTMILE.md) → Complete FirstMile analysis framework**\n\n## Execution Workflow\n\n### Phase 1: Data Validation (First 5 minutes)\n1. Load PLD (Parcel Level Detail) file from customer deal folder\n2. Validate required columns: Carrier, Service, Weight, Zone, Destination State, Cost, Date\n3. Check data quality: missing values, outliers, date ranges\n4. Generate data quality report if issues found\n\n### Phase 2: Comprehensive Analysis (10-15 minutes)\nExecute in this order:\n1. **Volume Profile**: Total shipments, daily average, marketplace mix\n2. **Carrier Mix**: Volume/spend by carrier with percentages\n3. **Service Level Distribution**: Services used with costs\n4. **Expanded Weight Distribution**: Critical billable weight thresholds\n5. **Dimensional Analysis**: Average dims, cubic volume (</>1 cu ft)\n6. **Zone Distribution**: Individual zones 1-8, Regional vs Cross-Country\n7. **Geographic Distribution**: Top 10 destination states\n8. **Cost Analysis**: Total spend, avg/median costs per parcel\n9. **Billable Weight Impact**: Actual vs billable weight comparison\n\n### Phase 3: FirstMile Opportunity Identification (5-10 minutes)\n1. Map volumes to Xparcel service levels (Ground, Expedited, Priority)\n2. Identify National vs Select network allocation\n3. Calculate potential savings by service level\n4. Flag optimization opportunities (e.g., packages at billable weight thresholds)\n\n### Phase 4: Report Generation (5-10 minutes)\n**Choose format based on need**:\n- **Quick Analysis**: Markdown summary with tables\n- **Executive Summary**: One-page PDF with key metrics\n- **Comprehensive Report**: 9-tab Excel with full analysis\n\n## Analysis Components (Execute in Order)\n\n### 1. Volume Profile\n```python\nimport pandas as pd\n\ndef volume_profile(df):\n    total_shipments = len(df)\n    date_range = (df['Date'].min(), df['Date'].max())\n    days_in_range = (date_range[1] - date_range[0]).days\n    daily_average = total_shipments / days_in_range\n\n    # Marketplace mix (if available)\n    marketplace_dist = df['Marketplace'].value_counts(normalize=True) if 'Marketplace' in df.columns else None\n\n    return {\n        'total_shipments': total_shipments,\n        'date_range': date_range,\n        'daily_average': daily_average,\n        'marketplace_distribution': marketplace_dist\n    }\n```\n\n**Output Format**:\n```markdown\n## Volume Profile\n- **Total Shipments**: {count:,} parcels\n- **Date Range**: {start_date} to {end_date} ({days} days)\n- **Daily Average**: {avg:,} parcels/day\n- **Marketplace Mix**: Amazon ({pct}%), Shopify ({pct}%), Other ({pct}%)\n```\n\n### 2. Carrier Mix\n```python\ndef carrier_mix(df):\n    carrier_volume = df['Carrier'].value_counts()\n    carrier_spend = df.groupby('Carrier')['Cost'].sum()\n    carrier_summary = pd.DataFrame({\n        'Volume': carrier_volume,\n        'Volume %': (carrier_volume / len(df) * 100).round(1),\n        'Spend': carrier_spend,\n        'Spend %': (carrier_spend / carrier_spend.sum() * 100).round(1)\n    })\n    return carrier_summary.sort_values('Spend', ascending=False)\n```\n\n**Output Format**:\n```markdown\n## Carrier Mix\n| Carrier | Volume | Volume % | Spend | Spend % |\n|---------|--------|----------|-------|---------|\n| [Name]  | X,XXX  | XX.X%    | $X,XXX| XX.X%   |\n```\n\n### 3. Expanded Weight Distribution\n**CRITICAL**: This analysis identifies billable weight optimization opportunities.\n\n```python\ndef expanded_weight_distribution(df):\n    # Under 1 lb (critical thresholds)\n    bins_under_1lb = [0, 0.25, 0.5, 0.75, 0.9375, 0.99, 1.0]  # 0-4oz, 5-8oz, 9-12oz, 13-15oz, 15.99oz, 16oz\n    labels_under_1lb = ['0-4oz', '5-8oz', '9-12oz', '13-15oz', '15.99oz', '16oz (1lb)']\n\n    # 1-5 lbs by billable pound\n    bins_1_5lb = [1.0, 2.0, 3.0, 4.0, 5.0]\n    labels_1_5lb = ['1-2lb', '2-3lb', '3-4lb', '4-5lb']\n\n    # Over 5 lbs\n    bins_over_5lb = [5.0, 10.0, 20.0, 50.0, float('inf')]\n    labels_over_5lb = ['5-10lb', '10-20lb', '20-50lb', '50+lb']\n\n    # Categorize and analyze\n    under_1lb = df[df['Weight'] < 1.0]\n    between_1_5lb = df[(df['Weight'] >= 1.0) & (df['Weight'] <= 5.0)]\n    over_5lb = df[df['Weight'] > 5.0]\n\n    return {\n        'under_1lb_dist': pd.cut(under_1lb['Weight'], bins=bins_under_1lb, labels=labels_under_1lb).value_counts(),\n        '1_5lb_dist': pd.cut(between_1_5lb['Weight'], bins=bins_1_5lb, labels=labels_1_5lb).value_counts(),\n        'over_5lb_dist': pd.cut(over_5lb['Weight'], bins=bins_over_5lb, labels=labels_over_5lb).value_counts()\n    }\n```\n\n**Key Thresholds to Flag**:\n- **15.99 oz**: Maximum before jumping to 2 lbs billable (packages just over this are optimization targets)\n- **32 oz**: Maximum before jumping to 3 lbs billable\n- **48 oz**: Maximum before jumping to 4 lbs billable\n\n**Output Format**:\n```markdown\n## Weight Distribution\n\n### Under 1 lb (Critical Optimization Zone)\n| Range | Count | % of Total | Spend | Avg Cost |\n|-------|-------|------------|-------|----------|\n| 0-4oz | X,XXX | XX.X% | $X,XXX | $X.XX |\n| 15.99oz | XXX ⚠️ | X.X% | $XXX | $X.XX |\n\n**Optimization Opportunity**: XXX packages at 15.99oz+ (consider packaging reduction to stay under 16oz threshold)\n```\n\n### 4. Zone Distribution\n```python\ndef zone_distribution(df):\n    zone_summary = df.groupby('Zone').agg({\n        'Zone': 'count',  # Volume\n        'Cost': ['sum', 'mean'],\n        'Transit_Days': 'mean'  # If available\n    }).round(2)\n\n    # Regional (1-4) vs Cross-Country (5-8)\n    regional = df[df['Zone'].isin([1, 2, 3, 4])]\n    cross_country = df[df['Zone'].isin([5, 6, 7, 8])]\n\n    return {\n        'zone_detail': zone_summary,\n        'regional_pct': len(regional) / len(df) * 100,\n        'cross_country_pct': len(cross_country) / len(df) * 100\n    }\n```\n\n**Output Format**:\n```markdown\n## Zone Distribution\n| Zone | Volume | % | Avg Cost | Avg Transit |\n|------|--------|---|----------|-------------|\n| 1-2  | X,XXX  | XX%| $X.XX    | X.X days    |\n\n**Regional (Zones 1-4)**: XX.X% | **Cross-Country (Zones 5-8)**: XX.X%\n\n**FirstMile Recommendation**:\n- Regional shipments → National Network + Xparcel Ground\n- Cross-Country <1lb → Select Network + Xparcel Ground (zone-skipping)\n```\n\n### 5. Geographic Distribution with Hub Mapping\n```python\n# Hub mapping from rules.md\nHUB_MAP = {\n    \"CA\": \"LAX - West Coast\",\n    \"TX\": \"DFW - South Central\",\n    \"FL\": \"MIA - Southeast\",\n    \"NY\": \"JFK/EWR - Northeast\",\n    \"IL\": \"ORD - Midwest\",\n    \"GA\": \"ATL - Southeast\",\n    \"WA\": \"SEA - Pacific Northwest\",\n    \"PA\": \"PHL - Mid-Atlantic\",\n    \"AZ\": \"PHX - Southwest\",\n    \"CO\": \"DEN - Mountain West\"\n}\n\ndef geographic_distribution(df):\n    state_summary = df.groupby('Destination_State').agg({\n        'Destination_State': 'count',\n        'Cost': 'sum'\n    }).sort_values('Destination_State', ascending=False).head(15)\n\n    # Map to FirstMile hubs\n    state_summary['Hub'] = state_summary.index.map(lambda x: HUB_MAP.get(x, \"National Network\"))\n\n    return state_summary\n```\n\n**Output Format**:\n```markdown\n## Top 15 Destination States\n| State | Volume | % | Hub Assignment |\n|-------|--------|---|----------------|\n| CA    | X,XXX  | XX%| LAX - West Coast |\n| TX    | X,XXX  | XX%| DFW - South Central |\n\n**Select Network Coverage**: XX.X% (major metro hubs)\n**National Network**: XX.X% (all other ZIPs)\n```\n\n## FirstMile Analysis Framework Compliance\n\n**MANDATORY**: All reports MUST follow `~/.claude/FIRSTMILE.md` standards.\n\n### ✅ REQUIRED Elements\n1. **Lead with SLA Compliance** (if performance report)\n2. **Use Xparcel service terminology** (Ground 3-8d, Expedited 2-5d, Priority 1-3d)\n3. **Network terminology**: \"National\" or \"Select\" (NEVER name UPS, FedEx, USPS)\n4. **Spell \"eCommerce\"** with camel-case 'C'\n5. **FirstMile blue headers** in Excel (#366092)\n6. **Hub mapping** for top destination states\n\n### ❌ PROHIBITED\n- Leading with daily delivery percentages\n- Naming specific carriers in reports\n- Presenting Xparcel as separate company (it's a ship-method under FirstMile)\n- Skipping zone distribution analysis\n- Missing geographic hub mapping\n\n## Excel Report Generation (9-Tab Structure)\n\n**When to generate**: Customer requests comprehensive report, executive summary, or QBR deck.\n\n**Required Tabs** (in order):\n1. **Executive Summary** - High-level KPIs in two-column table\n2. **SLA Compliance** - Delivered-only metrics with color scale\n3. **Transit Performance** - Daily distribution (0-7, 8+) with statistics\n4. **Geographic Distribution** - Top 15 states with hub assignments\n5. **Zone Analysis** - Zones 1-8 with Regional vs Cross-Country summary\n6. **Operational Metrics** - Volume metrics and optimization opportunities\n7. **In-Transit Detail** - Ships not delivered with SLA window status\n8. **Notes & Assumptions** - Business rules and definitions\n9. **Brand Style Guide** - Color swatches with HEX/RGB/CMYK values\n\n**Styling Requirements**:\n```python\nfrom openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n\n# FirstMile brand colors\nFIRSTMILE_BLUE = \"366092\"\nLIGHT_GRAY = \"DDDDDD\"\n\n# Header styling\nheader_fill = PatternFill(start_color=FIRSTMILE_BLUE, end_color=FIRSTMILE_BLUE, fill_type=\"solid\")\nheader_font = Font(color=\"FFFFFF\", bold=True, size=12)\nheader_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n\n# Apply to all table headers\nfor cell in worksheet['A1:Z1']:\n    cell.fill = header_fill\n    cell.font = header_font\n    cell.alignment = header_alignment\n```\n\n**Filename Format**: `FirstMile_Xparcel_Performance_{Customer}_{YYYYMMDD_HHMM}.xlsx`\n\n## Strict Compliance Rules\n\n### ✅ MUST DO\n- Follow FIRSTMILE.md analysis framework exactly\n- Lead ALL reports with SLA compliance metrics (if performance data)\n- Use billable weight rules (round up to next oz/lb)\n- Apply FirstMile blue (#366092) to Excel headers\n- Include geographic distribution with hub mapping\n- Map volumes to Xparcel service levels\n- Identify National vs Select network allocation\n- Calculate optimization opportunities\n\n### ❌ NEVER DO\n- Present daily delivery % as primary metric\n- Name specific carriers (UPS, FedEx, USPS) in reports\n- Present Xparcel as separate company\n- Skip zone distribution analysis\n- Skip geographic hub mapping\n- Use non-FirstMile terminology\n- Violate brand color standards\n\n## Quality Gates\n\nBefore marking analysis complete:\n- [ ] All 9 analysis components executed in order\n- [ ] FirstMile framework compliance validated\n- [ ] SLA compliance metrics leading report (if applicable)\n- [ ] Xparcel service terminology used correctly\n- [ ] Network terminology (National/Select) applied\n- [ ] Hub mapping included for top states\n- [ ] Optimization opportunities identified\n- [ ] Output saved in customer deal folder\n- [ ] Excel styling meets brand standards (if Excel generated)\n\n## Performance Metrics\n\n**Target Benchmarks**:\n- Analysis completion: <30 minutes for comprehensive report\n- Data quality validation: <5 minutes\n- Excel generation: <10 minutes\n- Accuracy: >95% match with source data\n\n**Monthly Review**:\n- Report quality: Customer feedback on clarity and insights\n- Accuracy: Corrections needed post-delivery\n- Optimization impact: Savings realized from recommendations\n\n## Error Handling\n\n**Missing Data**:\n- Document missing columns in data quality report\n- Estimate missing values if possible (note assumptions)\n- Flag incomplete analysis sections\n\n**Data Quality Issues**:\n- Outliers: Flag parcels with unusual weight/cost combinations\n- Duplicates: Identify and handle duplicate tracking numbers\n- Invalid values: Negative weights, zero costs, missing zones\n\n**Excel Generation Failures**:\n- Fall back to markdown summary if Excel library fails\n- Save partial Excel with error notes\n- Alert user to incomplete report\n\n## Related Documentation\n\n- [rules.md](../../rules.md) - Complete agent rules and Excel standards\n- [~/.claude/FIRSTMILE.md](~/.claude/FIRSTMILE.md) - FirstMile brand framework\n- [.claude/docs/templates/EXCEL_PROCESS_TEMPLATES.md](../docs/templates/EXCEL_PROCESS_TEMPLATES.md) - Excel templates\n",
    "github": {
      "stars": 1,
      "forks": 0,
      "owner": "WalkerVVV",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/197841232?v=4",
      "lastCommit": "2026-01-03T23:40:21Z",
      "createdAt": "2025-10-24T05:03:03Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:43.546Z"
  },
  {
    "id": "beyond-mcp",
    "source": {
      "type": "github-search",
      "repo": "basher83/lunar-claude",
      "path": "docs/research/beyond-mcp/04-skill.md",
      "url": "https://github.com/basher83/lunar-claude/blob/main/docs/research/beyond-mcp/04-skill.md"
    },
    "frontmatter": {},
    "content": "# Skill Approach Analysis\n\n**Agent:** Sub-Agent 4\n**Focus:** Compare Kalshi-markets skill (beyond-mcp) vs claude-mem search skill\n**Goal:** Identify why one skill might be auto-invoked while another isn't\n\n## 1. Skill Description Comparison\n\n### Kalshi-Markets Skill (beyond-mcp)\n\n**Description (from SKILL.md frontmatter):**\n```\nAccess Kalshi prediction market data including market prices, orderbooks, trades,\nevents, and series information. Use when the user asks about prediction markets,\nKalshi markets, betting odds, market prices, or needs to search or analyze\nprediction market data.\n```\n\n**Trigger keywords explicitly listed:**\n- prediction markets\n- Kalshi markets\n- betting odds\n- market prices\n- search/analyze prediction market data\n\n**Capability visibility:** HIGH\n- Description immediately names concrete capabilities (market prices, orderbooks, trades, events, series)\n- \"Use when...\" clause directly connects user language to skill activation\n- SKILL.md is 72 lines, front-loads actionable information\n- 10 scripts listed with clear \"When to use\" statements\n\n**Lines in SKILL.md:** 72 lines total\n- Frontmatter: 4 lines (name + 3-line description)\n- Instructions: 6 lines\n- Script list with contexts: 30 lines (10 scripts × 3 lines each)\n- Architecture/Quick Start: 32 lines\n\n### Claude-Mem Search Skill (current)\n\n**Description (from SKILL.md frontmatter):**\n```\nSearch claude-mem persistent memory for past sessions, observations, bugs fixed,\nfeatures implemented, decisions made, code changes, and previous work. Use when\nanswering questions about history, finding past decisions, or researching\nprevious implementations.\n```\n\n**Trigger keywords implied:**\n- past sessions\n- observations\n- bugs fixed\n- features implemented\n- decisions made\n- code changes\n- previous work\n- history\n- past decisions\n- previous implementations\n\n**Capability visibility:** MEDIUM-LOW\n- Description lists WHAT is searchable (sessions, observations, bugs) but...\n- ...buries HOW in nested operations/ files (9 operations)\n- \"Use when...\" clause is generic (\"answering questions about history\")\n- SKILL.md is 97 lines but most points to subdirectories\n\n**Lines in SKILL.md:** 97 lines total\n- Frontmatter: 4 lines (name + 3-line description)\n- \"When to Use\" section: 18 lines (good!)\n- \"Quick Decision Guide\": 6 lines (good routing!)\n- \"Available Operations\": 20 lines (list of 10 operations)\n- Rest: Common workflows, formatting, technical notes (49 lines)\n\n### Comparison\n\n**Winner for discoverability:** Kalshi-Markets by a significant margin\n\n**Key differences:**\n\n1. **Description specificity:**\n   - Kalshi: Names concrete data types (market prices, orderbooks, trades)\n   - Claude-mem: Names abstract categories (observations, decisions, changes)\n\n2. **Trigger keywords placement:**\n   - Kalshi: In frontmatter description AND README (explicit listing)\n   - Claude-mem: Only in frontmatter, no explicit \"triggers on\" section\n\n3. **Capability visibility:**\n   - Kalshi: 10 scripts visible in SKILL.md with \"When to use\" for each\n   - Claude-mem: 10 operations hidden in subdirectories, must navigate\n\n4. **Progressive disclosure balance:**\n   - Kalshi: Shows ALL capabilities upfront (72 lines), hides implementation (2377 lines in scripts/)\n   - Claude-mem: Shows routing/meta info (97 lines), hides operations docs (operations/*.md)\n\n5. **Actionability:**\n   - Kalshi: \"Use when user asks about X\" → Direct action (run script Y)\n   - Claude-mem: \"Use when user asks about X\" → Choose operation → Read operation doc → Execute\n\n**Missing from claude-mem:**\n\n1. No explicit trigger keyword list in SKILL.md or separate doc\n2. No \"When to use\" statement per operation IN the main SKILL.md\n3. Operations are abstractions (search by type) not concrete nouns (orderbook)\n4. Two-hop navigation (SKILL.md → operations/ → execute) vs one-hop (SKILL.md → execute)\n\n## 2. Implementation Pattern Analysis\n\n### Kalshi Pattern\n\n**SKILL.md structure:**\n```\nFRONTMATTER (trigger-rich description)\n↓\nINSTRUCTIONS (how to use scripts)\n↓\nPROGRESSIVE DISCLOSURE (warning against reading scripts)\n↓\nAVAILABLE SCRIPTS (10 × \"When to use\" statements)\n↓\nARCHITECTURE (self-contained philosophy)\n↓\nQUICK START (immediate examples)\n```\n\n**Scripts approach:**\n- 10 self-contained Python scripts (157-468 lines each)\n- Each script is independently executable\n- Embedded HTTP client in each (no shared dependencies)\n- `--help` and `--json` flags standard\n- Total: 2377 lines hidden in scripts/\n\n**Progressive disclosure:**\n- SKILL.md: Shows WHAT and WHEN (72 lines visible)\n- scripts/: Shows HOW and implementation (2377 lines hidden)\n- Instruction: \"Don't read scripts unless absolutely needed\"\n\n**How integrated:**\n- Claude reads SKILL.md → Identifies script → Runs `uv run scripts/X.py --help` → Executes\n- One-hop: SKILL.md has enough info to choose and run script\n- Scripts are terminal nodes (no further navigation)\n\n### Claude-Mem Pattern\n\n**SKILL.md structure:**\n```\nFRONTMATTER (trigger-rich description)\n↓\nWHEN TO USE (good examples)\n↓\nQUICK DECISION GUIDE (routing to operations)\n↓\nAVAILABLE OPERATIONS (list of 10 with file paths)\n↓\nCOMMON WORKFLOWS (meta-guide)\n↓\nRESPONSE FORMATTING (meta-guide)\n↓\nTECHNICAL NOTES + PERFORMANCE TIPS\n```\n\n**Operations docs approach:**\n- 10 operation documentation files (1.4k-6.2k bytes each)\n- Each file explains one search pattern\n- Documents HTTP API calls (not embedded tools)\n- Operations are documentation, not executables\n\n**Progressive disclosure:**\n- SKILL.md: Shows categories and routing (97 lines visible)\n- operations/: Shows API usage patterns (12 files to navigate)\n- Additional docs: common-workflows.md, formatting.md\n\n**How organized:**\n- Claude reads SKILL.md → Chooses operation category → Reads operations/X.md → Executes HTTP call\n- Two-hop: SKILL.md routes to operations/, then execute\n- Operations are intermediate nodes (documentation layer)\n\n### Comparison: Better for Autonomy?\n\n**Winner: Kalshi pattern is better for autonomy**\n\n**Reasoning:**\n\n1. **Execution path length:**\n   - Kalshi: 1 hop (SKILL.md → execute script)\n   - Claude-mem: 2 hops (SKILL.md → operations doc → execute HTTP)\n\n2. **Cognitive load:**\n   - Kalshi: \"When user asks about orderbook\" → Run orderbook.py\n   - Claude-mem: \"When user asks about past bugs\" → Choose operation (by-type? observations?) → Read doc → Execute\n\n3. **Decision points:**\n   - Kalshi: 1 decision (which script?)\n   - Claude-mem: 2 decisions (which operation category? which specific doc?)\n\n4. **Self-containment:**\n   - Kalshi: Scripts are tools (executable)\n   - Claude-mem: Operations are instructions (requires interpretation then tool use)\n\n**However, claude-mem has advantages:**\n\n1. More flexible (HTTP API can be called many ways)\n2. Better for complex queries (multiple parameters, filters)\n3. Better documentation of edge cases and formatting\n\n**But these advantages hurt discoverability and autonomy in practice.**\n\n## 3. Auto-Invocation Evidence\n\n### README Claims\n\n**Explicit claims from beyond-mcp/apps/4_skill/README.md:**\n\n1. Line 7: \"**Model-Invoked** - Claude autonomously activates based on context\"\n2. Line 16: \"Claude (detects keyword) -> Loads SKILL.md -> Runs relevant script -> Kalshi API\"\n3. Line 19: \"The skill automatically activates when Claude detects keywords like:\"\n4. Line 32: \"# Claude will auto-detect and use the skill\"\n5. Line 75: \"Want autonomous skill discovery\"\n\n### Evidence Found\n\n**Type 1: Design patterns that support auto-invocation**\n\n1. **Keyword-rich description in frontmatter:**\n   ```\n   Use when the user asks about prediction markets, Kalshi markets,\n   betting odds, market prices, or needs to search or analyze\n   prediction market data.\n   ```\n   This is the standard Claude Code pattern for skill triggering.\n\n2. **Concrete capability nouns:**\n   - Market prices, orderbooks, trades, events, series\n   - These are specific enough to match user questions\n\n3. **Direct \"When to use\" mappings:**\n   Each script has a clear trigger statement visible in SKILL.md\n\n**Type 2: Architecture supporting autonomy**\n\n1. **Self-contained scripts** reduce context reading\n2. **`--help` flag** allows Claude to discover usage without reading code\n3. **`--json` flag** provides structured output for processing\n\n**Type 3: Lack of counter-evidence**\n\n- No \"call Skill tool first\" instructions\n- No \"remember to invoke skill\" reminders\n- README assumes autonomy as default behavior\n\n### Evidence NOT Found\n\n**Direct proof of auto-invocation:**\n- No screenshots of Claude auto-invoking the skill\n- No logs showing \"skill auto-invoked\"\n- No metrics on invocation rate\n- No A/B test showing improved discovery\n\n**Mechanistic explanation:**\n- No documentation of HOW Claude decides to load skills\n- No threshold for trigger keywords (1 match? 2 matches?)\n- No explanation of skill ranking/priority\n\n### Confidence in Claim\n\n**Confidence: MEDIUM-HIGH**\n\n**Why Medium-High (not High):**\n\n1. **README makes strong claims** but provides no empirical evidence\n2. **Design patterns align with claimed behavior** (keyword-rich, self-contained)\n3. **No counter-evidence** (no instructions to manually invoke)\n4. **This is a showcase repo** - likely tested extensively before publishing\n\n**Why not Low:**\n\n1. The pattern is consistent across the repository\n2. The README is detailed and professional (not marketing fluff)\n3. The architecture makes sense for autonomy (reduce friction)\n\n**What would raise confidence to High:**\n\n1. Anthropic documentation confirming skill auto-invocation mechanism\n2. Logs or screenshots of real usage\n3. Metrics showing discovery rates\n4. Code in Claude Code CLI showing skill matching logic\n\n## 4. Root Cause Analysis: Why Claude Doesn't Use claude-mem\n\n### Hypothesis A: Weak Description\n\n**Evidence:**\n\n1. **Trigger keyword density:**\n   - Kalshi: 5 explicit trigger phrases in 3-line description\n   - Claude-mem: 10 implied keywords in 3-line description\n   - BUT: Kalshi's triggers are nouns (market prices), claude-mem's are abstractions (observations)\n\n2. **Description specificity:**\n   - Kalshi: \"market prices, orderbooks, trades, events, series\"\n   - Claude-mem: \"sessions, observations, bugs fixed, features implemented, decisions, code changes, previous work\"\n   - Claude-mem is LESS concrete (what's an \"observation\"?)\n\n3. **\"Use when\" clarity:**\n   - Kalshi: \"user asks about prediction markets, Kalshi markets, betting odds, market prices\"\n   - Claude-mem: \"answering questions about history, finding past decisions, researching previous implementations\"\n   - Claude-mem is more GENERIC (many tools could match \"history\")\n\n4. **Missing explicit trigger list:**\n   - Kalshi README has explicit section: \"The skill automatically activates when Claude detects keywords like:\"\n   - Claude-mem has no equivalent\n\n**Fix effort:** LOW (1-2 hours)\n- Rewrite SKILL.md frontmatter description\n- Add explicit trigger keyword list\n- Use concrete nouns instead of abstractions\n\n**Likelihood this is the root cause:** HIGH (70%)\n\n**Reasoning:**\n- This is the FIRST thing Claude sees when considering skills\n- Weak triggers = low match probability\n- Generic triggers = low priority vs other tools\n\n### Hypothesis B: Progressive Disclosure Tradeoff\n\n**Evidence:**\n\n1. **Capability visibility in SKILL.md:**\n   - Kalshi: All 10 scripts listed with \"When to use\" statements (30 lines)\n   - Claude-mem: All 10 operations listed but as file paths (20 lines), no inline \"When to use\"\n\n2. **Navigation depth:**\n   - Kalshi: SKILL.md → execute (1 hop)\n   - Claude-mem: SKILL.md → operations/ → execute (2 hops)\n\n3. **Decision complexity:**\n   - Kalshi: \"User asks about orderbook\" → orderbook.py (direct)\n   - Claude-mem: \"User asks about bugs\" → by-type.md or observations.md? (ambiguous)\n\n4. **Hidden information:**\n   - Kalshi: Implementation hidden (2377 lines in scripts/)\n   - Claude-mem: Usage patterns hidden (operations/ docs)\n   - Claude-mem hides the WRONG thing (how to use) vs the RIGHT thing (implementation)\n\n**Fix effort:** MEDIUM (1-2 days)\n- Inline \"When to use\" statements for each operation in SKILL.md\n- Reduce navigation depth (flatten operations/ into SKILL.md?)\n- Simplify operation categorization\n\n**Likelihood this is the root cause:** MEDIUM (50%)\n\n**Reasoning:**\n- Even after skill is loaded, Claude must navigate further\n- Extra navigation = extra LLM calls = higher chance of abandoning skill\n- BUT: The \"Quick Decision Guide\" section should help with this\n\n### Hypothesis C: Pattern Limitation (Skills Inherently Limited)\n\n**Evidence:**\n\n1. **No official documentation** on skill auto-invocation mechanism\n2. **Anecdotal evidence** suggests skills aren't reliably auto-invoked\n3. **Alternative patterns exist** (MCP, sub-agents) that may be preferred by Claude\n4. **Skill pattern is new** (introduced recently?) and may not be fully integrated\n\n**Against this hypothesis:**\n\n1. Beyond-MCP README confidently claims auto-invocation\n2. Anthropic publishes skill examples, suggesting they work\n3. Kalshi skill is well-designed and should work if any skill does\n\n**Fix effort:** HIGH (weeks to months)\n- Can't fix pattern limitations ourselves\n- Would need Anthropic to improve skill discovery\n- Alternative: Switch to different pattern (MCP, sub-agents)\n\n**Likelihood this is the root cause:** LOW (20%)\n\n**Reasoning:**\n- Beyond-MCP demonstrates skills can work\n- More likely our implementation is weak than pattern is broken\n- BUT: Without official docs, we can't rule this out\n\n### Most Likely Root Cause\n\n**Answer: Hypothesis A (Weak Description) - 70% confidence**\n\n**Reasoning:**\n\n1. **Comparison shows clear gaps** in claude-mem's description vs Kalshi\n2. **Trigger keywords matter** - this is how Claude matches user input to skills\n3. **Quick fix with high impact** - rewriting description is low-effort, high-reward\n4. **Supported by beyond-MCP evidence** - Kalshi's strong description is the first thing you see\n\n**Combined factors:**\n- Weak description (Hypothesis A) makes skill hard to discover\n- Progressive disclosure tradeoff (Hypothesis B) makes skill hard to use after discovery\n- Together these create a \"double penalty\" reducing autonomy\n\n**Recommended approach:**\n1. Fix Hypothesis A first (quick win)\n2. Measure improvement (does Claude use skill more?)\n3. If still not working, address Hypothesis B\n4. If still not working, consider Hypothesis C (pattern limitation)\n\n## 5. Specific Recommendations for claude-mem\n\n### Quick Wins (Hours)\n\n#### 1. Improve trigger keywords: Concrete nouns\n\n**Current description:**\n```\nSearch claude-mem persistent memory for past sessions, observations, bugs fixed,\nfeatures implemented, decisions made, code changes, and previous work.\n```\n\n**Problems:**\n- \"observations\" is abstract (what's an observation?)\n- \"previous work\" is generic (too broad)\n- Missing user language (\"what did we do last time?\")\n\n**Suggested rewrite:**\n```\nSearch persistent memory for past sessions, bug fixes, feature implementations,\narchitectural decisions, code changes, and completed work. Use when user asks\n\"what did we do before?\", \"did we fix this already?\", \"how did we implement X?\",\nor needs historical context about the codebase.\n```\n\n**Improvements:**\n- \"bug fixes\" (noun) vs \"bugs fixed\" (past participle)\n- \"feature implementations\" (concrete) vs \"features implemented\" (abstract)\n- \"architectural decisions\" (specific) vs \"decisions made\" (generic)\n- Explicit user language examples in quotes\n\n**Even better - add explicit trigger section after frontmatter:**\n\n```markdown\n## Auto-Invocation Triggers\n\nThis skill automatically activates when Claude detects user questions like:\n\n- \"What did we do last session?\"\n- \"Did we fix this bug before?\"\n- \"How did we implement authentication?\"\n- \"What changes were made to auth/login.ts?\"\n- \"What were we working on yesterday?\"\n- \"Show me past decisions about database choice\"\n\nKeywords: past work, previous sessions, bug fix history, implementation details,\narchitectural decisions, code change history, historical context\n```\n\n#### 2. Surface capabilities: Show operations in SKILL.md\n\n**Current structure:**\n```\n## Available Operations\n1. Search Observations - Find observations by keyword\n2. Search Sessions - Search session summaries\n... (8 more with one-line descriptions)\n```\n\n**Problem:** No \"When to use\" statements visible in SKILL.md\n\n**Suggested addition to SKILL.md:**\n\n```markdown\n## Available Operations (Quick Reference)\n\n### Full-Text Search\n- **Search Observations** - User asks: \"How did we implement X?\" or \"What bugs did we fix?\"\n- **Search Sessions** - User asks: \"What did we accomplish last time?\" or \"What was the goal?\"\n- **Search Prompts** - User asks: \"Did I ask about this before?\" or \"What did I request?\"\n\n### Filtered Search\n- **Search by Type** - User asks: \"Show me all bug fixes\" or \"List features we added\"\n- **Search by Concept** - User asks: \"What patterns did we discover?\" or \"Show gotchas\"\n- **Search by File** - User asks: \"What changes to auth.ts?\" or \"History of this file\"\n\n### Context Retrieval\n- **Get Recent Context** - User asks: \"What's been happening?\" or \"Catch me up\"\n- **Get Timeline** - User asks: \"What was happening around date X?\" or \"Show me context\"\n- **Timeline by Query** - User asks: \"When did we implement auth?\" (search + timeline)\n\nFor detailed usage of each operation, see operations/ directory.\n```\n\n**Impact:** Claude can make routing decision without reading operations/ files\n\n#### 3. Reduce cognitive load: Simplify decision tree\n\n**Current problem:** 10 operations organized into 4 categories feels complex\n\n**Suggested simplification in SKILL.md:**\n\n```markdown\n## Quick Decision Guide\n\n**Step 1: What's the user asking about?**\n\n1. Recent work (last few sessions) → [Get Recent Context](operations/recent-context.md)\n2. Specific topic/keyword → [Search Observations](operations/observations.md)\n3. Specific file history → [Search by File](operations/by-file.md)\n4. Timeline/chronology → [Get Timeline](operations/timeline.md)\n5. Other/complex query → Read full guide below\n\n**Step 2: Execute the search** (see operation file for details)\n\n**Step 3: Present results** (see [formatting guide](operations/formatting.md))\n```\n\n**Impact:** Reduces decision points from 10 options to 5 common cases\n\n### Medium Effort (Days)\n\n#### 1. Adopt scripts pattern: Executable helpers\n\n**Current pattern:**\n- Operations are documentation\n- Claude reads operations/X.md → Constructs HTTP call → Executes\n\n**Suggested pattern:**\n- Operations are scripts\n- Claude reads SKILL.md → Runs script with `--help` → Executes\n\n**Example: scripts/search-observations.py**\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nSearch claude-mem observations by keyword.\n\nUsage:\n    uv run scripts/search-observations.py \"authentication\" --limit 10 --format index\n    uv run scripts/search-observations.py \"bug\" --project myapp --format full\n\"\"\"\nimport click\nimport httpx\n\n@click.command()\n@click.argument('query')\n@click.option('--format', default='index', type=click.Choice(['index', 'full']))\n@click.option('--limit', default=10)\n@click.option('--project', default=None)\ndef search(query, format, limit, project):\n    \"\"\"Search observations by keyword\"\"\"\n    params = {'query': query, 'format': format, 'limit': limit}\n    if project:\n        params['project'] = project\n\n    response = httpx.get('http://localhost:37777/api/search/observations', params=params)\n    click.echo(response.text)\n\nif __name__ == '__main__':\n    search()\n```\n\n**Benefits:**\n- Reduces navigation depth (1 hop instead of 2)\n- Scripts are self-documenting (`--help`)\n- Easier for Claude to discover usage patterns\n- Matches the Kalshi pattern\n\n**Effort:** 1-2 days to create 10 scripts + update SKILL.md\n\n#### 2. Rebalance progressive disclosure: Show more, hide less\n\n**Current approach:**\n- SKILL.md shows meta-information (97 lines)\n- operations/ shows usage patterns (hidden)\n\n**Suggested approach:**\n- SKILL.md shows usage patterns (150 lines?)\n- scripts/ shows implementation (hidden)\n\n**What to move INTO SKILL.md:**\n- \"When to use\" for each operation (from operations/*.md)\n- Common example queries (from common-workflows.md)\n- Response format examples (from formatting.md)\n\n**What to move OUT of SKILL.md:**\n- Technical notes (port, FTS5 details)\n- Performance tips (can be in individual scripts)\n- Error handling details (in scripts)\n\n**Effort:** 1 day to reorganize\n\n### Large Changes (Weeks)\n\n#### 1. Change pattern entirely: MCP vs Skills\n\n**If quick and medium efforts don't improve auto-invocation:**\n\nConsider switching from Skill pattern to MCP pattern:\n\n**MCP advantages:**\n- Explicitly listed in Claude's tool list (high discoverability)\n- Structured function signatures (clear inputs/outputs)\n- Better for complex multi-parameter operations\n\n**MCP disadvantages:**\n- Requires server process (complexity)\n- Context loss between calls (stateless)\n- Not git-shareable (server config required)\n\n**Effort:** 1-2 weeks to implement MCP server + test\n\n**Likelihood needed:** LOW (10%)\n- Try fixing description first\n- Skills should work if implemented well\n\n## 6. Agent Autonomy Assessment\n\n### Auto-Invoked?\n\n**Answer: ❌ No (currently), ✅ Yes (possible with improvements)**\n\n### Why?\n\n**Current state (No):**\n\n1. **Weak trigger keywords** - \"observations\", \"previous work\" are too abstract\n2. **Generic use cases** - \"answering questions about history\" matches many tools\n3. **Hidden capabilities** - Must navigate to operations/ to understand what's possible\n4. **High cognitive load** - 10 operations in 4 categories requires multiple decisions\n\n**After improvements (Possible Yes):**\n\n1. **Strong trigger keywords** - Explicit list with user language examples\n2. **Specific use cases** - Clear mapping from user question to operation\n3. **Visible capabilities** - \"When to use\" statements in SKILL.md\n4. **Low cognitive load** - Quick decision guide with 5 common cases\n\n### Why Kalshi Works (Inference)\n\n**Kalshi is auto-invoked (likely) because:**\n\n1. **Rich trigger keywords in description:**\n   - \"prediction markets, Kalshi markets, betting odds, market prices\"\n   - These are concrete, specific nouns\n   - High match probability when user asks about these topics\n\n2. **Clear capability visibility:**\n   - 10 scripts listed with \"When to use\" statements\n   - No hidden navigation required\n   - One-hop execution path\n\n3. **Low cognitive load:**\n   - User question → Matching script (direct mapping)\n   - No intermediate routing decisions\n\n4. **Explicit design for autonomy:**\n   - \"Don't read scripts unless absolutely needed\"\n   - `--help` flag for discovery\n   - Self-contained tools\n\n### Confidence\n\n**Confidence in autonomy assessment: HIGH (80%)**\n\n**Why High:**\n\n1. **Clear comparison** shows specific gaps in claude-mem implementation\n2. **Design patterns** align with autonomy principles\n3. **Quick wins identified** that directly address the gaps\n\n**Why not 100%:**\n\n1. No official documentation on skill auto-invocation mechanism\n2. No empirical testing of improvements yet\n3. Possible that pattern itself has limitations\n\n### Evidence from README\n\n**Beyond-MCP README evidence supporting auto-invocation:**\n\n1. **Explicit claim:** \"Model-Invoked - Claude autonomously activates based on context\"\n2. **Keyword detection:** \"The skill automatically activates when Claude detects keywords like:\"\n3. **Flow diagram:** \"Claude (detects keyword) -> Loads SKILL.md -> Runs relevant script\"\n4. **Usage examples:** \"# Claude will auto-detect and use the skill\"\n5. **Feature list:** \"Want autonomous skill discovery\"\n\n**Quality of evidence: MEDIUM**\n\n- Claims are clear and specific\n- No empirical proof (logs, metrics, screenshots)\n- Consistent pattern across repository\n- Professional presentation suggests testing\n\n**Inference:** Auto-invocation is likely real but depends on proper implementation\n\n---\n\n## Summary & Next Steps\n\n### Key Findings\n\n1. **Root cause:** Weak trigger keywords and hidden capabilities\n2. **Quick fix:** Rewrite SKILL.md description with concrete triggers\n3. **Pattern validation:** Kalshi demonstrates skills CAN be auto-invoked\n4. **Confidence:** HIGH that improvements will help\n\n### Recommended Actions\n\n**Phase 1: Quick Wins (2-4 hours)**\n1. Rewrite SKILL.md description with concrete trigger keywords\n2. Add explicit \"Auto-Invocation Triggers\" section with user language examples\n3. Add \"When to use\" statements inline for each operation in SKILL.md\n\n**Phase 2: Validation (1-2 days)**\n1. Test with real user questions: \"what did we do last time?\"\n2. Observe if Claude invokes skill autonomously\n3. Measure improvement vs baseline\n\n**Phase 3: Medium Effort (if needed)**\n1. Create executable scripts (adopt Kalshi pattern)\n2. Rebalance progressive disclosure (show usage, hide implementation)\n3. Simplify decision tree (5 common cases instead of 10 operations)\n\n**Phase 4: Pattern Switch (if still needed)**\n1. Consider MCP implementation\n2. Compare discoverability metrics\n3. Choose best pattern for claude-mem use case\n\n### Confidence in Recommendations\n\n**Confidence: HIGH (85%)**\n\n- Comparison clearly shows gaps\n- Kalshi pattern is proven (in showcase repo)\n- Quick wins are low-risk, high-reward\n- Medium effort changes are well-understood\n\n**Risk factors:**\n- No official documentation on skill matching algorithm\n- Beyond-MCP might be showcase (not production-tested)\n- Pattern limitations can't be ruled out without trying\n\n**Mitigation:**\n- Start with quick wins (low investment)\n- Measure impact before medium effort changes\n- Keep MCP as backup plan\n\n---\n\n**Document version:** 1.0\n**Date:** 2025-11-10\n**Agent:** Sub-Agent 4 (Skill Analysis)\n**Status:** Analysis complete, recommendations ready for implementation\n",
    "github": {
      "stars": 10,
      "forks": 1,
      "owner": "basher83",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/129473091?v=4",
      "license": "MIT",
      "lastCommit": "2026-01-02T22:16:24Z",
      "createdAt": "2025-10-24T02:41:21Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:44.334Z"
  },
  {
    "id": "vienna-cooking",
    "source": {
      "type": "github-search",
      "repo": "sandraschi/advanced-memory-mcp",
      "path": "docs/Skills/vienna-cooking-skill.md",
      "url": "https://github.com/sandraschi/advanced-memory-mcp/blob/main/docs/Skills/vienna-cooking-skill.md"
    },
    "frontmatter": {
      "name": "vienna-cooking",
      "description": "Traditional Viennese cuisine expertise covering classic dishes, techniques, and cultural context. Use when user asks about Austrian cooking, Viennese recipes, coffee house culture, traditional European cooking, Schnitzel, Sachertorte, Apfelstrudel, Tafelspitz, or any Vienna/Austria food-related questions.",
      "metadata": "",
      "version": "1.0.0",
      "author": "Sandra Schipal",
      "tags": "[cooking, austrian, vienna, traditional, recipes, european-cuisine]"
    },
    "content": "# Vienna Cooking Skill\n\n## YAML Frontmatter (CORRECT FORMAT - see above between ---) ✅\n\n## Metadata (OLD FORMAT - markdown bullets, NOT parsed by Claude) ❌\n- **name**: vienna-cooking\n- **description**: Traditional Viennese cuisine expertise covering classic dishes, techniques, and cultural context\n- **version**: 1.0.0\n- **author**: Sandra Schipal\n- **tags**: cooking, austrian, vienna, traditional, recipes\n\n## Skill Overview\n\nThis skill provides comprehensive knowledge of traditional Viennese cooking, including classic recipes, techniques, ingredient substitutions, and cultural context. Claude will be able to guide users through preparing authentic Viennese dishes with historical background and practical cooking tips.\n\n## Core Knowledge Areas\n\n### 1. Classic Viennese Dishes\n\n#### Wiener Schnitzel\n- **Definition**: Thin veal cutlet breaded and pan-fried in clarified butter\n- **Key Technique**: Meat must be pounded thin (3-4mm), never flatten the breading while frying\n- **Traditional Side**: Potato salad with pumpkin seed oil or parsley potatoes\n- **Common Mistake**: Using pork instead of veal (that's \"Schnitzel Wiener Art\")\n- **Temperature**: Medium-high heat, butter should sizzle but not brown\n\n#### Tafelspitz\n- **Definition**: Boiled beef (usually from the hip) served with root vegetables and chive sauce\n- **Cuts**: Tafelspitz, Schulterscherzel, Beinfleisch\n- **Broth Preparation**: Start in cold water, add vegetables after 1 hour\n- **Classic Accompaniments**: \n  - Apfelkren (horseradish-apple sauce)\n  - Schnittlauchsauce (chive sauce)\n  - Spinat (creamed spinach)\n  - Röstkartoffeln (roasted potatoes)\n\n#### Sachertorte\n- **Definition**: Dense chocolate cake with apricot jam filling and dark chocolate glaze\n- **Key Points**: \n  - Original recipe is a closely guarded secret\n  - Must have exactly one layer of apricot jam\n  - Glaze should be smooth and shiny\n- **Serving**: Always with unsweetened Schlagobers (whipped cream)\n\n#### Apfelstrudel\n- **Technique**: Pulled dough (Strudel... should be paper-thin\n- **Traditional Method**: Hand-stretched on a table cloth\n- **Filling**: Apples (Boskoop ideal), raisins, sugar, cinnamon, breadcrumbs\n- **Baking**: Brush with melted butter during baking for crispy layers\n\n### 2. Coffee House Culture\n\n#### Viennese Coffee Variations\n- **Melange**: Coffee with steamed milk (similar to cappuccino)\n- **Brauner**: Espresso with a splash of milk (Kleiner/Grosser Brauner)\n- **Einspänner**: Strong coffee in a glass with whipped cream on top\n- **Verlängerter**: Espresso with hot water (like americano)\n- **Franziskaner**: Melange topped with whipped cream and cocoa\n\n#### Coffee House Etiquette\n- One coffee can last hours (no pressure to order more)\n- Glass of water always served alongside\n- Newspapers provided for reading\n- Waiters traditionally addressed as \"Herr Ober\"\n\n### 3. Beisl (Traditional Tavern) Food\n\n#### Classics\n- **Gulasch**: Hungarian-influenced beef stew, served with Semmelknödel\n- **Schweinsbraten**: Roast pork with dumplings and sauerkraut\n- **Beuschel**: Veal lungs in cream sauce (acquired taste!)\n- **Geröstete Knödel**: Fried bread dumpling slices\n- **Erdäpfelgulasch**: Potato goulash (vegetarian option)\n\n#### Suppn (Soups)\n- **Rindsuppe**: Clear beef broth (base for many variations)\n- **Leberknödelsuppe**: Liver dumpling soup\n- **Frittatensuppe**: Sliced pancake strips in broth\n- **Grießnockerlsuppe**: Semolina dumpling soup\n\n### 4. Seasonal Specialties\n\n#### Spargel (Spring - May/June)\n- White asparagus season is celebrated\n- Served with hollandaise or butter sauce\n- Often with Wiener Schnitzel or boiled ham\n\n#### Schwammerl (Autumn)\n- Wild mushroom season (Eierschwammerl/chanterelles)\n- Schwammerlgulasch, Schwammerlsauce with Nockerl\n\n#### Martinigansl (November 11)\n- St. Martin's Goose tradition\n- Roasted goose with red cabbage and dumplings\n- Sturm (partially fermented grape juice) season\n\n#### Karpfen (Christmas Eve)\n- Traditional carp for Christmas Eve dinner\n- Often fried in breadcrumbs\n- Buddhist/Catholic tradition (meatless)\n\n### 5. Essential Ingredients\n\n#### Fats\n- **Butter**: For Schnitzel, baking\n- **Schmalz**: Rendered pork fat for savory cooking\n- **Kürbiskernöl**: Pumpkin seed oil (Styrian, for salads)\n\n#### Dairy\n- **Topfen**: Fresh cheese similar to quark (for Topfenknödel, cakes)\n- **Sauerrahm**: Sour cream\n- **Schlagobers**: Heavy whipping cream (min. 36% fat)\n\n#### Spices & Seasonings\n- **Majoran**: Marjoram (essential for many meat dishes)\n- **Kümmel**: Caraway seeds\n- **Kren**: Horseradish (fresh grated)\n- **Schnittlauch**: Chives\n\n#### Bread & Dumplings\n- **Semmel**: Kaiser roll (essential for breadcrumbs, dumplings)\n- **Serviettenknödel**: Large dumpling cooked in cloth\n- **Semmelknödel**: Bread dumplings\n\n### 6. Techniques\n\n#### Breadcrumb Coating (Panier)\n1. Flour (season with salt)\n2. Beaten eggs\n3. Fresh breadcrumbs (from Semmeln, never packaged)\n- Pat meat dry, coat sequentially, let rest 10 minutes\n\n#### Dumpling Making\n- Stale bread is essential (never fresh)\n- Milk-egg mixture should soak 15-20 minutes\n- Test one dumpling first (should hold together)\n- Simmer, never boil vigorously\n\n#### Strudel Dough\n- High-gluten flour essential\n- Dough must rest 30+ minutes\n- Room temperature for stretching\n- Work from center outward on floured cloth\n\n### 7. Common Substitutions (for international cooks)\n\n- **Topfen** → Quark, farmer's cheese, or drained ricotta\n- **Schlagobers** → Heavy cream (36%+ fat)\n- **Sauerrahm** → Sour cream or crème fraîche\n- **Semmeln** → Kaiser rolls or crusty white rolls\n- **Kürbiskernöl** → Can't substitute! (unique flavor)\n- **Kren** → Fresh horseradish root\n- **Powidl** → Plum butter/Pflaumenmus\n\n### 8. Vienna-Specific Vocabulary\n\n- **Paradeiser**: Tomatoes (Paradeisersalat = tomato salad)\n- **Erdäpfel**: Potatoes (not Kartoffeln)\n- **Karfiol**: Cauliflower\n- **Fisolen**: Green beans\n- **Ribisel**: Currants\n- **Marillen**: Apricots\n- **Obers**: Cream\n- **Schale Gold**: Strong coffee (literally \"cup of gold\")\n\n## Usage Guidelines\n\n### When to Apply This Skill\n- User asks about Austrian or Viennese recipes\n- Questions about traditional European cooking techniques\n- Coffee house culture queries\n- Requests for authentic vs. adapted recipes\n- Troubleshooting traditional dish preparation\n\n### Communication Style\n- Provide cultural context alongside recipes\n- Mention authentic vs. practical substitutions\n- Include historical background when relevant\n- Warn about common mistakes\n- Suggest traditional accompaniments and pairings\n\n### Recipe Format\nWhen providing recipes, include:\n1. **Authentic name** (German + English)\n2. **Serves**: Number of portions\n3. **Time**: Prep + cooking\n4. **Difficulty**: Easy/Medium/Advanced\n5. **Ingredients** (with substitutions noted)\n6. **Method** (step-by-step)\n7. **Traditional Serving**: How it's served in Vienna\n8. **Tips**: Common mistakes to avoid\n\n## Example Interactions\n\n### User: \"How do I make Wiener Schnitzel?\"\n\n**Claude Response Using Skill:**\nHere's how to make authentic Wiener Schnitzel, the pride of Vienna:\n\n**Wiener Schnitzel** (Viennese Veal Cutlet)\n*Serves: 4 | Time: 30 minutes | Difficulty: Medium*\n\n**Ingredients:**\n- 4 veal cutlets (Kalbsschnitzel), 150-180g each\n- Salt\n- 100g flour\n- 2 eggs, beaten\n- 200g fresh breadcrumbs (from day-old Semmeln/Kaiser rolls)\n- 200g clarified butter\n- Lemon wedges for serving\n\n**Method:**\n1. **Pound the veal:** Place cutlets between plastic wrap, pound to 3-4mm thickness\n2. **Season:** Salt both sides lightly\n3. **Prepare stations:** Three shallow dishes with flour, beaten eggs, breadcrumbs\n4. **Coat:** Pat dry, dredge in flour, dip in egg, press into breadcrumbs (don't press too hard)\n5. **Rest:** Let coated schnitzels rest 10 minutes\n6. **Heat butter:** Medium-high heat in large pan, butter should cover bottom generously\n7. **Fry:** Place schnitzel carefully, it should sizzle immediately. Swirl pan gently to create waves in the breading. Fry 2-3 minutes per side until golden\n8. **Drain:** Briefly on paper towels\n\n**Critical Tips:**\n- NEVER press down while frying - the breading should stay fluffy\n- The schnitzel should \"swim\" in butter (wavy coating is the sign of perfection)\n- Veal is traditional - pork makes it \"Schnitzel Wiener Art\" (Viennese-style), not true Wiener Schnitzel\n\n**Traditional Serving:**\nServe immediately with lemon wedges and either:\n- Erdäpfelsalat (potato salad with pumpkin seed oil)\n- Petersilerdäpfel (parsley potatoes)\n- Green salad with vinegar-oil dressing\n\nIn Vienna, you'll never see it with sauce - the crispy coating is the star!\n\n---\n\n### User: \"What's a good Viennese dessert that's easier than Sachertorte?\"\n\n**Claude Response Using Skill:**\nTry **Topfenknödel** (sweet cheese dumplings) - a beloved Viennese comfort dessert that's much simpler than elaborate tortes!\n\n**Topfenknödel** (Sweet Cheese Dumplings)\n*Serves: 4 (3 dumplings each) | Time: 45 minutes | Difficulty: Easy*\n\n**Ingredients:**\n- 500g Topfen (quark/farmer's cheese), well-drained\n- 2 eggs\n- 80g fine semolina (Grieß)\n- 40g sugar\n- Pinch salt\n- Zest of 1 lemon\n\n**Coating:**\n- 100g breadcrumbs\n- 50g butter\n- 2 tbsp sugar\n- 1 tsp cinnamon\n\n**Method:**\n1. **Mix:** Combine Topfen, eggs, semolina, sugar, salt, and lemon zest. Let rest 20 minutes\n2. **Test:** Bring pot of water to gentle simmer. Form small test dumpling, cook 10 minutes. If it falls apart, add more semolina\n3. **Shape:** Form 12 dumplings with wet hands\n4. **Cook:** Simmer (don't boil!) 10-12 minutes. They'll float when done\n5. **Coat:** While cooking, brown breadcrumbs in butter, add sugar and cinnamon\n6. **Serve:** Roll cooked dumplings in crumb mixture\n\n**Serve with:** Powidl (plum butter) or stewed plums\n\n**Why it's perfect:**\n- Forgiving recipe (hard to mess up)\n- Make-ahead friendly\n- Quintessentially Viennese but not fussy\n- Kids love them!\n\n**Substitution:** If you can't find Topfen, use well-drained ricotta or farmer's cheese.\n\n---\n\n## Advanced Topics\n\n### Restaurant Recommendations Context\nIf users ask about where to eat in Vienna, Claude should mention:\n- **Traditional Beisln**: Gasthaus Pöschl, Zum Schwarzen Kameel\n- **Coffee Houses**: Café Central, Café Sperl, Café Hawelka\n- **Schnitzel**: Figlmüller, Schnitzelwirt\n- **Modern Austrian**: Steirereck, Mraz & Sohn\n- Always note: Reservations recommended, especially for dinner\n\n### Vegetarian Considerations\nTraditional Viennese cuisine is meat-heavy, but vegetarian options exist:\n- Erdäpfelgulasch\n- Käsespätzle\n- Schwammerlgulasch (mushroom goulash)\n- Many soups (without meat broth)\n- All desserts and pastries\n\n### Wine Pairing Basics\n- **White wines dominate**: Grüner Veltliner, Riesling from Wachau\n- **Gemischter Satz**: Traditional Viennese wine blend\n- **Sturm**: Partially fermented grape juice (autumn specialty)\n- **Heuriger tradition**: Wine taverns in wine-growing districts\n\n## Quality Standards\n\nWhen using this skill, Claude should:\n- ✅ Distinguish between authentic Viennese and adapted recipes\n- ✅ Provide cultural context that enhances understanding\n- ✅ Offer practical substitutions for international cooks\n- ✅ Warn about common pitfalls and mistakes\n- ✅ Use correct German terminology with translations\n- ✅ Respect the traditions while being helpful to beginners\n\n## Skill Limitations\n\nClaude should acknowledge when:\n- Asking about very specific restaurant details beyond general knowledge\n- Regional variations from other parts of Austria differ significantly\n- Family recipes might vary from \"standard\" versions\n- Modern fusion interpretations vs. traditional preparation\n\n---\n\n*This skill represents traditional Viennese cooking knowledge as practiced in Vienna, Austria. Recipes and techniques are based on established culinary tradition and home cooking practices.*\n",
    "github": {
      "stars": 3,
      "forks": 1,
      "owner": "sandraschi",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/34750307?v=4",
      "license": "AGPL-3.0",
      "lastCommit": "2025-12-21T01:26:07Z",
      "createdAt": "2025-10-01T14:16:16Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:45.773Z"
  },
  {
    "id": "tools",
    "source": {
      "type": "github-search",
      "repo": "flight505/ClaudeCommands",
      "path": "tools/newskill.md",
      "url": "https://github.com/flight505/ClaudeCommands/blob/main/tools/newskill.md"
    },
    "frontmatter": {},
    "content": "description: Create reusable command or Skill scaffolds tailored to your project's recurring tasks.\nargument-hint: <description>\nallowed-tools: Filesystem\n---\n\n# Create New Custom Skill\n\nYou are tasked with creating a new custom Skill for Claude based on the user's description. Follow the process below carefully to ensure the Skill is well-structured, follows best practices, and is ready for immediate use.\n\n## User's Skill Description\n\n$ARGUMENTS\n\n## Your Task\n\nCreate a complete, production-ready custom Skill following the structure and best practices outlined below. Use chain of thought reasoning to ensure the Skill is well-designed.\n\n<thinking>\n\nBefore creating the Skill, think through the following systematically:\n\n1. **Understand the purpose**: What specific problem does this Skill solve? What workflows does it enable?\n\n2. **Determine scope**: Is this Skill focused enough? Does it try to do one thing well, or is it too broad?\n\n3. **Identify when to use it**: In what situations should Claude invoke this Skill? What keywords or contexts should trigger it?\n\n4. **Plan the structure**: \n   - What metadata is required (name, description, version, dependencies)?\n   - Should this Skill include additional resource files?\n   - Does it need executable scripts or code?\n   - What examples would be helpful?\n\n5. **Consider the user's workflow**: How will this Skill integrate with their existing processes?\n\n6. **Think about completeness**: What information needs to be included to make this Skill immediately useful?\n\n</thinking>\n\n## Skill Structure Guidelines\n\n### Required Components\n\n1. **Metadata (YAML frontmatter)**:\n   - `name`: Human-friendly name (64 chars max)\n   - `description`: Clear description of what the Skill does and when to use it (200 chars max) - CRITICAL for Claude to know when to invoke this Skill\n   - `version`: Optional version tracking (e.g., 1.0.0)\n   - `dependencies`: Optional software packages required\n\n2. **Markdown Body**:\n   - Overview section explaining the Skill's purpose\n   - Clear instructions for Claude\n   - When to apply guidelines\n   - Examples (when helpful)\n   - Any specific workflows or processes\n\n### Best Practices\n\n- **Keep it focused**: Solve one specific, repeatable task well\n- **Write clear descriptions**: Be specific about when the Skill applies\n- **Include examples**: Show what success looks like when helpful\n- **Use clear structure**: Organize with headers and sections\n- **Be explicit**: Don't assume Claude knows your workflows or preferences\n\n### Progressive Disclosure\n\nThe Skill system uses progressive disclosure:\n1. **First level (metadata)**: Claude reads this to determine IF the Skill should be used\n2. **Second level (markdown body)**: Claude accesses this WHEN executing the Skill\n3. **Third level (resources)**: Additional files Claude can reference if needed\n\n## Creation Process\n\n1. **Analyze the description**: Understand what the user needs\n2. **Design the Skill structure**: Plan metadata, sections, and content\n3. **Create the directory**: Make a folder named after the Skill (lowercase with hyphens)\n4. **Write SKILL.md**: Include frontmatter and well-organized markdown content\n5. **Add resources if needed**: Create additional files only if the Skill is complex enough to warrant them\n6. **Present the result**: Show the user the complete Skill structure\n\n## Output Format\n\nAfter your thinking, create the Skill with the following structure:\n\n```\nskill-name/\n├── SKILL.md          (Required: main Skill file)\n├── REFERENCE.md      (Optional: supplemental information)\n├── EXAMPLES.md       (Optional: detailed examples)\n└── resources/        (Optional: scripts, templates, etc.)\n```\n\nFor most Skills, a single well-crafted SKILL.md file is sufficient.\n\n## Example SKILL.md Template\n\n```markdown\n---\nname: Skill Name\ndescription: Brief description of what this Skill does and when to use it\nversion: 1.0.0\ndependencies: package>=version (if needed)\n---\n\n## Overview\n\nExplain the Skill's purpose and value. When should Claude use this Skill? What problem does it solve?\n\n## Instructions\n\nProvide clear, specific instructions for Claude to follow when executing this Skill. Be detailed but organized.\n\n### Section 1\n[Detailed guidance]\n\n### Section 2\n[More guidance]\n\n## When to Apply\n\nList specific situations where this Skill should be used:\n- Condition 1\n- Condition 2\n- Condition 3\n\n## Examples\n\n### Example 1: [Scenario]\nInput: [Example input]\nExpected output: [What success looks like]\n\n### Example 2: [Another scenario]\n[Another example if helpful]\n\n## Additional Guidelines\n\nAny other important information, constraints, or best practices.\n```\n\n## Important Reminders\n\n- The `description` field is CRITICAL - Claude uses it to determine when to invoke your Skill\n- Keep Skills focused on one workflow rather than trying to do everything\n- Start simple - you can always expand the Skill later\n- Include examples when they would help Claude understand the expected output\n- Use clear, unambiguous language\n- Test with example prompts after creation\n\n## Now Execute\n\nBased on the user's description: \"$ARGUMENTS\"\n\n1. Think through the Skill design using the structured thinking process above\n2. Create the appropriate directory structure\n3. Write a complete, production-ready SKILL.md file\n4. Add any necessary resource files if the Skill requires them\n5. Present the complete Skill to the user with:\n   - The directory structure\n   - The full SKILL.md content\n   - Any additional files created\n   - A brief explanation of how to install and use it\n\nCreate the Skill files in the current directory under a new folder with an appropriate name (lowercase with hyphens).\n\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "flight505",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/20601200?v=4",
      "lastCommit": "2025-11-12T09:33:13Z",
      "createdAt": "2025-11-06T01:38:56Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:46.419Z"
  },
  {
    "id": "ui-ux-pro-max",
    "source": {
      "type": "github-search",
      "repo": "zhom/donutbrowser",
      "path": ".claude/skills/ui-ux-pro-max/SKILL.md",
      "url": "https://github.com/zhom/donutbrowser/blob/main/.claude/skills/ui-ux-pro-max/SKILL.md"
    },
    "frontmatter": {
      "name": "ui-ux-pro-max",
      "description": "\"UI/UX design intelligence. 50 styles, 21 palettes, 50 font pairings, 20 charts, 8 stacks (React, Next.js, Vue, Svelte, SwiftUI, React Native, Flutter, Tailwind). Actions: plan, build, create, design, implement, review, fix, improve, optimize, enhance, refactor, check UI/UX code. Projects: website, landing page, dashboard, admin panel, e-commerce, SaaS, portfolio, blog, mobile app, .html, .tsx, .vue, .svelte. Elements: button, modal, navbar, sidebar, card, table, form, chart. Styles: glassmorphism, claymorphism, minimalism, brutalism, neumorphism, bento grid, dark mode, responsive, skeuomorphism, flat design. Topics: color palette, accessibility, animation, layout, typography, font pairing, spacing, hover, shadow, gradient.\""
    },
    "content": "# UI/UX Pro Max - Design Intelligence\r\n\r\nSearchable database of UI styles, color palettes, font pairings, chart types, product recommendations, UX guidelines, and stack-specific best practices.\r\n\r\n## Prerequisites\r\n\r\nCheck if Python is installed:\r\n\r\n```bash\r\npython3 --version || python --version\r\n```\r\n\r\nIf Python is not installed, install it based on user's OS:\r\n\r\n**macOS:**\r\n```bash\r\nbrew install python3\r\n```\r\n\r\n**Ubuntu/Debian:**\r\n```bash\r\nsudo apt update && sudo apt install python3\r\n```\r\n\r\n**Windows:**\r\n```powershell\r\nwinget install Python.Python.3.12\r\n```\r\n\r\n---\r\n\r\n## How to Use This Skill\r\n\r\nWhen user requests UI/UX work (design, build, create, implement, review, fix, improve), follow this workflow:\r\n\r\n### Step 1: Analyze User Requirements\r\n\r\nExtract key information from user request:\r\n- **Product type**: SaaS, e-commerce, portfolio, dashboard, landing page, etc.\r\n- **Style keywords**: minimal, playful, professional, elegant, dark mode, etc.\r\n- **Industry**: healthcare, fintech, gaming, education, etc.\r\n- **Stack**: React, Vue, Next.js, or default to `html-tailwind`\r\n\r\n### Step 2: Search Relevant Domains\r\n\r\nUse `search.py` multiple times to gather comprehensive information. Search until you have enough context.\r\n\r\n```bash\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"<keyword>\" --domain <domain> [-n <max_results>]\r\n```\r\n\r\n**Recommended search order:**\r\n\r\n1. **Product** - Get style recommendations for product type\r\n2. **Style** - Get detailed style guide (colors, effects, frameworks)\r\n3. **Typography** - Get font pairings with Google Fonts imports\r\n4. **Color** - Get color palette (Primary, Secondary, CTA, Background, Text, Border)\r\n5. **Landing** - Get page structure (if landing page)\r\n6. **Chart** - Get chart recommendations (if dashboard/analytics)\r\n7. **UX** - Get best practices and anti-patterns\r\n8. **Stack** - Get stack-specific guidelines (default: html-tailwind)\r\n\r\n### Step 3: Stack Guidelines (Default: html-tailwind)\r\n\r\nIf user doesn't specify a stack, **default to `html-tailwind`**.\r\n\r\n```bash\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"<keyword>\" --stack html-tailwind\r\n```\r\n\r\nAvailable stacks: `html-tailwind`, `react`, `nextjs`, `vue`, `svelte`, `swiftui`, `react-native`, `flutter`\r\n\r\n---\r\n\r\n## Search Reference\r\n\r\n### Available Domains\r\n\r\n| Domain | Use For | Example Keywords |\r\n|--------|---------|------------------|\r\n| `product` | Product type recommendations | SaaS, e-commerce, portfolio, healthcare, beauty, service |\r\n| `style` | UI styles, colors, effects | glassmorphism, minimalism, dark mode, brutalism |\r\n| `typography` | Font pairings, Google Fonts | elegant, playful, professional, modern |\r\n| `color` | Color palettes by product type | saas, ecommerce, healthcare, beauty, fintech, service |\r\n| `landing` | Page structure, CTA strategies | hero, hero-centric, testimonial, pricing, social-proof |\r\n| `chart` | Chart types, library recommendations | trend, comparison, timeline, funnel, pie |\r\n| `ux` | Best practices, anti-patterns | animation, accessibility, z-index, loading |\r\n| `prompt` | AI prompts, CSS keywords | (style name) |\r\n\r\n### Available Stacks\r\n\r\n| Stack | Focus |\r\n|-------|-------|\r\n| `html-tailwind` | Tailwind utilities, responsive, a11y (DEFAULT) |\r\n| `react` | State, hooks, performance, patterns |\r\n| `nextjs` | SSR, routing, images, API routes |\r\n| `vue` | Composition API, Pinia, Vue Router |\r\n| `svelte` | Runes, stores, SvelteKit |\r\n| `swiftui` | Views, State, Navigation, Animation |\r\n| `react-native` | Components, Navigation, Lists |\r\n| `flutter` | Widgets, State, Layout, Theming |\r\n\r\n---\r\n\r\n## Example Workflow\r\n\r\n**User request:** \"Làm landing page cho dịch vụ chăm sóc da chuyên nghiệp\"\r\n\r\n**AI should:**\r\n\r\n```bash\r\n# 1. Search product type\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"beauty spa wellness service\" --domain product\r\n\r\n# 2. Search style (based on industry: beauty, elegant)\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"elegant minimal soft\" --domain style\r\n\r\n# 3. Search typography\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"elegant luxury\" --domain typography\r\n\r\n# 4. Search color palette\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"beauty spa wellness\" --domain color\r\n\r\n# 5. Search landing page structure\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"hero-centric social-proof\" --domain landing\r\n\r\n# 6. Search UX guidelines\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"animation\" --domain ux\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"accessibility\" --domain ux\r\n\r\n# 7. Search stack guidelines (default: html-tailwind)\r\npython3 .claude/skills/ui-ux-pro-max/scripts/search.py \"layout responsive\" --stack html-tailwind\r\n```\r\n\r\n**Then:** Synthesize all search results and implement the design.\r\n\r\n---\r\n\r\n## Tips for Better Results\r\n\r\n1. **Be specific with keywords** - \"healthcare SaaS dashboard\" > \"app\"\r\n2. **Search multiple times** - Different keywords reveal different insights\r\n3. **Combine domains** - Style + Typography + Color = Complete design system\r\n4. **Always check UX** - Search \"animation\", \"z-index\", \"accessibility\" for common issues\r\n5. **Use stack flag** - Get implementation-specific best practices\r\n6. **Iterate** - If first search doesn't match, try different keywords\r\n\r\n---\r\n\r\n## Common Rules for Professional UI\r\n\r\nThese are frequently overlooked issues that make UI look unprofessional:\r\n\r\n### Icons & Visual Elements\r\n\r\n| Rule | Do | Don't |\r\n|------|----|----- |\r\n| **No emoji icons** | Use SVG icons (Heroicons, Lucide, Simple Icons) | Use emojis like 🎨 🚀 ⚙️ as UI icons |\r\n| **Stable hover states** | Use color/opacity transitions on hover | Use scale transforms that shift layout |\r\n| **Correct brand logos** | Research official SVG from Simple Icons | Guess or use incorrect logo paths |\r\n| **Consistent icon sizing** | Use fixed viewBox (24x24) with w-6 h-6 | Mix different icon sizes randomly |\r\n\r\n### Interaction & Cursor\r\n\r\n| Rule | Do | Don't |\r\n|------|----|----- |\r\n| **Cursor pointer** | Add `cursor-pointer` to all clickable/hoverable cards | Leave default cursor on interactive elements |\r\n| **Hover feedback** | Provide visual feedback (color, shadow, border) | No indication element is interactive |\r\n| **Smooth transitions** | Use `transition-colors duration-200` | Instant state changes or too slow (>500ms) |\r\n\r\n### Light/Dark Mode Contrast\r\n\r\n| Rule | Do | Don't |\r\n|------|----|----- |\r\n| **Glass card light mode** | Use `bg-white/80` or higher opacity | Use `bg-white/10` (too transparent) |\r\n| **Text contrast light** | Use `#0F172A` (slate-900) for text | Use `#94A3B8` (slate-400) for body text |\r\n| **Muted text light** | Use `#475569` (slate-600) minimum | Use gray-400 or lighter |\r\n| **Border visibility** | Use `border-gray-200` in light mode | Use `border-white/10` (invisible) |\r\n\r\n### Layout & Spacing\r\n\r\n| Rule | Do | Don't |\r\n|------|----|----- |\r\n| **Floating navbar** | Add `top-4 left-4 right-4` spacing | Stick navbar to `top-0 left-0 right-0` |\r\n| **Content padding** | Account for fixed navbar height | Let content hide behind fixed elements |\r\n| **Consistent max-width** | Use same `max-w-6xl` or `max-w-7xl` | Mix different container widths |\r\n\r\n---\r\n\r\n## Pre-Delivery Checklist\r\n\r\nBefore delivering UI code, verify these items:\r\n\r\n### Visual Quality\r\n- [ ] No emojis used as icons (use SVG instead)\r\n- [ ] All icons from consistent icon set (Heroicons/Lucide)\r\n- [ ] Brand logos are correct (verified from Simple Icons)\r\n- [ ] Hover states don't cause layout shift\r\n- [ ] Use theme colors directly (bg-primary) not var() wrapper\r\n\r\n### Interaction\r\n- [ ] All clickable elements have `cursor-pointer`\r\n- [ ] Hover states provide clear visual feedback\r\n- [ ] Transitions are smooth (150-300ms)\r\n- [ ] Focus states visible for keyboard navigation\r\n\r\n### Light/Dark Mode\r\n- [ ] Light mode text has sufficient contrast (4.5:1 minimum)\r\n- [ ] Glass/transparent elements visible in light mode\r\n- [ ] Borders visible in both modes\r\n- [ ] Test both modes before delivery\r\n\r\n### Layout\r\n- [ ] Floating elements have proper spacing from edges\r\n- [ ] No content hidden behind fixed navbars\r\n- [ ] Responsive at 320px, 768px, 1024px, 1440px\r\n- [ ] No horizontal scroll on mobile\r\n\r\n### Accessibility\r\n- [ ] All images have alt text\r\n- [ ] Form inputs have labels\r\n- [ ] Color is not the only indicator\r\n- [ ] `prefers-reduced-motion` respected\r\n",
    "github": {
      "stars": 804,
      "forks": 45,
      "owner": "zhom",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/2717306?v=4",
      "license": "AGPL-3.0",
      "lastCommit": "2026-01-03T19:28:17Z",
      "createdAt": "2025-05-29T06:13:50Z",
      "topics": [
        "anti-detect",
        "anti-detect-browser",
        "anti-detection",
        "browser",
        "browser-management",
        "chromium",
        "firefox",
        "management",
        "manager",
        "orchestration",
        "orchestrator",
        "privacy"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:48.424Z"
  },
  {
    "id": "tasks",
    "source": {
      "type": "github-search",
      "repo": "alekspetrov/navigator",
      "path": ".agent/tasks/TASK-16-product-design-skill.md",
      "url": "https://github.com/alekspetrov/navigator/blob/main/.agent/tasks/TASK-16-product-design-skill.md"
    },
    "frontmatter": {},
    "content": "# TASK-16: Product Design Skill with Figma MCP Integration\n\n**Created**: 2025-10-21\n**Status**: In Progress\n**Priority**: High\n**Complexity**: High\n**Estimated Time**: 8-12 hours\n\n---\n\n## Context\n\n**Problem**: Design handoff from Figma to code is manual, time-consuming (6-10 hours), and error-prone. Design system drift happens when tokens/components diverge between design and implementation.\n\n**Goal**: Create a Navigator skill that automates design review, token extraction, component mapping, and implementation planning using Figma MCP integration. Reduce design handoff time from 6-10 hours to 15-20 minutes (95% reduction).\n\n**Why This Matters**:\n- Eliminates manual design token extraction (currently 1+ hour)\n- Detects design system drift automatically\n- Prevents duplicate components via similarity matching\n- Generates implementation plans with acceptance criteria\n- Maintains Navigator's token-efficient, autonomous workflow\n\n---\n\n## Design Decisions\n\n### Architecture: Hybrid Skill\n\n**Progressive Enhancement Strategy**:\n- **Phase 1**: Manual design review (works without Figma MCP)\n- **Phase 2**: Automated extraction via Figma MCP\n- **Phase 3**: Advanced features (drift detection, visual regression)\n\n**Why Hybrid**:\n- Immediate value without MCP dependency\n- Graceful degradation when Figma Desktop not running\n- Supports teams without Figma Enterprise (Code Connect)\n\n### Documentation Structure\n\n```\n.agent/\n└── design-system/           # New directory (separate from system/)\n    ├── design-tokens.json   # DTCG format (W3C standard)\n    ├── ui-kit-inventory.md  # Component catalog\n    ├── component-mapping.json # Figma node ID → code path\n    └── reviews/             # Temporal design reviews\n        └── YYYY-MM-DD-[feature].md\n```\n\n**Why separate from `system/`**:\n- Design reviews are temporal (archived after implementation)\n- System docs are living architecture\n- Different loading patterns (reviews on-demand, inventory always)\n\n### Token Optimization\n\n**Never load**:\n- All design review reports (50+ over time = 250k+ tokens)\n- Full Figma MCP responses (can be 350k+ tokens)\n- Entire UI kit component code\n\n**Always load when skill active**:\n- `ui-kit-inventory.md` (~3k tokens)\n- `design-tokens.json` (~2k tokens)\n- Specific design review for active task (~5k tokens)\n\n**Total**: ~10k tokens vs 150k+ loading everything (93% reduction)\n\n---\n\n## Implementation Plan\n\n### Phase 1: Core Skill Structure (Manual Workflow)\n\n**Goal**: Working skill without Figma MCP dependency\n\n#### Step 1.1: Create Skill Directory Structure\n\n```bash\n.claude-plugin/skills/product-design/\n├── SKILL.md                    # Main skill prompt (2-3k tokens)\n├── functions/\n│   ├── design_analyzer.py      # Extract patterns from Figma data\n│   ├── token_extractor.py      # Variables → DTCG design tokens\n│   ├── component_mapper.py     # Figma component → code mapping\n│   ├── design_system_auditor.py # Compare design vs implementation\n│   └── implementation_planner.py # Generate task docs\n├── templates/\n│   ├── design-review-report.md # Analysis output template\n│   ├── design-tokens-diff.md   # Token changes summary\n│   └── ui-kit-impact.md        # Component library impact\n└── examples/\n    └── dashboard-redesign-review.md # Reference example\n```\n\n**Acceptance Criteria**:\n- [ ] Directory structure created\n- [ ] All function files exist with docstrings\n- [ ] All template files exist with placeholder content\n- [ ] Example design review demonstrates output format\n\n#### Step 1.2: Implement design_analyzer.py\n\n**Purpose**: Extract design patterns from Figma MCP data or manual analysis\n\n**Core Algorithm**:\n```python\ndef analyze_design(figma_metadata, variables, ui_kit_inventory):\n    \"\"\"\n    Analyzes Figma design data and compares against existing UI kit.\n\n    Args:\n        figma_metadata: Figma MCP get_metadata response or manual description\n        variables: Figma MCP get_variable_defs response or manual token list\n        ui_kit_inventory: Current UI kit inventory JSON\n\n    Returns:\n        {\n            \"new_tokens\": [...],\n            \"new_components\": [...],\n            \"similar_components\": [...],\n            \"breaking_changes\": [...]\n        }\n    \"\"\"\n```\n\n**Features**:\n- Extract unique component types from metadata\n- Compare against ui_kit_inventory\n- Identify patterns (repeated structures = potential components)\n- Calculate similarity scores (Levenshtein distance on structure)\n- Flag breaking changes (modified existing component mappings)\n\n**Acceptance Criteria**:\n- [ ] Parses Figma MCP metadata format (XML-like sparse structure)\n- [ ] Identifies new components not in ui_kit_inventory\n- [ ] Calculates similarity scores for potential reuse (>70% = suggest extending existing)\n- [ ] Returns structured JSON output\n- [ ] Works with manual input when MCP unavailable\n\n#### Step 1.3: Implement token_extractor.py\n\n**Purpose**: Convert Figma variables to DTCG design tokens\n\n**DTCG Format** (W3C Design Tokens Community Group spec):\n```json\n{\n  \"color\": {\n    \"primary\": {\n      \"500\": {\n        \"$value\": \"#3B82F6\",\n        \"$type\": \"color\",\n        \"$description\": \"Primary brand color - buttons, links\"\n      }\n    }\n  },\n  \"spacing\": {\n    \"md\": {\n      \"$value\": \"16px\",\n      \"$type\": \"dimension\"\n    }\n  }\n}\n```\n\n**Features**:\n- Parse Figma `get_variable_defs` response\n- Semantic naming alignment (Figma \"Primary 500\" → \"color.primary.500\")\n- Type detection (color, dimension, typography, shadow)\n- Conflict resolution (flag manual review when tokens diverge)\n- Generate diff summary (added/modified/removed)\n\n**Acceptance Criteria**:\n- [ ] Outputs valid DTCG format JSON\n- [ ] Handles all token types (color, spacing, typography, radius, shadow)\n- [ ] Generates diff vs existing design-tokens.json\n- [ ] Preserves manual customizations in existing tokens\n- [ ] Works with manual token list when MCP unavailable\n\n#### Step 1.4: Implement component_mapper.py\n\n**Purpose**: Map Figma components to codebase components\n\n**Mapping Format**:\n```json\n{\n  \"figma_node_12345\": {\n    \"figma_name\": \"Button/Primary/Large\",\n    \"code_path\": \"src/components/ui/Button.tsx\",\n    \"code_component\": \"Button\",\n    \"props_mapping\": {\n      \"variant\": \"primary\",\n      \"size\": \"lg\"\n    },\n    \"confidence\": 0.98\n  }\n}\n```\n\n**Features**:\n- Parse Figma `get_code_connect_map` response\n- Search codebase for component files (glob patterns)\n- Fuzzy matching component names (Button, ButtonComponent, Btn)\n- Extract variant mappings from component props\n- Identify unmapped components (need creation)\n\n**Acceptance Criteria**:\n- [ ] Generates component-mapping.json\n- [ ] Searches project using glob patterns (*.tsx, *.vue, *.jsx)\n- [ ] Fuzzy matches component names (90%+ confidence threshold)\n- [ ] Lists unmapped Figma components separately\n- [ ] Updates existing mappings without overwriting manual edits\n\n#### Step 1.5: Implement design_system_auditor.py\n\n**Purpose**: Compare design vs implementation, find drift\n\n**Audit Checks**:\n1. Token drift (Figma vs design-tokens.json)\n2. Component reuse opportunities (similar structures)\n3. Unused tokens (exist in code, not in Figma)\n4. Missing tokens (exist in Figma, not in code)\n5. Tailwind config alignment\n\n**Output Format**:\n```markdown\n## Token Alignment\n\n✅ **In Sync**: 87 tokens match exactly\n⚠️  **Drift Detected**: 5 tokens modified in Figma, not in code\n  - color.primary.600: #2563EB (Figma) vs #1D4ED8 (code)\n❌ **Missing in Code**: 12 new tokens from design\n🗑️  **Unused in Design**: 3 tokens exist in code, not used in Figma\n\n## Component Reuse Analysis\n\n**StatBadge**: 78% similarity to existing `Badge` component\n- Recommendation: Extend Badge with `variant=\"stat\"` prop\n- Avoids: Creating duplicate component\n```\n\n**Acceptance Criteria**:\n- [ ] Compares Figma tokens vs design-tokens.json\n- [ ] Identifies drift with specific values\n- [ ] Calculates component similarity scores\n- [ ] Parses Tailwind config for token usage\n- [ ] Generates markdown audit report\n\n#### Step 1.6: Implement implementation_planner.py\n\n**Purpose**: Generate implementation task docs with phased breakdown\n\n**Template-Driven Generation**:\n- Uses `templates/design-review-report.md` for structure\n- Injects analysis results from previous functions\n- Generates phased implementation plan\n- Creates acceptance criteria checklist\n- Estimates complexity per component\n\n**Task Doc Structure**:\n```markdown\n# TASK-XX: [Feature] Implementation\n\n## Design Review\nReference: .agent/design-system/reviews/YYYY-MM-DD-[feature].md\n\n## Implementation Phases\n\n### Phase 1: Design Tokens (estimated 2 hours)\n**Priority**: High (foundation)\n\n#### Subtasks\n1. Add new color tokens to design-tokens.json\n2. Run Style Dictionary build\n3. Update Tailwind @theme\n\n**Acceptance Criteria**:\n- [ ] All new tokens available in Tailwind utilities\n- [ ] No breaking changes to existing token references\n\n### Phase 2: [Component Name] (estimated 3 hours)\n...\n```\n\n**Acceptance Criteria**:\n- [ ] Generates valid Navigator task document\n- [ ] Phases ordered by dependency (tokens → atoms → molecules → organisms)\n- [ ] Complexity estimates included\n- [ ] Acceptance criteria for each phase\n- [ ] Migration strategy for breaking changes\n\n#### Step 1.7: Create SKILL.md Main Prompt\n\n**Structure** (following Navigator skill pattern):\n\n```markdown\n---\nauto_invoke:\n  triggers:\n    - \"Review this design\"\n    - \"Analyze Figma mockup\"\n    - \"Design handoff for {feature}\"\n    - \"Check design system impact\"\n    - \"Plan implementation for design\"\n  description: \"Automates design review, token extraction, and implementation planning\"\n---\n\n# Product Design Skill\n\n[Skill instructions...]\n```\n\n**Key Sections**:\n1. **What This Skill Does** (problem statement)\n2. **When to Auto-Invoke** (trigger scenarios)\n3. **Workflow Protocol** (5-step process)\n4. **Figma MCP Integration** (when available, how to use)\n5. **Manual Workflow** (fallback without MCP)\n6. **Design System Documentation** (loading strategy)\n7. **Predefined Functions** (what each does)\n8. **Templates** (output formats)\n9. **Token Optimization** (Navigator principles)\n10. **Troubleshooting** (common issues)\n\n**Acceptance Criteria**:\n- [ ] SKILL.md is 2-3k tokens (measured)\n- [ ] Auto-invoke triggers comprehensive\n- [ ] Clear workflow steps (1-5)\n- [ ] Examples for manual and MCP workflows\n- [ ] Troubleshooting section covers MCP token limits\n\n#### Step 1.8: Create Template Files\n\n**templates/design-review-report.md**:\n- Header with date, Figma link, reviewer\n- New design tokens section (added/modified/removed)\n- New components section (atoms/molecules/organisms)\n- Design system impact analysis\n- Implementation recommendations (phased approach)\n\n**templates/design-tokens-diff.md**:\n- Side-by-side comparison table\n- Visual indicators (✅ match, ⚠️ drift, ❌ missing, 🗑️ unused)\n- Migration notes for breaking changes\n\n**templates/ui-kit-impact.md**:\n- Component reuse opportunities\n- Similarity scores with recommendations\n- New components required\n- Breaking changes to existing components\n\n**Acceptance Criteria**:\n- [ ] All templates use consistent markdown formatting\n- [ ] Placeholder variables clearly marked ({{PLACEHOLDER}})\n- [ ] Templates match example output in documentation\n- [ ] Visual indicators (emoji) used appropriately\n\n#### Step 1.9: Create Example Design Review\n\n**examples/dashboard-redesign-review.md**:\n- Complete example matching template structure\n- Real-world scenario (dashboard with metrics)\n- Shows token extraction results\n- Shows component analysis\n- Shows implementation plan\n\n**Purpose**: Reference for users and validation of template quality\n\n**Acceptance Criteria**:\n- [ ] Example demonstrates all template sections\n- [ ] Realistic design scenario (not trivial)\n- [ ] Shows both simple and complex components\n- [ ] Includes breaking change example\n\n---\n\n### Phase 2: Figma MCP Integration\n\n**Goal**: Automate extraction using Figma Desktop MCP server\n\n#### Step 2.1: MCP Server Detection\n\n**Function**: `detect_mcp_server()`\n\n**Logic**:\n```python\ndef detect_mcp_server():\n    \"\"\"\n    Detects which Figma MCP server is available.\n\n    Returns:\n        {\n            \"type\": \"local\" | \"remote\" | \"none\",\n            \"url\": \"http://127.0.0.1:3845/mcp\" | \"https://mcp.figma.com/mcp\" | None,\n            \"tools_available\": [\"get_design_context\", \"get_variable_defs\", ...]\n        }\n    \"\"\"\n```\n\n**Detection Strategy**:\n1. Check for local server (http://127.0.0.1:3845/mcp)\n2. If unavailable, check for remote server config\n3. If none, return manual workflow mode\n\n**Acceptance Criteria**:\n- [ ] Detects local Figma Desktop MCP server\n- [ ] Falls back to remote server if configured\n- [ ] Returns \"none\" when no MCP available\n- [ ] Lists available tools per server type\n\n#### Step 2.2: Large Selection Handling\n\n**Problem**: Figma MCP returns >350k tokens for large screens, exceeds Claude Code 25k default limit\n\n**Solution**: Metadata-first approach\n\n**Workflow**:\n```markdown\n1. Use `get_metadata` first (sparse XML, low tokens)\n2. Parse metadata to identify component node IDs\n3. Fetch components individually via `get_design_context`\n4. Aggregate results\n```\n\n**Environment Variable Recommendation**:\n```bash\n# In skill documentation\nexport MAX_MCP_OUTPUT_TOKENS=100000\n```\n\n**Acceptance Criteria**:\n- [ ] Uses `get_metadata` before `get_design_context`\n- [ ] Breaks large selections into smaller chunks\n- [ ] Documents MAX_MCP_OUTPUT_TOKENS requirement\n- [ ] Handles timeout errors gracefully\n\n#### Step 2.3: Automated Token Extraction\n\n**Integration**: `token_extractor.py` + `get_variable_defs`\n\n**Workflow**:\n1. Call Figma MCP `get_variable_defs`\n2. Parse response JSON\n3. Convert to DTCG format\n4. Compare with existing design-tokens.json\n5. Generate diff summary\n\n**Acceptance Criteria**:\n- [ ] Automatically extracts all Figma variables\n- [ ] Handles all variable types (color, number, string, boolean)\n- [ ] Preserves variable descriptions from Figma\n- [ ] Generates complete diff report\n\n#### Step 2.4: Automated Component Mapping\n\n**Integration**: `component_mapper.py` + `get_code_connect_map`\n\n**Workflow**:\n1. Call Figma MCP `get_code_connect_map`\n2. Parse response with node IDs → code paths\n3. Verify code paths exist in project\n4. Update component-mapping.json\n\n**Note**: Requires Figma Enterprise for Code Connect\n\n**Acceptance Criteria**:\n- [ ] Extracts Code Connect mappings when available\n- [ ] Falls back to fuzzy matching when Code Connect unavailable\n- [ ] Validates code paths exist before mapping\n- [ ] Warns when Code Connect not configured\n\n---\n\n### Phase 3: Advanced Features\n\n**Goal**: Enterprise-grade design system workflow\n\n#### Step 3.1: Visual Regression Test Generation\n\n**Integration**: Generate Chromatic test configuration\n\n**Output**: `.storybook/test-runner.ts` config for visual regression\n\n**Acceptance Criteria**:\n- [ ] Generates Chromatic configuration\n- [ ] Creates stories for new components\n- [ ] Documents visual testing workflow\n\n#### Step 3.2: Design System Drift Alerts\n\n**Workflow**: Run auditor on every design review, flag high-priority drift\n\n**Alert Thresholds**:\n- 🚨 Critical: >10 tokens drifted, breaking changes\n- ⚠️  Warning: >5 tokens drifted, no breaking changes\n- ℹ️  Info: New tokens/components added\n\n**Acceptance Criteria**:\n- [ ] Automatic drift detection on every review\n- [ ] Priority levels assigned\n- [ ] Actionable recommendations provided\n\n#### Step 3.3: Breaking Change Migration Planner\n\n**Feature**: Generate migration guides for breaking component changes\n\n**Output**: Migration markdown with:\n- Before/after code examples\n- Automated codemod scripts (if possible)\n- Manual steps required\n- Rollout strategy\n\n**Acceptance Criteria**:\n- [ ] Detects breaking changes automatically\n- [ ] Generates migration documentation\n- [ ] Provides rollback strategy\n\n---\n\n### Phase 4: Project-Specific Generation via nav-skill-creator\n\n**Goal**: Zero-config skill generation per project\n\n#### Step 4.1: Codebase Analysis Integration\n\n**nav-skill-creator workflow**:\n```\n1. User: \"Create a skill for product design with Figma integration\"\n2. nav-skill-creator invoked\n3. Analyzes codebase:\n   - React vs Vue detection\n   - Tailwind config location\n   - Component directory structure\n   - Testing framework (Jest, Vitest)\n   - Storybook presence\n4. Generates product-design skill:\n   - Tailored to framework\n   - Pre-configured paths\n   - Template examples match project conventions\n5. Creates .agent/design-system/ structure\n6. Runs initial UI kit inventory scan\n7. Extracts current design tokens from Tailwind config\n```\n\n**Acceptance Criteria**:\n- [ ] nav-skill-creator can generate product-design skill\n- [ ] Auto-detects framework (React/Vue/Svelte)\n- [ ] Pre-configures all file paths\n- [ ] Scans existing components for initial inventory\n\n#### Step 4.2: Initial UI Kit Inventory Scan\n\n**Function**: `scan_existing_components()`\n\n**Logic**:\n```python\ndef scan_existing_components(project_root):\n    \"\"\"\n    Scans codebase for existing UI components.\n\n    Returns:\n        {\n            \"atoms\": [...],\n            \"molecules\": [...],\n            \"organisms\": [...]\n        }\n    \"\"\"\n```\n\n**Detection Strategy**:\n- Glob search: `src/components/**/*.{tsx,vue,jsx}`\n- Parse component exports\n- Categorize by directory structure (if atomic design used)\n- Extract props/variants via AST parsing\n\n**Acceptance Criteria**:\n- [ ] Finds all component files in project\n- [ ] Extracts component names and paths\n- [ ] Attempts atomic design categorization\n- [ ] Generates initial ui-kit-inventory.md\n\n---\n\n## Testing Strategy\n\n### Unit Tests\n\n**Functions to Test**:\n- `design_analyzer.py`: Component extraction, similarity matching\n- `token_extractor.py`: DTCG format conversion, diff generation\n- `component_mapper.py`: Fuzzy matching, path validation\n- `design_system_auditor.py`: Drift detection, comparison logic\n- `implementation_planner.py`: Task doc generation\n\n**Framework**: pytest\n\n**Coverage Target**: 80%+ for all functions\n\n### Integration Tests\n\n**Scenarios**:\n1. Full manual workflow (no MCP)\n2. Full automated workflow (with MCP mocked)\n3. Hybrid workflow (partial MCP data)\n4. Error handling (MCP timeout, invalid Figma data)\n\n**Test Data**:\n- Mock Figma MCP responses (JSON fixtures)\n- Sample design-tokens.json\n- Sample ui-kit-inventory.md\n\n### Manual Testing\n\n**Test Project**: Navigator plugin itself\n\n**Test Scenarios**:\n1. Review hypothetical dashboard redesign (manual)\n2. Extract tokens from mock Figma data\n3. Generate implementation plan\n4. Verify task doc follows Navigator format\n\n---\n\n## Rollout Plan\n\n### Step 1: Internal Testing (Navigator Project)\n- Test skill on Navigator plugin codebase\n- Refine based on real usage\n- Document learnings\n\n### Step 2: Documentation Update\n- Add product-design skill to README.md\n- Update plugin.json with new skill\n- Create usage guide in .agent/sops/development/\n\n### Step 3: Version Bump\n- Increment to v3.2.0 (minor version, new feature)\n- Update marketplace.json\n- Run version audit script\n\n### Step 4: Release\n- Commit all changes\n- Create git tag v3.2.0\n- Push to GitHub\n- Create GitHub release with changelog\n\n---\n\n## Success Metrics\n\n### Efficiency Gains\n\n**Before Product Design Skill**:\n- Manual design review: 2-4 hours\n- Component discovery: 1-2 hours\n- Token extraction: 1 hour\n- Implementation planning: 2-3 hours\n- **Total**: 6-10 hours\n\n**After**:\n- Automated design review: 5 minutes\n- Component discovery: Instant\n- Token extraction: Instant\n- Implementation planning: 10 minutes\n- **Total**: 15-20 minutes (developer review)\n\n**Time Savings**: ~95% reduction\n\n### Quality Metrics\n\n- **Design system drift**: Detected automatically on every review\n- **Component reuse rate**: Tracked in ui-kit-inventory\n- **Token consistency**: 100% via automated sync\n- **Implementation accuracy**: Acceptance criteria in every task\n\n---\n\n## Documentation Requirements\n\n### README.md Update\n\nAdd section:\n```markdown\n### product-design\n\nAutomates design review, token extraction, and implementation planning.\n\n**Auto-invokes when user says**:\n- \"Review this design\"\n- \"Analyze Figma mockup\"\n- \"Design handoff for [feature]\"\n- \"Check design system impact\"\n\n**Features**:\n- Figma MCP integration (auto-detects local/remote server)\n- Design token extraction (DTCG format)\n- Component mapping (Figma → code)\n- Design system drift detection\n- Implementation plan generation\n- Tailwind CSS integration\n\n**Setup** (optional - enhances automation):\n```bash\n# Install Figma Desktop MCP\nclaude mcp add --transport http figma-desktop http://127.0.0.1:3845/mcp\n```\n\n**Usage**:\n```\n\"Review the dashboard redesign from Figma\"\n# Skill analyzes design, extracts tokens, maps components,\n# generates implementation plan as Navigator task doc\n```\n```\n\n### SOP Creation\n\n**File**: `.agent/sops/development/product-design-workflow.md`\n\n**Contents**:\n- Step-by-step workflow\n- Figma MCP setup instructions\n- Troubleshooting common issues\n- Token optimization best practices\n\n---\n\n## Risks and Mitigations\n\n### Risk: Figma MCP Token Limit Exceeded\n\n**Likelihood**: High (large designs can be 350k+ tokens)\n\n**Mitigation**:\n- Metadata-first approach (use `get_metadata` before `get_design_context`)\n- Component-by-component extraction (avoid full screen selections)\n- Document `MAX_MCP_OUTPUT_TOKENS=100000` requirement\n- Clear error messaging with resolution steps\n\n### Risk: Figma MCP Not Available\n\n**Likelihood**: Medium (requires Figma Desktop running)\n\n**Mitigation**:\n- Graceful fallback to manual workflow\n- Clear messaging about manual vs automated modes\n- Manual workflow still provides value (template-driven analysis)\n\n### Risk: Component Similarity Matching False Positives\n\n**Likelihood**: Medium (algorithm may suggest wrong component)\n\n**Mitigation**:\n- Confidence threshold (>70% for recommendations)\n- Always flag for manual review\n- Show similarity score and reasoning\n- Developer has final decision\n\n### Risk: DTCG Format Changes\n\n**Likelihood**: Low (W3C spec approaching v1.0.0)\n\n**Mitigation**:\n- Monitor W3C Design Tokens spec updates\n- Version DTCG format in design-tokens.json\n- Provide migration script if spec changes\n\n---\n\n## Dependencies\n\n### Required\n- Python 3.8+ (for predefined functions)\n- Navigator v3.1+ (skill architecture)\n\n### Optional (Enhanced Features)\n- Figma Desktop app (for local MCP server)\n- Figma Enterprise plan (for Code Connect)\n- Style Dictionary (for token transformation)\n- Tailwind CSS 4.0+ (for @theme support)\n\n---\n\n## Next Steps\n\n1. ✅ Create TASK-16 implementation plan document (this file)\n2. ⏳ Create skill directory structure\n3. ⏳ Implement Phase 1 functions (manual workflow)\n4. ⏳ Create SKILL.md main prompt\n5. ⏳ Create template files\n6. ⏳ Create example design review\n7. ⏳ Register skill in plugin.json\n8. ⏳ Test manual workflow\n9. ⏳ Implement Phase 2 (Figma MCP integration)\n10. ⏳ Update documentation (README, SOPs)\n11. ⏳ Version bump and release\n\n---\n\n**Last Updated**: 2025-10-21\n**Navigator Version**: 3.1.0\n**Target Version**: 3.2.0\n",
    "github": {
      "stars": 49,
      "forks": 4,
      "owner": "alekspetrov",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/5360806?v=4",
      "lastCommit": "2025-12-09T10:48:49Z",
      "createdAt": "2025-10-10T12:35:30Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:50.593Z"
  },
  {
    "id": "template-skill",
    "source": {
      "type": "github-search",
      "repo": "bernierllc/agents-environment-config",
      "path": ".cursor/commands/skills/template-skill.md",
      "url": "https://github.com/bernierllc/agents-environment-config/blob/main/.cursor/commands/skills/template-skill.md"
    },
    "frontmatter": {
      "name": "\"Template Skill\"",
      "description": "\"Replace with description of the skill and when Claude should use it.\"",
      "tags": "[\"skill\", \"template-skill\"]"
    },
    "content": "You are a template skill skill command inside Cursor. Replace with description of the skill and when Claude should use it.\n\n{{selection}}\n\n---\n\n## Source Skill (converted)\n\n{{file:~/.claude/skills/template-skill/SKILL.md}}\n",
    "github": {
      "stars": 0,
      "forks": 1,
      "owner": "bernierllc",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/225073874?v=4",
      "lastCommit": "2025-12-23T00:09:39Z",
      "createdAt": "2025-10-31T18:08:08Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:53.424Z"
  },
  {
    "id": "typescript-build-guardian",
    "source": {
      "type": "github-search",
      "repo": "mkelam/PDFLab",
      "path": ".claude/skills/typescript-build-guardian.SKILL.md",
      "url": "https://github.com/mkelam/PDFLab/blob/main/.claude/skills/typescript-build-guardian.SKILL.md"
    },
    "frontmatter": {
      "name": "typescript-build-guardian",
      "description": "Offensive TypeScript build pipeline optimization and validation. Triggered when reviewing tsconfig.json, TypeScript compilation errors, build performance issues, strict mode migration, module resolution problems, or preparing TypeScript projects for production. Framework-aware (Next.js/Remix). Scans for build output completeness, configuration issues, type safety opportunities, and performance bottlenecks. Produces auto-scan reports with migration paths."
    },
    "content": "# TypeScript Build Guardian\n\n**Mission:** Prevent TypeScript build failures and optimize compilation pipelines through proactive scanning and evidence-based recommendations. This skill operates in **offensive mode** - finding type safety improvements and performance gains, not just catching errors.\n\n## Activation Triggers\n\n- User mentions tsconfig.json review\n- TypeScript compilation errors\n- \"Should I enable strict mode?\"\n- Build performance issues\n- Module resolution errors\n- Pre-production TypeScript validation\n- Import/export problems\n- Source map debugging issues\n\n## Framework Awareness\n\nThis skill understands framework-specific TypeScript configurations:\n\n- **Next.js**: App Router vs Pages Router, server components, `next.config.js` integration\n- **Remix**: Route modules, loader/action types, `.server` files\n- **Pure Node.js**: Backend services, no framework\n- **Express + TypeScript**: API servers, middleware typing\n\nClaude should ask which framework if unclear from context.\n\n## Scan Methodology\n\n### 1. Initial Context Gathering\n\n**Ask if not provided:**\n- \"What framework are you using?\" (Next.js/Remix/Express/other)\n- \"Show me your tsconfig.json\"\n- \"Show me your package.json\" (to identify dependencies)\n- \"What build errors are you seeing?\" (if any)\n- \"Are you using any template engines?\" (EJS, Pug, Handlebars)\n\n### 2. Critical Build Pipeline Scan\n\nExecute ALL checks in this section. Each is based on real production incidents.\n\n#### 🔴 CRITICAL: Build Output Completeness\n**Historical Failure:** TypeScript doesn't copy .ejs files, causing production template rendering failures\n\n**Scan for:**\n- [ ] Template files (.ejs, .pug, .hbs) - are they copied to output?\n- [ ] Static JSON files (configs, i18n) - are they in dist/?\n- [ ] Public assets referenced by code - are they accessible?\n- [ ] Package.json \"files\" field - does it include necessary assets?\n\n**Red flags:**\n- Only .ts/.tsx files in src/, but templates exist\n- No copy script in package.json\n- tsconfig excludes necessary files\n- outDir doesn't match what gets deployed\n\n**Optimization:**\n```json\n// package.json - Add copy script\n{\n  \"scripts\": {\n    \"build\": \"tsc && npm run copy-assets\",\n    \"copy-assets\": \"cp -r src/views dist/ && cp -r src/public dist/\"\n  }\n}\n```\n\n```javascript\n// Or use build tool (tsup, esbuild)\n// tsup.config.ts\nexport default {\n  entry: ['src/index.ts'],\n  loader: {\n    '.ejs': 'copy',\n    '.json': 'copy'\n  }\n}\n```\n\n**Framework-specific notes:**\n- **Next.js**: Uses `public/` folder automatically, but check API routes for server-side templates\n- **Remix**: Check `app/` folder structure, `.server` files need special handling\n- **Express**: Manually copy views folder (this is the historical incident)\n\n#### 🔴 CRITICAL: tsconfig.json Configuration\n**Historical Failure:** Misconfigured paths cause import errors in production\n\n**Scan for:**\n- [ ] `outDir` matches deployment expectations\n- [ ] `rootDir` correctly set (or left unset if flat structure)\n- [ ] `moduleResolution` appropriate for Node.js (\"node16\" or \"bundler\")\n- [ ] `esModuleInterop` enabled (prevents import quirks)\n- [ ] `skipLibCheck` set to true (performance, avoids type conflicts)\n- [ ] `target` matches Node.js version or browser support\n\n**Red flags:**\n- `outDir: \"build\"` but Dockerfile copies `dist/`\n- `paths` with aliases but no `baseUrl`\n- `module: \"commonjs\"` with ESM imports\n- Missing `types` for @types packages\n- `strict: true` when code has 45+ errors (too aggressive)\n\n**Optimization for Next.js:**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"lib\": [\"dom\", \"dom.iterable\", \"ES2022\"],\n    \"jsx\": \"preserve\",\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"bundler\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"incremental\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,  // Migrate gradually\n    \"paths\": {\n      \"@/*\": [\"./src/*\"]\n    }\n  },\n  \"include\": [\"next-env.d.ts\", \"**/*.ts\", \"**/*.tsx\"],\n  \"exclude\": [\"node_modules\"]\n}\n```\n\n**Optimization for Remix:**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"lib\": [\"DOM\", \"DOM.Iterable\", \"ES2022\"],\n    \"jsx\": \"react-jsx\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"Bundler\",\n    \"types\": [\"@remix-run/node\", \"vite/client\"],\n    \"isolatedModules\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,  // Migrate gradually\n    \"paths\": {\n      \"~/*\": [\"./app/*\"]\n    }\n  }\n}\n```\n\n#### 🟡 HIGH: Strict Mode Migration Path\n**Historical Failure:** Enabling strict mode all at once revealed 45 errors, blocking development\n\n**Scan for:**\n- [ ] Current strictness level\n- [ ] Number of type errors if strict enabled\n- [ ] Which strict flags are most impactful\n\n**Red flags:**\n- `strict: true` with 45+ errors (too aggressive)\n- All strict flags disabled (missing safety)\n- No plan to improve type safety\n\n**Offensive Migration Strategy:**\n\n**Phase 1: Low-hanging fruit (Enable immediately)**\n```json\n{\n  \"compilerOptions\": {\n    \"strict\": false,\n    \"noImplicitAny\": true,           // ✅ Start here\n    \"strictFunctionTypes\": true,      // ✅ Low impact\n    \"esModuleInterop\": true,          // ✅ Import safety\n    \"skipLibCheck\": true              // ✅ Performance win\n  }\n}\n```\n\n**Phase 2: Gradual tightening (1-2 weeks)**\n```json\n{\n  \"compilerOptions\": {\n    \"noImplicitAny\": true,\n    \"strictFunctionTypes\": true,\n    \"strictBindCallApply\": true,      // ➕ Add this\n    \"strictPropertyInitialization\": true  // ➕ Add this\n  }\n}\n```\n\n**Phase 3: Full strict (When ready)**\n```json\n{\n  \"compilerOptions\": {\n    \"strict\": true  // Equivalent to all flags enabled\n  }\n}\n```\n\n**Impact analysis:**\n- `noImplicitAny`: Catches ~60% of type bugs with minimal refactoring\n- `strictNullChecks`: Catches ~30% but requires significant refactoring\n- `strictPropertyInitialization`: Low value in Next.js/Remix (class components rare)\n\n**Recommendation:** Start with Phase 1, fix errors over 2 weeks, then move to Phase 2.\n\n#### 🟡 HIGH: Module Resolution Nightmares\n**Historical Pattern:** Import errors that work in dev but fail in production\n\n**Scan for:**\n- [ ] Path aliases configured correctly (`@/`, `~/`)\n- [ ] `baseUrl` set when using `paths`\n- [ ] Consistent import styles (ESM vs CommonJS)\n- [ ] Extension handling (.js vs .ts in imports)\n- [ ] Barrel exports creating circular dependencies\n\n**Red flags:**\n```typescript\n// ❌ Extension in import (breaks some bundlers)\nimport { foo } from './utils.ts'\n\n// ❌ Mixing default and named imports inconsistently\nimport express from 'express'\nimport * as express from 'express'\n\n// ❌ Path alias without baseUrl\n// tsconfig: { \"paths\": { \"@/*\": [\"src/*\"] } }\n// Missing: \"baseUrl\": \".\"\n\n// ❌ Circular dependency via barrel export\n// src/index.ts exports from src/utils.ts\n// src/utils.ts imports from src/index.ts\n```\n\n**Optimization:**\n```json\n// tsconfig.json - Proper path aliases\n{\n  \"compilerOptions\": {\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"@/*\": [\"src/*\"],\n      \"@components/*\": [\"src/components/*\"],\n      \"@utils/*\": [\"src/utils/*\"]\n    }\n  }\n}\n```\n\n```typescript\n// ✅ Consistent import style\nimport express from 'express'  // Default export\nimport { Router } from 'express'  // Named export\n\n// ✅ No extensions in imports\nimport { foo } from './utils'  // Not ./utils.ts\n\n// ✅ Avoid deep barrel exports\n// Instead of: src/components/index.ts exporting everything\n// Use direct imports: import { Button } from '@components/Button'\n```\n\n**Framework-specific:**\n- **Next.js**: `@/*` alias configured by default in `tsconfig.json`\n- **Remix**: `~/*` is convention for `app/` folder\n- **Express**: Manual `paths` configuration required\n\n#### 🟠 MEDIUM: Source Map Configuration\n**Historical Issue:** Can't debug production builds due to missing source maps\n\n**Scan for:**\n- [ ] `sourceMap: true` in tsconfig.json\n- [ ] Source maps deployed with production build\n- [ ] Source map paths resolve correctly\n- [ ] Debugging works in production\n\n**Red flags:**\n- `sourceMap: false` (can't debug)\n- Source maps in .gitignore (won't deploy)\n- `inlineSourceMap: true` in production (security risk - exposes source code)\n\n**Optimization:**\n```json\n// Development\n{\n  \"compilerOptions\": {\n    \"sourceMap\": true,\n    \"inlineSourceMap\": false\n  }\n}\n\n// Production (separate build config)\n{\n  \"compilerOptions\": {\n    \"sourceMap\": true,        // External source maps\n    \"inlineSourceMap\": false, // Never inline in production\n    \"declarationMap\": true    // For libraries\n  }\n}\n```\n\n**Security consideration:** Don't deploy source maps to public production if you want to protect IP. Upload to error tracking services (Sentry) instead.\n\n#### 🟠 MEDIUM: Build Performance\n**Historical Issue:** Slow TypeScript compilation blocking development\n\n**Scan for:**\n- [ ] Incremental compilation enabled\n- [ ] `skipLibCheck: true` (huge performance win)\n- [ ] Project references for monorepos\n- [ ] Unnecessary `include` patterns\n\n**Red flags:**\n- `skipLibCheck: false` (slow, unnecessary)\n- No `incremental: true` (rebuilds from scratch)\n- Including `node_modules` in compilation\n- No `.tsbuildinfo` in .gitignore\n\n**Optimization:**\n```json\n{\n  \"compilerOptions\": {\n    \"incremental\": true,      // ✅ 2-5x faster rebuilds\n    \"skipLibCheck\": true,     // ✅ 10-50% faster builds\n    \"tsBuildInfoFile\": \".tsbuildinfo\"\n  },\n  \"exclude\": [\n    \"node_modules\",\n    \"dist\",\n    \".next\",\n    \"build\"\n  ]\n}\n```\n\n**Advanced: Project References (Monorepos)**\n```json\n// packages/shared/tsconfig.json\n{\n  \"compilerOptions\": { \"composite\": true },\n  \"references\": []\n}\n\n// apps/api/tsconfig.json\n{\n  \"references\": [\n    { \"path\": \"../../packages/shared\" }\n  ]\n}\n```\n\n**Build time expectations:**\n- Small project (<100 files): <3s\n- Medium project (100-500 files): <10s\n- Large project (500+ files): <30s with incremental\n\nIf slower, investigate with `tsc --diagnostics`.\n\n#### 🟢 LOW: Linting & Formatting Integration\n**Not failure-critical but improves team workflow**\n\n**Scan for:**\n- [ ] ESLint configured for TypeScript\n- [ ] Prettier integrated\n- [ ] Pre-commit hooks (Husky + lint-staged)\n- [ ] IDE settings shared (.vscode/settings.json)\n\n**Optimization:**\n```json\n// .eslintrc.json\n{\n  \"parser\": \"@typescript-eslint/parser\",\n  \"plugins\": [\"@typescript-eslint\"],\n  \"extends\": [\n    \"eslint:recommended\",\n    \"plugin:@typescript-eslint/recommended\"\n  ],\n  \"rules\": {\n    \"@typescript-eslint/no-explicit-any\": \"warn\",\n    \"@typescript-eslint/no-unused-vars\": [\"error\", { \"argsIgnorePattern\": \"^_\" }]\n  }\n}\n```\n\n### 3. Cross-Reference: Docker Integration\n\n**If Docker skill is also active**, validate that TypeScript build outputs match Docker expectations:\n\n```markdown\n⚠️ CROSS-SKILL ALERT: Docker + TypeScript\n\nYour TypeScript config outputs to: `dist/`\nYour Dockerfile copies from: `dist/`  ✅ Match!\n\nBut TypeScript doesn't copy:\n- src/views/*.ejs (missing in dist/)\n- src/public/* (missing in dist/)\n\nACTION: Add copy script or update Dockerfile:\n  COPY views/ /app/views/\n  COPY public/ /app/public/\n```\n\n### 4. Production Readiness Checklist\n\nGenerate this checklist in the auto-scan report:\n\n```\nTYPESCRIPT BUILD READINESS SCORE: X/10\n\n✅ tsconfig.json properly configured\n✅ Build output includes all necessary files\n✅ Source maps enabled for debugging\n✅ Incremental compilation enabled\n⚠️  Strict mode partially enabled (migration in progress)\n⚠️  Some path aliases not in baseUrl\n❌ Missing: Copy script for .ejs templates\n❌ Critical: Import errors in 3 files\n❌ Performance: Build time >60s (should be <30s)\n\nRISK LEVEL: [LOW/MEDIUM/HIGH/CRITICAL]\nBLOCKERS: X critical issues must be resolved\nOPTIMIZATIONS: Y performance wins available\n```\n\n## Output Format: Auto-Scan Report\n\n```\n═══════════════════════════════════════════════\n🛡️ TYPESCRIPT BUILD GUARDIAN - SCAN RESULTS\n═══════════════════════════════════════════════\n\n📊 SCAN SCOPE\n• Framework: Next.js 14 (App Router)\n• TypeScript: 5.3.2\n• tsconfig: Found\n• Build time: 45s (target: <30s)\n\n🚨 CRITICAL FINDINGS: [count]\n[List each critical issue with:\n - What's wrong\n - Why it's dangerous (cite historical incident)\n - How to fix (code example)]\n\n⚠️  HIGH PRIORITY: [count]\n[Same format as critical]\n\n💡 OPTIMIZATIONS: [count]\n[Performance improvements, type safety upgrades]\n\n🎯 STRICT MODE MIGRATION PATH:\nCurrent: Phase 0 (no strict flags)\nRecommendation: Enable Phase 1 flags (noImplicitAny, strictFunctionTypes)\nEstimated errors: ~12 (fixable in 2-3 hours)\nFull strict timeline: 2-3 weeks\n\n⚡ PERFORMANCE ANALYSIS:\nCurrent build time: 45s\nWith skipLibCheck: ~30s (33% faster)\nWith incremental: ~20s (56% faster)\nTarget achieved: 20s < 30s ✅\n\n═══════════════════════════════════════════════\nFINAL VERDICT\n═══════════════════════════════════════════════\nProduction Ready: [YES/NO/BLOCKED]\nRisk Level: [LOW/MEDIUM/HIGH/CRITICAL]\nEstimated Fix Time: [X hours]\n\nNEXT ACTIONS:\n1. [Most critical fix]\n2. [Second priority]\n3. [Optional optimization]\n\n═══════════════════════════════════════════════\n```\n\n## Reference Materials\n\nFor detailed error patterns and historical incidents, see:\n- `references/error-patterns.md` - TypeScript incident database with resolutions\n- `references/strict-mode-migration.md` - Comprehensive strict mode migration guide\n\n## Advanced Scanning\n\n**When to escalate:**\n- User says \"comprehensive TypeScript audit\"\n- Build time >60s\n- 20+ type errors\n- Monorepo with multiple tsconfig files\n- Complex module resolution setup\n\n**Escalation actions:**\n- Run `tsc --noEmit` to get full error list\n- Use `tsc --diagnostics` for performance analysis\n- Check all tsconfig files in monorepo\n- Analyze import graph for circular dependencies\n- Review webpack/vite config for TypeScript loaders\n\n## Framework-Specific Guidance\n\n### Next.js\n\n**App Router (Next.js 13+):**\n- Server Components are TypeScript-first\n- Use `'use client'` directive correctly\n- Async Server Components need proper typing\n- Route handlers use `NextRequest`/`NextResponse`\n\n**Common issues:**\n```typescript\n// ❌ Missing 'use client'\n'use client'  // Must be first line\nimport { useState } from 'react'\n\n// ❌ Server Component with client-only APIs\n// app/page.tsx (Server Component by default)\nexport default function Page() {\n  const [state, setState] = useState(0)  // Error!\n  // Fix: Add 'use client' or move to client component\n}\n\n// ✅ Proper route handler typing\nimport { NextRequest, NextResponse } from 'next/server'\n\nexport async function GET(request: NextRequest) {\n  return NextResponse.json({ data: 'hello' })\n}\n```\n\n### Remix\n\n**Route modules:**\n- Loader/action return types must be serializable\n- `.server` files don't bundle to client\n- `useLoaderData<typeof loader>()` for type inference\n\n**Common issues:**\n```typescript\n// ❌ Non-serializable loader return\nexport const loader = async () => {\n  return { date: new Date() }  // Error in production!\n}\n\n// ✅ Serialize dates\nexport const loader = async () => {\n  return json({ date: new Date().toISOString() })\n}\n\n// ✅ Type-safe loader data\nexport const loader = async () => {\n  return json({ user: { name: \"John\" } })\n}\n\nexport default function Route() {\n  const data = useLoaderData<typeof loader>()\n  data.user.name  // ✅ Typed correctly\n}\n```\n\n## Key Principles\n\n1. **Offensive mindset:** Don't just fix errors, optimize the build pipeline\n2. **Evidence-based:** Every check maps to a real historical incident\n3. **Gradual migration:** Strict mode in phases, not all at once\n4. **Framework-aware:** Next.js ≠ Remix ≠ Express\n5. **Performance-conscious:** Fast builds = happy developers\n6. **Cross-skill integration:** Validate Docker + TypeScript coherence\n\n## Quick Reference: Common Fixes\n\n```bash\n# Copy non-TS files to dist\nnpm install --save-dev cpx\n# package.json\n\"scripts\": {\n  \"copy\": \"cpx 'src/**/*.{ejs,json}' dist\"\n}\n\n# Fix import resolution errors\n# tsconfig.json - add baseUrl + paths\n\n# Speed up builds\n# tsconfig.json - add skipLibCheck: true, incremental: true\n\n# Gradual strict mode\n# tsconfig.json - enable flags one by one\n\n# Check for errors without emitting\ntsc --noEmit\n\n# Analyze build performance\ntsc --diagnostics\n```\n",
    "github": {
      "stars": 1,
      "forks": 0,
      "owner": "mkelam",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/130832145?v=4",
      "lastCommit": "2025-12-21T18:09:06Z",
      "createdAt": "2025-10-29T11:23:14Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:54.230Z"
  },
  {
    "id": "interactive-template-generator",
    "source": {
      "type": "github-search",
      "repo": "Dexploarer/claudius-skills",
      "path": "templates/tools/interactive-template-skill.md",
      "url": "https://github.com/Dexploarer/claudius-skills/blob/main/templates/tools/interactive-template-skill.md"
    },
    "frontmatter": {
      "name": "interactive-template-generator",
      "description": "Interactively guide users through selecting and customizing Claude Code templates when they ask to \"generate a template\" or \"create a new skill/command/agent\"",
      "allowed-tools": "[Read, Write, Edit, Grep, Glob]"
    },
    "content": "# Interactive Template Generator Skill\n\nGuide users through an interactive process to select, customize, and generate Claude Code configurations.\n\n## Activation Triggers\n\nThis skill activates when users say:\n- \"generate a template\"\n- \"create a new skill\"\n- \"create a new command\"\n- \"create a new subagent\"\n- \"help me set up a Claude Code configuration\"\n- \"what template should I use?\"\n\n## Instructions\n\nWhen this skill is activated, follow this interactive process:\n\n### Step 1: Identify What They Need\n\nAsk the user what they want to create:\n\n```\nI can help you create:\n\n1. **Skill** - Auto-triggered functionality based on user requests\n2. **Command** - Explicit slash commands (e.g., /deploy)\n3. **Subagent** - Specialized background workers\n4. **Hook** - Event-driven automation\n5. **MCP Configuration** - External service integration\n6. **Complete Setup** - Full project configuration\n7. **Language Service** - Go/Rust/PHP service generator\n8. **GitHub Workflow** - CI/CD automation\n\nWhat would you like to create?\n```\n\n### Step 2: Gather Requirements Based on Type\n\n#### For Skills:\n\nAsk these questions:\n1. **Level**: \"What complexity level?\"\n   - Beginner (simple, focused tasks like formatters)\n   - Intermediate (framework-specific like React generators)\n   - Advanced (full-stack features)\n\n2. **Name**: \"What should the skill be called? (use kebab-case)\"\n\n3. **Description**: \"When should this activate? Be specific about the trigger phrases.\"\n\n4. **Tools**: \"Which tools does it need?\"\n   - Read-only: Read, Grep, Glob\n   - Code generation: Read, Write, Edit, Grep, Glob\n   - Full access: Read, Write, Edit, Bash, Grep, Glob, Task\n\n5. **Purpose**: \"What will this skill do?\"\n\nThen:\n- Copy the appropriate template (beginner/intermediate/advanced)\n- Customize the frontmatter with name, description, tools\n- Save to `.claude/skills/{name}.md`\n- Provide next steps for customization\n\n#### For Commands:\n\nAsk these questions:\n1. **Type**: \"What kind of command?\"\n   - Basic (single operation, quick execution)\n   - Workflow (multi-step, with rollback/validation)\n\n2. **Name**: \"What should the command be called? (without /)\"\n\n3. **Description**: \"What does this command do?\"\n\n4. **Options**: \"Does it need options? (e.g., --dry-run, --env, --force)\"\n\nThen:\n- Copy the appropriate template\n- Customize with name and description\n- Save to `.claude/commands/{name}.md`\n- Provide usage example: `/{name}`\n\n#### For Subagents:\n\nAsk these questions:\n1. **Role**: \"What role should the subagent have?\"\n   - Analyzer (read-only, review/audit)\n   - Generator (create files and code)\n   - Domain Expert (specialized knowledge)\n\n2. **Name**: \"What should the subagent be called?\"\n\n3. **Expertise**: \"What domain expertise should it have?\"\n\n4. **Tools**: Based on role, suggest appropriate tools\n\nThen:\n- Copy the appropriate template\n- Customize with name, description, expertise\n- Save to `.claude/agents/{name}.md`\n- Provide usage example: `\"use the {name} subagent to...\"`\n\n#### For Language Services:\n\nAsk these questions:\n1. **Language**: \"Which language?\"\n   - Go\n   - Rust\n   - PHP (Laravel)\n\n2. **Service name**: \"What should the service be called?\"\n\n3. **Resource**: \"What entity will it manage?\" (e.g., users, products, orders)\n\n4. **Database**: \"Which database?\" (PostgreSQL, MySQL, SQLite)\n\nThen:\n- Read the appropriate language template\n- Explain what will be generated\n- Provide the exact phrase to use for generation\n- Show project structure that will be created\n\n#### For GitHub Workflows:\n\nAsk these questions:\n1. **Purpose**: \"What should the workflow do?\"\n   - CI (testing, linting)\n   - CD (deployment)\n   - Security scanning\n   - Release automation\n   - Performance testing\n\n2. **Technology**: \"What's your tech stack?\"\n   - Node.js\n   - Python\n   - Go\n   - Rust\n   - PHP\n\n3. **Triggers**: \"When should it run?\"\n   - On push to specific branches\n   - On pull requests\n   - On schedule\n   - Manual trigger\n\nThen:\n- Read the GitHub workflow template\n- Show relevant workflow examples\n- Explain required secrets\n- Provide customization points\n\n### Step 3: Generate the Configuration\n\nAfter gathering requirements:\n\n1. **Copy the template** from the appropriate location:\n   - Skills: `templates/skills/{level}-skill-template.md`\n   - Commands: `templates/commands/{type}-command-template.md`\n   - Subagents: `templates/subagents/{role}-template.md`\n\n2. **Customize the frontmatter**:\n   ```yaml\n   ---\n   name: {user-provided-name}\n   description: {user-provided-description}\n   allowed-tools: [{user-selected-tools}]\n   ---\n   ```\n\n3. **Save to the correct location**:\n   - Skills: `.claude/skills/{name}.md`\n   - Commands: `.claude/commands/{name}.md`\n   - Subagents: `.claude/agents/{name}.md`\n\n4. **Create directories if needed**: Use Write tool to create parent directories\n\n### Step 4: Provide Next Steps\n\nAfter generation, tell the user:\n\n```\n✅ Created {type}: .claude/{type}s/{name}.md\n\n📝 Next steps:\n1. Review the generated file\n2. Customize the instructions for your specific use case\n3. Add examples relevant to your project\n4. Test the {type}:\n   {usage-example}\n\n💡 Tips:\n- Read through the template comments for guidance\n- Start simple and add complexity as needed\n- Test with different phrasings to ensure it activates correctly\n\nNeed help customizing? Just ask!\n```\n\n### Step 5: Offer Customization Help\n\nIf the user asks for help customizing:\n\n1. **Read the generated file** to see what was created\n\n2. **Ask specific questions**:\n   - \"What specific functionality do you want to add?\"\n   - \"Do you have an example of what you want it to generate?\"\n   - \"Are there any specific frameworks or libraries to integrate?\"\n\n3. **Provide concrete examples** based on their responses\n\n4. **Edit the file** to add the customizations\n\n## Examples\n\n### Example 1: Creating a Beginner Skill\n\n```\nUser: \"I want to create a skill for formatting JSON files\"",
    "github": {
      "stars": 3,
      "forks": 2,
      "owner": "Dexploarer",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/211557447?v=4",
      "lastCommit": "2025-11-23T08:36:41Z",
      "createdAt": "2025-10-31T20:15:38Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:54.826Z"
  },
  {
    "id": "tutorials",
    "source": {
      "type": "github-search",
      "repo": "Dexploarer/claudius-skills",
      "path": "examples/beginner/tutorials/your-first-skill.md",
      "url": "https://github.com/Dexploarer/claudius-skills/blob/main/examples/beginner/tutorials/your-first-skill.md"
    },
    "frontmatter": {},
    "content": "# Tutorial: Creating Your First Skill\n\nLearn how to create a simple, useful skill from scratch.\n\n## What You'll Build\n\nA skill that automatically adds TODO comments to your code when you describe what needs to be done.\n\n## Prerequisites\n\n- Claude Code installed\n- Basic understanding of markdown\n- A test project to try it in\n\n## Step 1: Create the Skill File\n\nCreate a new file in your project:\n\n```bash\nmkdir -p .claude/skills\ntouch .claude/skills/todo-adder.md\n```\n\n## Step 2: Add the YAML Frontmatter\n\nOpen `todo-adder.md` and add:\n\n```yaml\n---\nname: todo-adder\ndescription: Adds TODO comments to code when user describes what needs to be done later\nallowed-tools: [Edit, Read, Write]\n---\n```\n\n**What this does:**\n- `name`: Unique identifier for your skill\n- `description`: Tells Claude when to use this skill (be specific!)\n- `allowed-tools`: Limits what the skill can do (optional)\n\n## Step 3: Write the Instructions\n\nAfter the frontmatter, add:\n\n```markdown\n# TODO Comment Adder\n\nHelps users quickly add TODO comments to their code.\n\n## When to Activate\n\n- \"add a TODO to remind me to...\"\n- \"leave a TODO comment for...\"\n- \"mark this for later...\"\n- \"remind me to come back and...\"\n\n## Process\n\n1. **Identify the location**:\n   - If user mentions specific file, use that\n   - If in context of code discussion, use current file\n   - Ask if unclear\n\n2. **Format the TODO**:\n   - Use language-appropriate comment syntax\n   - Include date (optional)\n   - Include description from user\n   - Add context if needed\n\n3. **Insert the TODO**:\n   - Place at the logical location\n   - Use proper indentation\n   - Don't break existing code\n\n## TODO Format Examples\n\n**JavaScript/TypeScript:**\n```javascript\n// TODO: Add error handling for API failures\n```\n\n**Python:**\n```python\n# TODO: Optimize this query - currently N+1\n```\n\n**HTML:**\n```html\n<!-- TODO: Add accessibility attributes -->\n```\n\n## Best Practices\n\n- Keep TODO comments concise but clear\n- Include WHY something is needed\n- Add priority if mentioned (TODO(P1):)\n- Suggest creating issue for complex TODOs\n```\n\n## Step 4: Test Your Skill\n\n1. Save the file\n2. Start Claude Code in your project\n3. Try it out:\n\n```\n\"add a TODO to remind me to add error handling in app.js\"\n```\n\nClaude should:\n1. Recognize your skill should activate\n2. Read app.js\n3. Add a properly formatted TODO comment\n4. Show you the change\n\n## Step 5: Refine and Improve\n\nBased on testing, you might:\n\n- Adjust the description for better matching\n- Add more trigger phrases\n- Improve the formatting rules\n- Add support for more languages\n\n## Tips for Success\n\n1. **Be specific in the description** - \"Adds TODO comments\" is better than \"helps with TODOs\"\n2. **Test with variations** - Try different phrasings\n3. **Start simple** - Don't overcomplicate the first version\n4. **Check the tool restrictions** - Only include tools you actually need\n\n## Common Mistakes\n\n❌ **Too broad description:** \"Helps with code organization\"\n✅ **Specific description:** \"Adds TODO comments to code when user describes tasks for later\"\n\n❌ **Missing trigger phrases:** Just instructions, no examples\n✅ **Clear triggers:** List specific phrases that should activate it\n\n❌ **Complex logic:** Trying to do too much\n✅ **Focused purpose:** One skill, one job\n\n## Next Steps\n\nNow that you've created your first skill:\n\n1. Create more skills for your workflow\n2. Try adding [slash commands](./your-first-command.md)\n3. Explore [subagents](./creating-a-subagent.md)\n4. Share your skill with the community!\n\n## Full Example\n\nHere's the complete `todo-adder.md`:\n\n```markdown\n---\nname: todo-adder\ndescription: Adds TODO comments to code when user describes what needs to be done later\nallowed-tools: [Edit, Read, Write]\n---\n\n# TODO Comment Adder\n\nHelps users quickly add TODO comments to their code.\n\n## When to Activate\n- \"add a TODO to remind me to...\"\n- \"leave a TODO comment for...\"\n- \"mark this for later...\"\n\n## Process\n1. Identify the location (file and line)\n2. Format TODO with proper comment syntax\n3. Insert at appropriate location\n4. Confirm with user\n\n## Examples\n\nJavaScript:\n```javascript\n// TODO: Add validation\n```\n\nPython:\n```python\n# TODO: Optimize performance\n```\n```\n\n## Resources\n\n- [Skill Template](../../../templates/skill-template.md) - Blank template\n- [More Examples](../simple-skills/) - See other simple skills\n- [Claude Code Docs](https://docs.claude.com/claude-code/skills) - Official docs\n\nHappy skill creating! 🎉\n",
    "github": {
      "stars": 3,
      "forks": 2,
      "owner": "Dexploarer",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/211557447?v=4",
      "lastCommit": "2025-11-23T08:36:41Z",
      "createdAt": "2025-10-31T20:15:38Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:02:55.498Z"
  },
  {
    "id": "safe-file-reader",
    "source": {
      "type": "github-search",
      "repo": "Xe/site",
      "path": ".claude/skills/safe-file-reader/SKILL.md",
      "url": "https://github.com/Xe/site/blob/main/.claude/skills/safe-file-reader/SKILL.md"
    },
    "frontmatter": {
      "name": "safe-file-reader",
      "description": "Read files without making changes. Use when you need read-only file access.",
      "allowed-tools": "Read, Grep, Glob"
    },
    "content": "# Safe File Reader\n\nThis Skill provides read-only file access.\n\n## Instructions\n\n1. Use Read to view file contents\n2. Use Grep to search within files\n3. Use Glob to find files by pattern\n",
    "github": {
      "stars": 708,
      "forks": 118,
      "owner": "Xe",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/529003?v=4",
      "license": "Zlib",
      "lastCommit": "2026-01-02T23:59:46Z",
      "createdAt": "2016-12-14T07:12:19Z",
      "topics": [
        "deno",
        "go",
        "hacktoberfest",
        "lume",
        "nix",
        "personal-website",
        "self-driving-car",
        "web"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:58.692Z"
  },
  {
    "id": "c-code-formatter",
    "source": {
      "type": "github-search",
      "repo": "calcitem/Sanmill",
      "path": ".claude/skills/cpp-formatter/SKILL.md",
      "url": "https://github.com/calcitem/Sanmill/blob/main/.claude/skills/cpp-formatter/SKILL.md"
    },
    "frontmatter": {
      "name": "\"C++ Code Formatter\"",
      "description": "\"Format C++ code in Sanmill project to ensure consistent code style; use when formatting C++ code or checking code style compliance.\""
    },
    "content": "# C++ Code Formatter\n\n## Purpose\n\nThis skill helps format Sanmill's C++ code to ensure code style consistency and maintainability across the codebase.\n\n## Use Cases\n\n- Format C++ code after modifications\n- Check code style compliance\n- Format code before committing\n- Batch format project C++ files\n- Validate code style in CI/CD pipelines\n\n## Quick Commands\n\n### Using Project Script (Recommended)\n\n```bash\n# Format all C++ and Dart code (will auto-commit)\n./format.sh\n\n# Format without auto-commit\n./format.sh s\n```\n\nThe script formats:\n- All `.h` and `.cpp` files in `src/`, `include/`, `tests/`\n- All Dart files in the project\n- Uses project's `.clang-format` configuration\n\n### Manual Formatting\n\n```bash\n# Format single file\nclang-format -i src/position.cpp\n\n# Format multiple files\nclang-format -i src/*.cpp src/*.h\n\n# Check without modifying (dry-run)\nclang-format --dry-run --Werror src/position.cpp\n```\n\n## Configuration\n\n### Project Configuration Files\n- **`.clang-format`** - C++ formatting rules (project root)\n- **`CPPLINT.cfg`** - Code style checking rules\n- **`.editorconfig`** - Editor-specific settings\n\n### View Current Configuration\n```bash\ncat .clang-format\n```\n\n## Code Style Checking\n\n```bash\n# Check specific file with cpplint\ncpplint --config=CPPLINT.cfg src/position.cpp\n\n# The configuration file defines which checks to enable/disable\n```\n\n## Git Integration\n\n### Pre-commit Workflow\n```bash\n# 1. Make code changes\n# 2. Format code\n./format.sh s\n\n# 3. Review changes\ngit diff\n\n# 4. If correct, commit\ngit add .\ngit commit -m \"Your commit message\"\n```\n\n### Format Only Staged Files\n```bash\ngit diff --cached --name-only --diff-filter=ACM | \\\ngrep -E '\\.(cpp|h|cc|hpp)$' | \\\nxargs clang-format -i\n```\n\n## Common Issues & Solutions\n\n### 1. Format Breaks Code Structure\n- **Check**: Verify `.clang-format` configuration\n- **Check**: Ensure clang-format version matches team standard\n- **Workaround**: Use `// clang-format off` and `// clang-format on` for special blocks\n\n### 2. Batch Formatting Creates Large Changes\n- **Solution**: Format in batches and commit separately\n- **Label**: Use clear commit message like \"style: Format C++ code\"\n- **Communicate**: Notify team members to sync\n\n### 3. Format Conflicts Between Developers\n- **Ensure**: All use same `.clang-format` file\n- **Ensure**: All use same clang-format version\n- **Establish**: Team formatting conventions\n\n## Best Practices\n\n1. **Format frequently**: Format after each significant change\n2. **Format before commits**: Always format before committing\n3. **Review formatting changes**: Don't blindly commit formatting\n4. **Use project script**: Prefer `./format.sh` over manual commands\n5. **Separate formatting commits**: Keep formatting separate from logic changes\n6. **Don't hand-edit formatting**: Let tools do the work\n\n## Tools Required\n\n### clang-format\n```bash\n# Check if installed\nclang-format --version\n\n# Install on Ubuntu/Debian\nsudo apt-get install clang-format\n\n# Install on macOS\nbrew install clang-format\n```\n\n### cpplint (Optional)\nFor additional style checking beyond formatting.\n\n## Output Format\n\nFormatting operations should report:\n- ✓ Files formatted successfully\n- ⚠ Files with style violations\n- ✗ Files that failed to format\n- 📊 Total files processed\n- 💡 Style improvement recommendations\n\n## Reference Resources\n\n- **Configuration**: `.clang-format`, `CPPLINT.cfg`, `.editorconfig` (project root)\n- **Format script**: `format.sh` (project root)\n- **clang-format docs**: https://clang.llvm.org/docs/ClangFormat.html\n- **C++ source locations**: `src/`, `include/`, `tests/`\n",
    "github": {
      "stars": 187,
      "forks": 27,
      "owner": "calcitem",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/51521682?v=4",
      "license": "GPL-3.0",
      "lastCommit": "2025-12-30T04:47:54Z",
      "createdAt": "2019-06-18T15:58:23Z",
      "topics": [
        "ai",
        "alpha-beta-pruning",
        "bitboard",
        "board-game",
        "brettspiel",
        "ea15e84462",
        "flutter",
        "game",
        "malom",
        "merrills",
        "mill",
        "mills",
        "minimax",
        "morris",
        "mtdf",
        "muehle",
        "muhle",
        "nine-mens-morris",
        "pvs",
        "stockfish"
      ]
    },
    "scrapedAt": "2026-01-04T13:02:59.542Z"
  },
  {
    "id": "hosting",
    "source": {
      "type": "github-search",
      "repo": "ChrisTowles/blog",
      "path": ".claude/skills/hosting/skill.md",
      "url": "https://github.com/ChrisTowles/blog/blob/main/.claude/skills/hosting/skill.md"
    },
    "frontmatter": {},
    "content": "# Hosting Skill - GCP Cloud Run Deployment\n\nThis skill helps deploy and manage the blog on Google Cloud Platform Cloud Run.\n\n## Commands\n\n### Deploy to GCP Cloud Run\n\nTo deploy the blog to GCP:\n\n1. Build and push Docker image to Artifact Registry:\n```bash\ncd packages/blog\ngcloud builds submit \\\n  --tag us-central1-docker.pkg.dev/blog-chris-towles/blog-images/blog:latest \\\n  --project=blog-chris-towles\n```\n\n2. Deploy to Cloud Run:\n```bash\ngcloud run deploy blog \\\n  --image us-central1-docker.pkg.dev/blog-chris-towles/blog-images/blog:latest \\\n  --platform managed \\\n  --region us-central1 \\\n  --add-cloudsql-instances blog-chris-towles:us-central1:blog-db \\\n  --allow-unauthenticated \\\n  --project=blog-chris-towles\n```\n\n### View Logs\n\nView application logs from Cloud Run:\n```bash\ngcloud run logs read blog --region=us-central1 --project=blog-chris-towles\n```\n\nFollow logs in real-time:\n```bash\ngcloud run logs tail blog --region=us-central1 --project=blog-chris-towles\n```\n\n### Check Status\n\nCheck service status and configuration:\n```bash\ngcloud run services describe blog --region=us-central1 --project=blog-chris-towles\n```\n\nList all Cloud Run services:\n```bash\ngcloud run services list --project=blog-chris-towles\n```\n\nGet service URL:\n```bash\ngcloud run services describe blog --region=us-central1 --project=blog-chris-towles --format='value(status.url)'\n```\n\n### Manage Environment Variables\n\nSet environment variables:\n```bash\ngcloud run services update blog \\\n  --set-env-vars=\"KEY1=value1,KEY2=value2\" \\\n  --region=us-central1 \\\n  --project=blog-chris-towles\n```\n\nView current environment variables:\n```bash\ngcloud run services describe blog --region=us-central1 --project=blog-chris-towles --format='value(spec.template.spec.containers[0].env)'\n```\n\n### Rollback\n\nList revisions:\n```bash\ngcloud run revisions list --service=blog --region=us-central1 --project=blog-chris-towles\n```\n\nRollback to a previous revision:\n```bash\ngcloud run services update-traffic blog \\\n  --to-revisions=REVISION_NAME=100 \\\n  --region=us-central1 \\\n  --project=blog-chris-towles\n```\n\n## Project Configuration\n\n- **Project ID**: blog-chris-towles\n- **Region**: us-central1\n- **Service Name**: blog\n- **Image Registry**: us-central1-docker.pkg.dev/blog-chris-towles/blog-images\n- **Database**: Cloud SQL PostgreSQL (blog-db)\n\n## Usage Notes\n\n- Cloud Run auto-scales from 0 to configured max instances\n- Cold starts occur when scaling from 0\n- Secrets should be managed via Secret Manager, not environment variables\n- Database connections use Cloud SQL Proxy automatically when configured\n",
    "github": {
      "stars": 4,
      "forks": 0,
      "owner": "ChrisTowles",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/3102683?v=4",
      "lastCommit": "2026-01-04T07:20:05Z",
      "createdAt": "2024-06-09T23:07:19Z",
      "topics": [
        "blog",
        "nuxt",
        "typescript"
      ]
    },
    "scrapedAt": "2026-01-04T13:03:00.177Z"
  },
  {
    "id": "slipstream",
    "source": {
      "type": "github-search",
      "repo": "ClutchEngineering/sidecar.clutch.engineering",
      "path": ".claude/skills/slipstream/Skill.md",
      "url": "https://github.com/ClutchEngineering/sidecar.clutch.engineering/blob/main/.claude/skills/slipstream/Skill.md"
    },
    "frontmatter": {
      "name": "slipstream",
      "description": "Expert assistance for Slipstream static site generation. Use when building static websites with Slipstream, a SwiftUI-like static site generator."
    },
    "content": "# Slipstream Expert\n\nYou are an expert in the Slipstream static site generation framework. Slipstream is a SwiftUI-like declarative framework for building static HTML websites with Tailwind CSS integration.\n\n## Core Principles\n\n1. **SwiftUI-like Syntax**: Slipstream uses result builders and the `View` protocol to create HTML in a declarative, hierarchical manner\n2. **Tailwind CSS Integration**: All utility modifiers map to Tailwind CSS classes\n3. **Static Site Generation**: Designed for building static HTML that can be deployed to platforms like GitHub Pages\n4. **Type-Safe HTML**: Provides Swift types for W3C HTML elements with compile-time safety\n\n## Architecture\n\n### View Protocol\n\nThe `View` protocol is the foundation of Slipstream:\n\n```swift\npublic protocol View {\n  associatedtype Content: View\n  @ViewBuilder var body: Self.Content { get }\n  func render(_ container: Element) throws\n}\n```\n\n- Implement `body` for custom views\n- Use `@ViewBuilder` to enable declarative syntax\n- Only implement `render(_:environment:)` when creating new HTML element types\n\n### Result Builders\n\nSlipstream uses Swift result builders (`@ViewBuilder`) to enable hierarchical view construction. This allows:\n- Multiple children in a single block\n- Conditional content with `if/else`\n- Loops with `ForEach`\n- Optional content\n\n## Common View Types\n\n### Layout Views\n\n- `VStack(alignment:spacing:)` - Vertical stack (maps to flex column)\n- `HStack(alignment:spacing:)` - Horizontal stack (maps to flex row)\n- `Container` - Tailwind CSS container for centered, max-width content\n- `ResponsiveStack` - Adapts between vertical/horizontal based on breakpoints\n\n### Text and Typography\n\n- `Text(\"content\")` - Basic paragraph text (renders as `<p>`)\n- `H1`, `H2`, `H3`, `H4`, `H5`, `H6` - Heading elements\n- `Paragraph` - Explicit paragraph wrapper\n- `Bold`, `Strong`, `Italic`, `Emphasis` - Text styling\n- `Code`, `Variable`, `SampleOutput` - Code-related elements\n- `MarkdownText` - Render Markdown as HTML\n\n### Forms\n\n- `Form` - Form container\n- `TextField(name:type:placeholder:)` - Text input\n- `TextArea(name:placeholder:)` - Multi-line text\n- `Button`, `SubmitButton`, `ResetButton` - Buttons\n- `Checkbox`, `RadioButton` - Selection inputs\n- `Picker` with `Option` and `OptGroup` - Dropdown selects\n- `Slider`, `ColorPicker`, `FileInput` - Specialized inputs\n- `Label`, `Fieldset`, `Legend` - Form organization\n\n### Media\n\n- `Image(URL)` - Images\n- `Picture` with `Source` - Responsive images\n- `Audio`, `Video` with `Source` and `Track` - Media playback\n- `Canvas` - Canvas element\n- `IFrame` - Embedded content\n\n### Semantic HTML\n\n- `Header`, `Footer`, `Navigation` - Page structure\n- `Article`, `Section`, `Aside` - Content organization\n- `DocumentMain` - Main content area\n- `Figure`, `FigureCaption` - Figures with captions\n- `Blockquote` - Quotations\n- `List`, `ListItem` - Lists\n- `DescriptionList`, `DescriptionTerm`, `DefinitionDescription` - Definition lists\n- `Table`, `TableHeader`, `TableBody`, `TableFooter`, `TableRow`, `TableCell`, `TableHeaderCell` - Tables\n- `Details`, `Summary` - Collapsible content\n- `Divider` - Horizontal rule\n- `LineBreak` - Line break\n\n### SVG and MathML\n\n- `SVG`, `SVGCircle`, `SVGRect`, `SVGPath`, `SVGDefs`, `SVGLinearGradient`, `SVGStop` - SVG graphics\n- `Math`, `MRow`, `MI`, `MO`, `MN`, `MFrac`, `MSqrt`, `MSup`, `MSub` - Mathematical notation\n\n### Utilities\n\n- `Comment(\"text\")` - HTML comments\n- `DOMString(\"raw html\")` - Raw HTML injection\n- `ForEach(collection, id:)` - Iterate over collections\n\n## Tailwind CSS Modifiers\n\n### Typography\n\n```swift\n.fontSize(.extraLarge)\n.fontWeight(.bold)\n.bold()\n.fontStyle(.italic)\n.italic()\n.fontDesign(.monospaced)  // .serif, .sans\n.fontLeading(.loose)  // line height\n.textAlignment(.center)\n.textColor(.blue, shade: .x500)\n.textDecoration(.underline)  // .lineThrough\n.listStyle(.disc)  // .decimal, .none\n```\n\n### Spacing\n\n```swift\n.padding()  // all edges\n.padding(.horizontal, 16)\n.padding(.vertical, 8)\n.padding(.top, 4)\n.margin(8)\n.margin(.bottom, 16)\n```\n\n### Sizing\n\n```swift\n.frame(width: 48, height: 48)\n.frame(minWidth: 48, maxWidth: 96)\n.frame(minHeight: 100)\n```\n\n### Colors and Backgrounds\n\n```swift\n.textColor(.red, shade: .x500)\n.backgroundColor(.blue, shade: .x100)\n.background(.ultraThin)  // material background\n```\n\n### Borders and Effects\n\n```swift\n.border(1, .black)\n.cornerRadius(.large)\n.shadow(.large)\n.outline(.solid, width: 2, color: .red)\n.ring(width: 2, color: .blue)\n```\n\n### Layout\n\n```swift\n.display(.block)  // .flex, .grid, .none\n.position(.absolute, edges: .top, 0)\n.placement(top: 10, left: 10, zIndex: 10)\n.zIndex(100)\n.overflow(.scroll)  // .hidden, .auto\n.float(.left)\n.visibility(.hidden)\n```\n\n### Flexbox and Grid\n\n```swift\n.flexDirection(.row)  // .column\n.justifyContent(.spaceBetween)  // .center, .start, .end\n.alignItems(.center)  // .start, .end, .baseline\n.flexGap(16)\n.gridCellColumns(2)\n.gridCellRows(3)\n```\n\n### Transforms and Transitions\n\n```swift\n.offset(x: 16, y: 32)\n.opacity(0.5)\n.colorInvert()\n.animation(.easeIn(duration: 0.25))\n.transition(.all)\n```\n\n### Responsive and State\n\n```swift\n.fontSize(.base)\n.fontSize(.large, condition: .desktop)\n.textColor(.blue)\n.textColor(.red, condition: .hover)\n```\n\nAvailable conditions:\n- Breakpoints: `.mobile`, `.tablet`, `.desktop`, `.widescreen`\n- States: `.hover`, `.focus`, `.active`, `.visited`, `.disabled`\n\n## Environment System\n\nSlipstream provides SwiftUI-like environment for passing data down the view hierarchy:\n\n### Define Custom Environment Key\n\n```swift\nstruct PathKey: EnvironmentKey {\n  static var defaultValue: String = \"/\"\n}\n\nextension EnvironmentValues {\n  var path: String {\n    get { self[PathKey.self] }\n    set { self[PathKey.self] = newValue }\n  }\n}\n```\n\n### Use Environment\n\n```swift\nstruct MyView: View {\n  @Environment(\\.path) var path\n\n  var body: some View {\n    Text(\"Current path: \\(path)\")\n  }\n}\n\n// Inject environment value\nMyView()\n  .environment(\\.path, \"/home\")\n```\n\n## Rendering\n\n### Render Single View\n\n```swift\nimport Slipstream\n\nstruct HelloWorld: View {\n  var body: some View {\n    Text(\"Hello, world!\")\n  }\n}\n\nlet html = try renderHTML(HelloWorld())\nprint(html)\n```\n\n### Render Sitemap\n\n```swift\nimport Foundation\nimport Slipstream\n\nlet sitemap: Sitemap = [\n  \"index.html\": HomePage(),\n  \"about/index.html\": AboutPage(),\n  \"blog/post-1.html\": BlogPost(id: 1)\n]\n\nlet outputURL = URL(filePath: #filePath)\n  .deletingLastPathComponent()\n  .deletingLastPathComponent()\n  .appending(path: \"site\")\n\ntry renderSitemap(sitemap, to: outputURL)\n```\n\n## W3C Global Attributes\n\nAll views support W3C global attributes:\n\n```swift\n.id(\"unique-id\")\n.className(\"custom-class\")\n.title(\"Tooltip\")\n.data(\"key\", \"value\")\n.accessibilityLabel(\"Screen reader text\")\n.language(\"en\")\n.dir(.ltr)  // .rtl\n.contentEditable(.true)\n.spellcheck(true)\n.draggable(true)\n.tabindex(1)\n.disabled(true)\n.hidden()\n.inert(true)\n.autofocus(true)\n```\n\n## Best Practices\n\n### 1. Composition\n\nBreak complex views into smaller, reusable components:\n\n```swift\nstruct Navigation: View {\n  var body: some View {\n    Header {\n      HStack {\n        Link(\"Home\", destination: \"/\")\n        Link(\"About\", destination: \"/about\")\n      }\n    }\n  }\n}\n\nstruct HomePage: View {\n  var body: some View {\n    Navigation()\n    DocumentMain {\n      H1(\"Welcome\")\n      Text(\"Home page content\")\n    }\n  }\n}\n```\n\n### 2. Use Semantic HTML\n\nChoose semantic elements over generic containers:\n- Use `Header`, `Footer`, `Navigation`, `Article` instead of generic divs\n- Use `H1`-`H6` for headings, not styled `Text`\n- Use `Strong` for semantic emphasis, `Bold` for visual only\n\n### 3. Responsive Design\n\nApply responsive modifiers for different screen sizes:\n\n```swift\nText(\"Responsive\")\n  .fontSize(.base)\n  .fontSize(.large, condition: .tablet)\n  .fontSize(.extraLarge, condition: .desktop)\n  .padding(4)\n  .padding(8, condition: .tablet)\n```\n\n### 4. Tailwind CSS Alignment\n\nRemember that Slipstream uses Tailwind's predefined sizes, not exact pixel values. The framework finds the closest Tailwind class to requested values.\n\n### 5. Environment for Shared State\n\nUse environment for configuration that needs to flow down the hierarchy:\n\n```swift\nstruct Theme {\n  var primaryColor: TailwindColor\n  var accentColor: TailwindColor\n}\n\n// Define environment key, inject at top level, read in child views\n```\n\n## Common Patterns\n\n### Page Layout Template\n\n```swift\nstruct PageTemplate<Content: View>: View {\n  let title: String\n  @ViewBuilder let content: () -> Content\n\n  var body: some View {\n    Container {\n      Header {\n        Navigation {\n          Link(\"Home\", destination: \"/\")\n          Link(\"About\", destination: \"/about\")\n        }\n      }\n      DocumentMain {\n        H1(title)\n        content()\n      }\n      Footer {\n        Text(\"© 2025 My Site\")\n      }\n    }\n  }\n}\n\n// Usage\nPageTemplate(title: \"About\") {\n  Text(\"About page content\")\n}\n```\n\n### Card Component\n\n```swift\nstruct Card<Content: View>: View {\n  @ViewBuilder let content: () -> Content\n\n  var body: some View {\n    VStack(alignment: .leading, spacing: 16) {\n      content()\n    }\n    .padding(24)\n    .backgroundColor(.white)\n    .cornerRadius(.large)\n    .shadow(.medium)\n  }\n}\n```\n\n### Hero Section\n\n```swift\nstruct Hero: View {\n  let title: String\n  let subtitle: String\n\n  var body: some View {\n    VStack(alignment: .center, spacing: 24) {\n      H1(title)\n        .fontSize(.extraLarge5)\n        .fontWeight(.bold)\n        .textColor(.gray, shade: .x900)\n      Text(subtitle)\n        .fontSize(.extraLarge)\n        .textColor(.gray, shade: .x600)\n    }\n    .padding(.vertical, 96)\n    .textAlignment(.center)\n  }\n}\n```\n\n## Package Setup\n\n### Package.swift\n\n```swift\n// swift-tools-version: 5.10\nimport PackageDescription\n\nlet package = Package(\n  name: \"mysite\",\n  platforms: [\n    .macOS(\"14\"),\n    .iOS(\"17\"),\n  ],\n  dependencies: [\n    .package(url: \"https://github.com/jverkoey/slipstream.git\", branch: \"main\"),\n  ],\n  targets: [\n    .executableTarget(name: \"mysite\", dependencies: [\n      .product(name: \"Slipstream\", package: \"slipstream\"),\n    ]),\n  ]\n)\n```\n\n### Tailwind CSS Configuration\n\ntailwind.config.js:\n```javascript\nmodule.exports = {\n  content: [\"./site/**/*.html\"],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n```\n\ntailwind.css:\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n### Build Command\n\n```bash\nswift run && npx tailwindcss -i tailwind.css -o ./site/output.css --minify\n```\n\n## Quick Reference\n\n### Color Palette\n\nAvailable colors: `.black`, `.white`, `.gray`, `.red`, `.orange`, `.amber`, `.yellow`, `.lime`, `.green`, `.emerald`, `.teal`, `.cyan`, `.sky`, `.blue`, `.indigo`, `.violet`, `.purple`, `.fuchsia`, `.pink`, `.rose`\n\nShades: `.x50`, `.x100`, `.x200`, `.x300`, `.x400`, `.x500`, `.x600`, `.x700`, `.x800`, `.x900`, `.x950`\n\n### Font Sizes\n\n`.extraSmall`, `.small`, `.base`, `.large`, `.extraLarge`, `.extraLarge2`, `.extraLarge3`, `.extraLarge4`, `.extraLarge5`, `.extraLarge6`, `.extraLarge7`, `.extraLarge8`, `.extraLarge9`\n\n### Font Weights\n\n`.thin`, `.extraLight`, `.light`, `.normal`, `.medium`, `.semibold`, `.bold`, `.extraBold`, `.black`\n\n### Spacing Scale\n\nCommon values: 0, 1, 2, 4, 6, 8, 10, 12, 16, 20, 24, 32, 40, 48, 56, 64, 96, 128\n\n### Shadow Sizes\n\n`.small`, `.medium`, `.large`, `.extraLarge`, `.extraLarge2`, `.inner`, `.none`\n\n### Corner Radius\n\n`.none`, `.small`, `.medium`, `.large`, `.extraLarge`, `.extraLarge2`, `.extraLarge3`, `.full`\n\n## When to Use This Skill\n\nUse this skill when:\n- Building static websites with Swift\n- Converting SwiftUI-like code to HTML\n- Working with Tailwind CSS utilities in Swift\n- Creating reusable view components for web\n- Setting up Slipstream projects\n- Rendering sitemaps and multi-page sites\n- Implementing responsive layouts\n- Using W3C HTML elements and attributes\n- Working with SVG or MathML in Slipstream\n- Managing environment values\n- Troubleshooting Slipstream rendering\n\n## Documentation References\n\nThe complete Slipstream documentation is available at:\n- Main docs: https://slipstream.clutch.engineering/documentation/slipstream/\n- GitHub: https://github.com/jverkoey/slipstream\n- Site template: https://github.com/jverkoey/slipstream-site-template\n\nLocal documentation is in: `Sources/Slipstream/Documentation.docc/`\n",
    "github": {
      "stars": 11,
      "forks": 4,
      "owner": "ClutchEngineering",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/181276038?v=4",
      "lastCommit": "2026-01-03T15:06:09Z",
      "createdAt": "2024-09-14T19:07:29Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:00.919Z"
  },
  {
    "id": "buslog",
    "source": {
      "type": "github-search",
      "repo": "PoulpYBifle/BusLog",
      "path": ".claude/skills/buslog/SKILL.MD",
      "url": "https://github.com/PoulpYBifle/BusLog/blob/main/.claude/skills/buslog/SKILL.MD"
    },
    "frontmatter": {},
    "content": "# BusLog Skill - Business Logic Documentation\n\n**Purpose**: Use BusLog to analyze and document business logic workflows in the current codebase.\n\n---\n\n## When to Use This Skill\n\nInvoke this skill when:\n- User asks to document business logic or workflows\n- User wants to understand how a feature works across files\n- User needs to map dependencies (APIs, services, libraries)\n- User requests architecture documentation\n- User asks \"how does [feature] work?\"\n\nDo NOT use for:\n- Simple code explanations (use direct response)\n- Single file analysis (use Read tool)\n- General questions about the codebase\n\n---\n\n## Skill Execution Steps\n\n### Phase 1: Initialize BusLog\n\n1. Check if BusLog is initialized:\n   ```bash\n   python -m buslog.cli list\n   ```\n\n2. If not initialized, run:\n   ```bash\n   python -m buslog.cli init --name \"{project_name}\"\n   ```\n\n3. Generate analysis prompt:\n   ```bash\n   python -m buslog.cli analyze\n   ```\n\n### Phase 2: Analyze Codebase\n\n4. Read the generated analysis prompt from `.business-logic/` or command output\n\n5. Identify business workflows by:\n   - Scanning API endpoints (routes, controllers)\n   - Finding event handlers and listeners\n   - Locating scheduled jobs or background tasks\n   - Tracking database operations and transactions\n\n6. For each workflow identified, gather:\n   - **Entry points**: Endpoints, events, CLI commands\n   - **Files involved**: Controllers, services, models (with line numbers)\n   - **External dependencies**: APIs called, third-party services\n   - **Internal dependencies**: Services, repositories, utilities used\n   - **Libraries**: npm/pip packages and their purpose\n   - **Flow**: Step-by-step execution path\n\n### Phase 3: Document Workflows\n\n7. For each workflow, create documentation using:\n   ```bash\n   python -m buslog.cli add workflow-name\n   ```\n\n8. Edit the workflow file at `.business-logic/workflows/workflow-name.md` with:\n\n   ```markdown\n   # Workflow: [Name]\n\n   ## Description\n   [Business purpose - what problem does this solve?]\n\n   ## Déclencheurs\n   - **Endpoint**: `POST /api/path`\n   - **Event**: `event.name`\n   - **CLI**: `command-name`\n\n   ## Composants Utilisés\n\n   ### Fichiers\n   - `path/to/file.ts:10-45` - [Role in workflow]\n\n   ### APIs Externes\n   - **Service Name** (`api.example.com/endpoint`) - [Purpose]\n\n   ### Services Internes\n   - `ServiceName` - [What it does]\n\n   ### Librairies Tierces\n   - `package-name` (v1.0.0) - [How it's used]\n\n   ## Flux d'Exécution\n\n   ```mermaid\n   graph TD\n       A[Start] --> B{Decision}\n       B -->|Success| C[Action]\n       B -->|Error| D[Handle Error]\n       C --> E[End]\n       D --> E\n   ```\n\n   ## Dépendances Métier\n   - Déclenche: `other-workflow`\n   - Requis par: `other-workflow`\n\n   ## Notes & Annotations\n   - Important business rules\n   - Edge cases to watch\n   - Performance considerations\n   ```\n\n### Phase 4: Present Results\n\n9. Inform the user:\n   ```\n   I've documented [N] business workflows in BusLog:\n\n   1. [Workflow Name] - [Brief description]\n   2. [Workflow Name] - [Brief description]\n\n   View the documentation:\n   - Run: python -m buslog.cli serve\n   - Open: http://localhost:8080\n\n   All workflows are saved in .business-logic/workflows/\n   ```\n\n10. Optionally, suggest next steps:\n    - \"Would you like me to add diagrams for specific workflows?\"\n    - \"Should I analyze dependencies between workflows?\"\n    - \"Do you want me to add more details to any workflow?\"\n\n---\n\n## Output Format\n\nAlways provide:\n1. **Summary**: List of workflows identified\n2. **Location**: Where documentation was saved\n3. **Next steps**: How to view and use the documentation\n\nExample:\n```\n✅ BusLog Analysis Complete\n\nDocumented 5 workflows:\n1. User Registration - POST /api/auth/register\n2. Payment Processing - Stripe webhook handling\n3. Email Notifications - Background job queue\n4. Data Export - GET /api/export/csv\n5. Admin Dashboard - Aggregated metrics\n\n📁 Documentation: .business-logic/workflows/\n🌐 View UI: python -m buslog.cli serve\n\nNext: Would you like me to add more details to any workflow?\n```\n\n---\n\n## Best Practices\n\n1. **Be thorough**: Include ALL files involved, even small utilities\n2. **Trace dependencies**: Follow the execution path completely\n3. **Use Mermaid diagrams**: Visual flow helps understanding\n4. **Note business rules**: Document WHY, not just WHAT\n5. **Include line numbers**: Make it easy to find code\n6. **Keep updated**: Update workflows when code changes\n\n---\n\n## Common Patterns to Look For\n\n### API Endpoints\n- Express: `app.get()`, `router.post()`\n- FastAPI: `@app.get`, `@router.post`\n- Django: `path()`, `url()`\n- NestJS: `@Get()`, `@Post()`\n\n### Event Handlers\n- Node: `.on()`, `addEventListener()`\n- Python: `@event.listens_for`\n- Custom: `@subscribe`, `@handler`\n\n### Background Jobs\n- Bull/BullMQ: `queue.add()`\n- Celery: `@task`, `@periodic_task`\n- Cron: `@schedule`, `cron.schedule()`\n\n### Database Operations\n- ORMs: Sequelize, TypeORM, SQLAlchemy, Prisma\n- Transactions: `BEGIN`, `COMMIT`, `ROLLBACK`\n- Migrations: Version control for schema\n\n---\n\n## Error Handling\n\nIf BusLog is not installed:\n```bash\npip install -e /path/to/AIDD-logic-metier-workflow\n```\n\nIf commands fail:\n- Use `python -m buslog.cli` instead of `buslog`\n- Check that you're in the correct project directory\n- Verify BusLog is initialized with `python -m buslog.cli list`\n\n---\n\n## Skill Metadata\n\n- **Version**: 1.0\n- **Author**: PoulpYBifle\n- **Compatible with**: Any codebase (language-agnostic)\n- **Requirements**: Python 3.10+, BusLog installed\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "PoulpYBifle",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/58371552?v=4",
      "license": "MIT",
      "lastCommit": "2025-10-30T10:56:44Z",
      "createdAt": "2025-10-30T08:43:40Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:01.806Z"
  },
  {
    "id": "zed-config",
    "source": {
      "type": "github-search",
      "repo": "wcygan/dotfiles",
      "path": ".claude/skills/zed-config/SKILL.md",
      "url": "https://github.com/wcygan/dotfiles/blob/main/.claude/skills/zed-config/SKILL.md"
    },
    "frontmatter": {
      "name": "zed-config",
      "description": "Configure Zed editor settings, keybindings, language servers, themes, and AI agents. Use when modifying Zed configuration, adding keybindings, setting up language servers, or configuring external agents like Claude Code. Keywords: zed, editor, settings, keybindings, keymap, LSP, language server, theme, agent, configure",
      "allowed-tools": "Read, Edit, Bash(cat:*), Bash(jq:*)"
    },
    "content": "# Zed Editor Configuration Skill\n\nManages Zed editor configuration through `settings.json` and `keymap.json` files.\n\n## Important Context\n\nThis dotfiles repository has **symlinked** the global Zed configuration:\n- Configuration location: `config/zed/`\n- Symlinked to: `~/.config/zed/`\n- **All changes are applied globally** to the Zed installation\n\nConfiguration files:\n- `config/zed/settings.json` - Main editor settings\n- `config/zed/keymap.json` - Custom keybindings\n\n## Configuration Categories\n\n### 1. Editor Appearance & Behavior\n\n**Theme Configuration:**\n```json\n{\n  \"theme\": {\n    \"mode\": \"system\",           // \"system\", \"light\", or \"dark\"\n    \"light\": \"One Light\",\n    \"dark\": \"Zedokai\"\n  },\n  \"icon_theme\": \"Catppuccin Frappé\"\n}\n```\n\n**Font Settings:**\n```json\n{\n  \"ui_font_size\": 14,\n  \"buffer_font_size\": 14.0,\n  \"buffer_font_weight\": 430.0\n}\n```\n\n**Editor Behavior:**\n```json\n{\n  \"preferred_line_length\": 120,\n  \"soft_wrap\": \"preferred_line_length\",\n  \"autosave\": {\n    \"after_delay\": {\n      \"milliseconds\": 200\n    }\n  },\n  \"show_completions_on_input\": true\n}\n```\n\n### 2. Terminal Configuration\n\n```json\n{\n  \"terminal\": {\n    \"shell\": {\n      \"program\": \"fish\"        // Shell program (fish, zsh, bash)\n    }\n  }\n}\n```\n\n### 3. Language Server Protocol (LSP)\n\n**Structure:** Use nested objects (not dot-delimited strings)\n\n**Deno:**\n```json\n{\n  \"lsp\": {\n    \"deno\": {\n      \"settings\": {\n        \"deno\": {\n          \"enable\": true\n        }\n      }\n    }\n  }\n}\n```\n\n**Rust Analyzer:**\n```json\n{\n  \"lsp\": {\n    \"rust-analyzer\": {\n      \"initialization_options\": {\n        \"inlayHints\": {\n          \"maxLength\": null,\n          \"lifetimeElisionHints\": {\n            \"enable\": \"skip_trivial\",\n            \"useParameterNames\": true\n          }\n        }\n      },\n      \"binary\": {\n        \"ignore_system_version\": false\n      },\n      \"enable_lsp_tasks\": true\n    }\n  }\n}\n```\n\n**Python (Ruff):**\n```json\n{\n  \"lsp\": {\n    \"ruff\": {\n      \"initialization_options\": {\n        \"settings\": {}\n      }\n    }\n  }\n}\n```\n\n### 4. Language-Specific Settings\n\nConfigure per-language overrides in the `languages` object:\n\n```json\n{\n  \"languages\": {\n    \"Python\": {\n      \"language_servers\": [\"ty\", \"ruff\"],\n      \"format_on_save\": \"on\",\n      \"formatter\": [\n        {\n          \"language_server\": {\n            \"name\": \"ruff\"\n          }\n        }\n      ]\n    },\n    \"TypeScript\": {\n      \"language_servers\": [\"deno\", \"!typescript-language-server\", \"!vtsls\", \"!eslint\"],\n      \"formatter\": \"language_server\"\n    },\n    \"TSX\": {\n      \"language_servers\": [\"deno\", \"!typescript-language-server\", \"!vtsls\", \"!eslint\"],\n      \"formatter\": \"language_server\"\n    }\n  }\n}\n```\n\n**Language server prioritization:**\n- List servers in order of preference\n- Prefix with `!` to disable (e.g., `\"!typescript-language-server\"`)\n- Use `\"...\"` to expand remaining registered servers\n\n**Available options per language:**\n- `tab_size`: Indentation spaces\n- `formatter`: Code formatting tool\n- `format_on_save`: `\"on\"`, `\"off\"`, or `\"language_server\"`\n- `enable_language_server`: Toggle LSP\n- `hard_tabs`: Use tabs instead of spaces\n- `preferred_line_length`: Max line width\n- `soft_wrap`: Line wrapping behavior\n\n### 5. AI Agent Configuration\n\n**Agent Settings:**\n```json\n{\n  \"agent\": {\n    \"always_allow_tool_actions\": true,\n    \"use_modifier_to_send\": true,\n    \"play_sound_when_agent_done\": true,\n    \"default_profile\": \"write\",\n    \"default_model\": {\n      \"provider\": \"google\",\n      \"model\": \"gemini-2.5-pro-exp-03-25\"\n    }\n  }\n}\n```\n\n**External Agents (Claude Code, Gemini CLI):**\n```json\n{\n  \"agent_servers\": {\n    \"claude\": {\n      \"env\": {\n        \"CLAUDE_CODE_EXECUTABLE\": \"/path/to/custom-executable\"\n      }\n    },\n    \"custom-agent\": {\n      \"command\": \"node\",\n      \"args\": [\"~/projects/agent/index.js\", \"--acp\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n**Edit Predictions:**\n```json\n{\n  \"show_edit_predictions\": false,\n  \"edit_predictions\": {\n    \"mode\": \"eager\",              // \"eager\" or \"manual\"\n    \"enabled_in_text_threads\": false\n  }\n}\n```\n\n### 6. Keybindings Configuration\n\n**File:** `config/zed/keymap.json`\n\n**Structure:** JSON array of binding objects with optional contexts\n\n```json\n[\n  {\n    \"context\": \"Editor\",          // Optional: \"Editor\", \"Terminal\", \"Workspace\"\n    \"bindings\": {\n      \"cmd-k\": \"assistant::InlineAssist\",\n      \"cmd-t\": \"workspace::NewTerminal\",\n      \"cmd-1\": \"workspace::ToggleLeftDock\"\n    }\n  }\n]\n```\n\n**Key Syntax:**\n- **Modifiers:** `cmd-`, `ctrl-`, `alt-`, `shift-`, `fn-`\n- **Sequences:** Space-separated (e.g., `\"cmd-k cmd-s\"`)\n- **Platform-agnostic:** Use `secondary-` to adapt to platform\n\n**Common Actions:**\n- `workspace::ToggleBottomDock` - Toggle bottom panel\n- `workspace::NewTerminal` - Open new terminal\n- `workspace::ToggleLeftDock` - Toggle file explorer\n- `workspace::ToggleRightDock` - Toggle right panel\n- `assistant::InlineAssist` - Inline AI assist\n- `file_finder::Toggle` - Open file finder\n- `task::Spawn` - Run task\n- `pane::ActivatePreviousItem` / `pane::ActivateNextItem` - Navigate tabs\n- `[\"agent::NewExternalAgentThread\", { \"agent\": \"claude_code\" }]` - Open agent with args\n\n**Context Options:**\n- `\"Editor\"` - Any editor pane\n- `\"Editor && mode == full\"` - Main code editors only\n- `\"Terminal\"` - Terminal panes\n- `\"!Editor && !Terminal\"` - Everywhere except editors/terminals\n\n**Disable Binding:**\n```json\n{\n  \"context\": \"Workspace\",\n  \"bindings\": {\n    \"cmd-r\": null              // Disables cmd-r\n  }\n}\n```\n\n### 7. SSH Connections\n\nConfigure remote development environments:\n\n```json\n{\n  \"ssh_connections\": [\n    {\n      \"host\": \"betty\",\n      \"projects\": [\n        {\n          \"paths\": [\"/home/user/Development/project\"]\n        }\n      ]\n    }\n  ]\n}\n```\n\n### 8. Tabs Configuration\n\n```json\n{\n  \"tabs\": {\n    \"git_status\": true,         // Show git status colors\n    \"file_icons\": true          // Show file type icons\n  }\n}\n```\n\n## Workflow Instructions\n\n### Reading Current Configuration\n\n**Always start by reading the current config:**\n```\nRead config/zed/settings.json\nRead config/zed/keymap.json\n```\n\n### Modifying Settings\n\n**Use Edit tool for precise changes:**\n1. Read the current configuration\n2. Identify the exact JSON to modify\n3. Use Edit to replace the specific section\n4. Preserve formatting and comments\n\n**Example - Add new language server:**\n```json\n// Before:\n{\n  \"lsp\": {\n    \"deno\": { ... }\n  }\n}\n\n// After:\n{\n  \"lsp\": {\n    \"deno\": { ... },\n    \"gopls\": {\n      \"initialization_options\": {\n        \"usePlaceholders\": true\n      }\n    }\n  }\n}\n```\n\n### Adding Keybindings\n\n**Append to the bindings object:**\n```json\n// Existing:\n{\n  \"bindings\": {\n    \"cmd-t\": \"workspace::NewTerminal\"\n  }\n}\n\n// Add new binding:\n{\n  \"bindings\": {\n    \"cmd-t\": \"workspace::NewTerminal\",\n    \"cmd-r\": \"editor::Rename\"\n  }\n}\n```\n\n### Validating Configuration\n\nAfter modifications:\n1. Check JSON syntax is valid (preserve trailing commas in objects/arrays)\n2. Ensure comments use `//` format\n3. Verify nested structure (especially for `lsp` and `languages`)\n\n## Common Configuration Tasks\n\n### Add New Language Server\n\n1. Read current `lsp` section\n2. Add server configuration under `lsp` key\n3. Optionally add language-specific settings under `languages`\n\n### Change Theme\n\n```json\n{\n  \"theme\": {\n    \"mode\": \"dark\",\n    \"dark\": \"Tokyo Night\"\n  }\n}\n```\n\n### Configure Formatter\n\n```json\n{\n  \"languages\": {\n    \"JavaScript\": {\n      \"formatter\": {\n        \"external\": {\n          \"command\": \"prettier\",\n          \"arguments\": [\"--stdin-filepath\", \"{buffer_path}\"]\n        }\n      },\n      \"format_on_save\": \"on\"\n    }\n  }\n}\n```\n\n### Add Custom Keybinding for External Agent\n\n```json\n{\n  \"bindings\": {\n    \"cmd-3\": [\"agent::NewExternalAgentThread\", { \"agent\": \"claude_code\" }]\n  }\n}\n```\n\n## Best Practices\n\n1. **Read before editing** - Always read current config to understand structure\n2. **Preserve comments** - Keep existing `//` comments intact\n3. **Use nested objects** - For LSP, use `{ \"lsp\": { \"server\": { \"settings\": {} } } }`\n4. **Test incrementally** - Make one change at a time\n5. **Check Zed docs** - Reference official docs for available options\n6. **Language-specific overrides** - Use `languages` object to override global settings\n7. **Context-aware keybindings** - Use `context` to scope shortcuts appropriately\n\n## Troubleshooting\n\n**Invalid JSON:** Zed supports extended JSON with `//` comments but requires valid structure\n\n**LSP not working:**\n- Check `enable_language_server` setting\n- Verify `initialization_options` structure\n- Ensure language server binary is installed\n\n**Keybinding conflicts:**\n- Check base keymap setting (`\"base_keymap\": \"JetBrains\"`)\n- Use `null` to disable conflicting default bindings\n- Test in relevant context (Editor vs Terminal vs Workspace)\n\n**Changes not applying:**\n- Configuration is symlinked from `config/zed/` to `~/.config/zed/`\n- Changes take effect immediately in Zed (no restart needed for most settings)\n- Some LSP changes require server restart\n\n## Quick Reference\n\n**Current Configuration Files:**\n- Settings: `config/zed/settings.json`\n- Keybindings: `config/zed/keymap.json`\n- Conversations: `config/zed/conversations/`\n- Custom Prompts: `config/zed/prompts/`\n- Themes: `config/zed/themes/`\n\n**Current Base Keymap:** JetBrains\n\n**Current Languages Configured:**\n- Python (ty, ruff LSP, format on save)\n- TypeScript (Deno LSP, disable default TS servers)\n- TSX (Deno LSP, disable default TS servers)\n\n**Current LSP Servers:**\n- Deno (enabled)\n- Rust Analyzer (inlay hints, analyzer target dir)\n- Ruff (Python)\n",
    "github": {
      "stars": 182,
      "forks": 16,
      "owner": "wcygan",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/32029716?v=4",
      "lastCommit": "2026-01-02T17:58:01Z",
      "createdAt": "2023-03-10T04:06:04Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:05.446Z"
  },
  {
    "id": "safe-refactoring",
    "source": {
      "type": "github-search",
      "repo": "LorenFrankLab/non_local_detector",
      "path": ".claude/skills/safe-refactoring/skill.md",
      "url": "https://github.com/LorenFrankLab/non_local_detector/blob/main/.claude/skills/safe-refactoring/skill.md"
    },
    "frontmatter": {
      "name": "safe-refactoring",
      "description": "Change code structure without changing behavior, with zero tolerance for behavioral changes",
      "tags": "[refactoring, testing, scientific, quality]",
      "version": "1.0"
    },
    "content": "# Safe Refactoring for Scientific Code\n\n## Overview\n\nChange code structure without changing behavior. Zero tolerance for behavioral changes during refactoring.\n\n**Core principle:** Establish baseline, refactor, verify exact match (within floating-point noise).\n\n**Announce at start:** \"I'm using the safe-refactoring skill to restructure this code.\"\n\n## When to Use This Skill\n\n**Use for:**\n- Improving code readability without changing logic\n- Extracting reusable functions\n- Renaming variables/functions for clarity\n- Reorganizing code structure\n- Performance optimization (without changing numerical behavior)\n\n**Don't use for:**\n- Changing behavior or algorithms (use scientific-tdd instead)\n- Adding new features (use scientific-tdd instead)\n- Fixing bugs (use scientific-tdd or fix directly with tests)\n\n## Process Checklist\n\nCopy to TodoWrite:\n\n```\nSafe Refactoring Progress:\n- [ ] Run full test suite (establish baseline)\n- [ ] Run snapshot tests (establish baseline)\n- [ ] Capture coverage report\n- [ ] Perform refactoring\n- [ ] Run full test suite (must match baseline exactly)\n- [ ] Run snapshot tests (must match baseline exactly)\n- [ ] Compare coverage (should stay same or improve)\n- [ ] Run quality checks (ruff + black)\n- [ ] Verify no numerical differences\n- [ ] Commit refactoring\n```\n\n## Strict Rules\n\n**ZERO tolerance for:**\n- Any test that passed before and fails after\n- Any test that failed before and passes after (suggests test was broken)\n- Any snapshot differences (not even floating-point noise)\n- Decreased test coverage\n- Any behavioral changes\n\n**If any of these occur:** Revert and investigate why.\n\n## Detailed Steps\n\n### Step 1: Run Full Test Suite (Baseline)\n\n```bash\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest -v 2>&1 | tee /tmp/baseline_tests.txt\n```\n\n**Record:**\n- Total tests: `grep \"passed\" /tmp/baseline_tests.txt`\n- Any failures (if refactoring existing code with known issues)\n- Test execution time\n\n**Expected:** All tests pass (or document any known failures)\n\n### Step 2: Run Snapshot Tests (Baseline)\n\n```bash\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest -m snapshot -v 2>&1 | tee /tmp/baseline_snapshots.txt\n```\n\n**CRITICAL:** Snapshots must match exactly after refactoring.\n\n**Expected:** All snapshot tests pass\n\n### Step 3: Capture Coverage Report\n\n```bash\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest --cov=non_local_detector --cov-report=term --cov-report=json:coverage_baseline.json\n```\n\n**Record:** Coverage percentage for files being refactored\n\n**Why:** Coverage should not decrease during refactoring (ideally improves)\n\n### Step 4: Perform Refactoring\n\n**Refactoring techniques:**\n\n1. **Extract function:**\n   ```python\n   # Before\n   def complex_function():\n       # ... 50 lines of code\n       result = x * 2 + y\n       # ... more code\n       return final_result\n\n   # After\n   def complex_function():\n       # ... code\n       result = _calculate_intermediate(x, y)\n       # ... code\n       return final_result\n\n   def _calculate_intermediate(x, y):\n       return x * 2 + y\n   ```\n\n2. **Rename for clarity:**\n   ```python\n   # Before\n   def f(x):\n       return x * 2\n\n   # After\n   def calculate_doubled_value(value):\n       return value * 2\n   ```\n\n3. **Reorganize structure:**\n   ```python\n   # Before: All in one file\n\n   # After: Separated into modules\n   # - core_logic.py\n   # - utilities.py\n   # - validation.py\n   ```\n\n4. **Optimize performance (numerically equivalent):**\n   ```python\n   # Before\n   for i in range(n):\n       result[i] = f(x[i])\n\n   # After (JAX)\n   result = jax.vmap(f)(x)\n   ```\n\n**During refactoring:**\n- Make small, incremental changes\n- Test after each change if possible\n- Keep numerical operations identical\n- Maintain exact same algorithms\n\n### Step 5: Run Full Test Suite (Verify Match)\n\n```bash\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest -v 2>&1 | tee /tmp/refactored_tests.txt\n```\n\n**Compare to baseline:**\n```bash\ndiff /tmp/baseline_tests.txt /tmp/refactored_tests.txt\n```\n\n**MUST verify:**\n- Same number of tests run\n- Same tests pass\n- Same tests fail (if any)\n- Similar execution time (within 20%)\n\n**If differences:**\n- Any new test failures: REVERT IMMEDIATELY\n- Any new test passes: Investigate (test was broken?)\n- Different test count: Investigate (tests missing or duplicated?)\n\n### Step 6: Run Snapshot Tests (Verify Match)\n\n```bash\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest -m snapshot -v 2>&1 | tee /tmp/refactored_snapshots.txt\n```\n\n**CRITICAL:** Must match baseline EXACTLY.\n\n**Expected:** All snapshot tests pass, no differences\n\n**If snapshot differences:**\n1. **DO NOT UPDATE SNAPSHOTS**\n2. Investigate why behavior changed\n3. This is NOT a refactoring if behavior changed\n4. Revert and reconsider approach\n\n### Step 7: Compare Coverage\n\n```bash\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest --cov=non_local_detector --cov-report=term --cov-report=json:coverage_refactored.json\n```\n\n**Compare:**\n```bash\n# If you have jq installed\njq '.totals.percent_covered' coverage_baseline.json\njq '.totals.percent_covered' coverage_refactored.json\n```\n\n**Expected:**\n- Coverage stays same or improves\n- Never decreases\n\n**If coverage decreased:**\n- Some code paths no longer tested\n- Investigate and fix or revert\n\n### Step 8: Run Quality Checks\n\n```bash\n/Users/edeno/miniconda3/envs/spectral_connectivity/bin/ruff check src/\n/Users/edeno/miniconda3/envs/spectral_connectivity/bin/ruff format src/\n/Users/edeno/miniconda3/envs/non_local_detector/bin/black src/\n```\n\n**Expected:** All checks pass\n\n**Fix any issues:** Refactoring is good opportunity to improve code quality\n\n### Step 9: Verify No Numerical Differences\n\nFor mathematical code, verify numerical equivalence:\n\n```bash\n# Run golden regression\n/Users/edeno/miniconda3/envs/non_local_detector/bin/pytest \\\n  src/non_local_detector/tests/test_golden_regression.py -v\n```\n\n**Expected:** Exact match (or differences < 1e-14)\n\n**If differences > 1e-14:**\n- This is NOT a pure refactoring\n- Behavior has changed\n- Use numerical-validation skill instead\n\n### Step 10: Commit Refactoring\n\n**Only commit if ALL checks pass:**\n\n```bash\ngit add <refactored_files> <test_files>\ngit commit -m \"refactor: improve <component> code structure\n\n- Extract <function> for reusability\n- Rename <variable> for clarity\n- Reorganize <module> structure\n\nNo behavioral changes:\n- All tests pass (N tests)\n- Snapshots unchanged\n- Coverage: X% → Y%\n\n🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\"\n```\n\n## Performance Optimization Refactoring\n\nWhen optimizing for performance:\n\n1. **Capture performance baseline:**\n   ```bash\n   pytest --durations=10 > /tmp/baseline_durations.txt\n   ```\n\n2. **Make optimization**\n\n3. **Verify numerical equivalence** (use numerical-validation skill)\n\n4. **Measure performance improvement:**\n   ```bash\n   pytest --durations=10 > /tmp/optimized_durations.txt\n   ```\n\n5. **Document improvement:**\n   ```\n   Optimization: Use JAX vmap instead of for loop\n   Speedup: 3.2x (450ms → 140ms)\n   Numerical difference: < 1e-14 (verified)\n   ```\n\n## Integration with Other Skills\n\n- **Before refactoring:** Consider if change actually needs new behavior (use scientific-tdd instead)\n- **With numerical-validation:** If refactoring mathematical code, use numerical-validation to verify equivalence\n- **With jax skill:** When optimizing JAX code, use jax skill for best practices\n\n## Example Workflow\n\n**Task:** Extract position decoding logic into reusable function\n\n```\n1. Baseline:\n   - Run pytest: 427 passed, 0 failed\n   - Run snapshots: 15 passed, 0 failed\n   - Coverage: 69%\n\n2. Refactor:\n   - Extract _decode_position_from_posterior() function\n   - Update 3 call sites to use new function\n   - No logic changes, just extraction\n\n3. Verify:\n   - Run pytest: 427 passed, 0 failed ✓\n   - Run snapshots: 15 passed, 0 failed ✓\n   - Coverage: 69% (unchanged) ✓\n\n4. Quality:\n   - Ruff: All checks pass ✓\n   - Black: Formatted ✓\n\n5. Commit:\n   \"refactor: extract position decoding into reusable function\"\n```\n\n## Red Flags\n\n**STOP and revert if:**\n- Any test changes status (pass → fail or fail → pass)\n- Any snapshot differences appear\n- Coverage decreases\n- Numerical differences > 1e-14\n- You're tempted to update snapshots\n- You're adding new logic (use scientific-tdd instead)\n\n**Safe to proceed if:**\n- All tests match baseline exactly\n- No snapshot changes\n- Coverage same or better\n- Code quality improves\n- No new functionality added\n\n## Common Mistakes\n\n**\"It's just a small behavioral change\"**\n- No such thing in refactoring\n- Any behavioral change = not refactoring\n- Use scientific-tdd for behavioral changes\n\n**\"I'll update the snapshots since the new output is better\"**\n- That's not refactoring, it's changing behavior\n- Refactoring = zero snapshot changes\n- Use scientific-tdd if output should change\n\n**\"Tests are slow, I'll skip them\"**\n- Never skip tests during refactoring\n- Tests are your safety net\n- Without tests, you can't verify it's a refactoring\n\n**\"Coverage went down but the code is better\"**\n- Better code shouldn't lose coverage\n- Investigate why coverage decreased\n- Fix or revert\n",
    "github": {
      "stars": 1,
      "forks": 1,
      "owner": "LorenFrankLab",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/40640751?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-15T16:21:03Z",
      "createdAt": "2023-06-07T17:09:21Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:06.159Z"
  },
  {
    "id": "taxasge-backend-dev",
    "source": {
      "type": "github-search",
      "repo": "KouemouSah/taxasge",
      "path": ".claude/skills/taxasge-backend-dev/Skill.md",
      "url": "https://github.com/KouemouSah/taxasge/blob/main/.claude/skills/taxasge-backend-dev/Skill.md"
    },
    "frontmatter": {
      "name": "TaxasGE Backend Dev",
      "description": "Patterns backend FastAPI architecture 3-tiers, complète DEV_AGENT avec best practices backend",
      "version": "2.0.0"
    },
    "content": "# TaxasGE Backend Dev Skill\n\n## Overview\n\nCe skill **complète le DEV_AGENT** avec des patterns spécifiques backend FastAPI. Il ne remplace PAS le workflow de développement mais fournit des références techniques et templates pour l'implémentation backend.\n\n**Principe fondamental** : Guide technique, pas workflow (le workflow est dans DEV_AGENT.md et DEV_WORKFLOW.md).\n\n---\n\n## When to Use This Skill\n\nClaude invoquera automatiquement ce skill quand :\n- DEV_AGENT implémente tâche backend\n- Besoin patterns FastAPI\n- Questions architecture 3-tiers\n- Référence technique backend\n\n**Ne PAS utiliser pour** :\n- Workflow développement (voir DEV_WORKFLOW.md)\n- Validation (voir Go/No-Go Validator)\n- Orchestration (voir Orchestrator)\n\n---\n\n## Core Responsibilities\n\n### 1. Architecture 3-Tiers Stricte\n\n**Séparation obligatoire** :\n```\nRoutes (API Layer)        ← Validation input, Response format, RBAC\n    ↓\nServices (Business Logic) ← Logique métier, Orchestration\n    ↓\nRepositories (Data)       ← CRUD, Queries SQL, Transactions\n```\n\n**Règles absolues** :\n- ❌ Jamais de business logic dans routes\n- ❌ Jamais de SQL direct dans services\n- ❌ Jamais de validation métier dans repositories\n\n### 2. Référence Documentation Backend\n\n**Source unique de vérité** : `.github/docs-internal/Documentations/Backend/`\n\nStructure :\n```\n.github/docs-internal/Documentations/Backend/\n├── API_REFERENCE.md              ← Endpoints documentés\n├── ARCHITECTURE.md               ← Architecture globale\n├── DATABASE_SCHEMA.md            ← Schéma DB\n├── ERROR_HANDLING.md             ← Gestion erreurs RFC 7807\n├── AUTHENTICATION.md             ← JWT + RBAC\n└── DEPLOYMENT.md                 ← Déploiement Cloud Run\n```\n\n**Important** : Toujours référencer cette documentation, ne PAS dupliquer.\n\n### 3. Templates Code\n\n**Templates disponibles** :\n- `templates/endpoint_template.py` - Structure route FastAPI\n- `templates/service_template.py` - Structure service métier\n- `templates/repository_template.py` - Structure repository DB\n\n**Usage** : Copie template → Adapte au use case → Implémente\n\n---\n\n## Patterns Backend\n\n### Pattern 1 : Route FastAPI Standard\n\n**Template** : `templates/endpoint_template.py`\n\n**Structure** :\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom app.services.{module}_service import {Module}Service\nfrom app.core.auth import require_role\nfrom app.schemas.{module} import {Module}Create, {Module}Response\n\nrouter = APIRouter(prefix=\"/{module}\", tags=[\"{Module}\"])\n\n@router.post(\"/\", response_model={Module}Response, status_code=201)\n@require_role(\"citizen\")\nasync def create_{module}(\n    data: {Module}Create,\n    service: {Module}Service = Depends()\n):\n    \"\"\"\n    Créer {module}.\n    \n    **Source** : .github/docs-internal/Documentations/Backend/API_REFERENCE.md\n    \n    **Validation** :\n    - Pydantic {Module}Create automatique\n    - RBAC : Rôle \"citizen\" requis\n    \n    **Errors** :\n    - 400 : Validation error\n    - 401 : Non authentifié\n    - 403 : Non autorisé\n    \"\"\"\n    return await service.create(data)\n```\n\n**Points clés** :\n1. **Prefix + Tags** : Organisation Swagger\n2. **Response model** : Type-safety sortie\n3. **RBAC decorator** : `@require_role()` AVANT handler\n4. **Dependency injection** : Service via `Depends()`\n5. **Docstring complète** : Source + Validation + Errors\n\n---\n\n### Pattern 2 : Service Métier\n\n**Template** : `templates/service_template.py`\n\n**Structure** :\n```python\nfrom app.database.repositories.{module}_repository import {Module}Repository\nfrom app.core.errors import ValidationError, ResourceNotFoundError\n\nclass {Module}Service:\n    \"\"\"\n    Service {module}.\n    \n    **Source** : .github/docs-internal/Documentations/Backend/ARCHITECTURE.md\n    \"\"\"\n    \n    def __init__(self):\n        self.repo = {Module}Repository()\n    \n    async def create(self, data: {Module}Create) -> {Module}:\n        \"\"\"\n        Créer {module} avec validations métier.\n        \n        **Business Rules** :\n        1. {Règle métier 1}\n        2. {Règle métier 2}\n        \n        **Source** : .github/docs-internal/Documentations/Backend/API_REFERENCE.md\n        \"\"\"\n        # Validations métier\n        await self._validate_business_rules(data)\n        \n        # Créer via repository\n        result = await self.repo.create(data)\n        \n        # Actions post-création (emails, notifications, etc.)\n        await self._post_creation_actions(result)\n        \n        return result\n    \n    async def _validate_business_rules(self, data: {Module}Create):\n        \"\"\"\n        Validations métier spécifiques.\n        \n        Raises:\n            ValidationError: Si validation échoue\n        \"\"\"\n        # Exemple : Vérifier unicité\n        existing = await self.repo.get_by_field(data.unique_field)\n        if existing:\n            raise ValidationError(\n                field=\"unique_field\",\n                message=\"Already exists\",\n                value=data.unique_field\n            )\n    \n    async def _post_creation_actions(self, entity):\n        \"\"\"Actions après création (async, non-bloquantes).\"\"\"\n        # Exemple : Envoyer email confirmation\n        pass\n```\n\n**Points clés** :\n1. **Injection repository** : Dans `__init__`\n2. **Business logic SEULEMENT** : Pas de SQL\n3. **Méthodes privées** : `_validate_*`, `_post_*`\n4. **Exceptions custom** : `ValidationError`, `ResourceNotFoundError`\n5. **Docstrings avec source** : Traçabilité\n\n---\n\n### Pattern 3 : Repository DB\n\n**Template** : `templates/repository_template.py`\n\n**Structure** :\n```python\nfrom sqlalchemy import select, update, delete\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.models.{module} import {Module}\nfrom app.database.connection import get_db\nfrom app.core.errors import ResourceNotFoundError\nfrom fastapi import Depends\n\nclass {Module}Repository:\n    \"\"\"\n    Repository {module}.\n    \n    **Source** : database/schema.sql\n    \"\"\"\n    \n    def __init__(self, db: AsyncSession = Depends(get_db)):\n        self.db = db\n    \n    async def create(self, data: {Module}Create) -> {Module}:\n        \"\"\"\n        Créer {module} en DB.\n        \n        **Source** : database/schema.sql ligne {X}\n        \"\"\"\n        entity = {Module}(**data.dict())\n        self.db.add(entity)\n        await self.db.flush()  # Get ID\n        await self.db.refresh(entity)\n        return entity\n    \n    async def get_by_id(self, id: int) -> {Module}:\n        \"\"\"\n        Récupérer {module} par ID.\n        \n        Raises:\n            ResourceNotFoundError: Si non trouvé\n        \"\"\"\n        query = select({Module}).where({Module}.id == id)\n        result = await self.db.execute(query)\n        entity = result.scalar_one_or_none()\n        \n        if not entity:\n            raise ResourceNotFoundError(\n                resource=\"{Module}\",\n                identifier=str(id)\n            )\n        \n        return entity\n    \n    async def list(\n        self, \n        filters: dict = None,\n        limit: int = 100,\n        offset: int = 0\n    ) -> list[{Module}]:\n        \"\"\"\n        Lister {module}s avec pagination.\n        \n        **Source** : .github/docs-internal/Documentations/Backend/API_REFERENCE.md\n        \"\"\"\n        query = select({Module})\n        \n        # Filtres dynamiques\n        if filters:\n            for key, value in filters.items():\n                if hasattr({Module}, key):\n                    query = query.where(getattr({Module}, key) == value)\n        \n        # Pagination\n        query = query.limit(limit).offset(offset)\n        \n        result = await self.db.execute(query)\n        return result.scalars().all()\n    \n    async def update(self, id: int, data: {Module}Update) -> {Module}:\n        \"\"\"Mettre à jour {module}.\"\"\"\n        entity = await self.get_by_id(id)\n        \n        for key, value in data.dict(exclude_unset=True).items():\n            setattr(entity, key, value)\n        \n        await self.db.flush()\n        await self.db.refresh(entity)\n        return entity\n    \n    async def delete(self, id: int) -> bool:\n        \"\"\"Supprimer {module}.\"\"\"\n        entity = await self.get_by_id(id)\n        await self.db.delete(entity)\n        return True\n```\n\n**Points clés** :\n1. **Injection DB session** : Via `Depends(get_db)`\n2. **Queries SQL SEULEMENT** : Pas de business logic\n3. **Gestion erreurs** : `ResourceNotFoundError` si non trouvé\n4. **Pagination** : `limit` + `offset`\n5. **Source schema.sql** : Référence ligne exacte\n\n---\n\n## Checklist Implémentation Backend\n\nAvant de considérer tâche backend terminée :\n\n### Code\n- [ ] **Architecture 3-tiers** respectée (Routes → Services → Repositories)\n- [ ] **Pydantic models** avec validation complète\n- [ ] **Error handling** RFC 7807 implémenté\n- [ ] **RBAC** configuré (`@require_role`)\n- [ ] **Docstrings** complètes avec sources\n\n### Sources (Règle 0)\n- [ ] **Schema DB** vérifié (`database/schema.sql`)\n- [ ] **Documentation backend** consultée (`.github/docs-internal/Documentations/Backend/`)\n- [ ] **Code existant** respecté (patterns cohérents)\n\n### Tests\n- [ ] **Tests unitaires** services écrits (>85% coverage)\n- [ ] **Tests endpoints** écrits (>85% coverage)\n- [ ] **Tests repositories** écrits (>90% coverage)\n- [ ] **Tests passent** (100%)\n\n### Qualité\n- [ ] **Lint** : 0 erreurs flake8\n- [ ] **Type check** : 0 erreurs mypy\n- [ ] **Build** : Réussi\n\n### Documentation\n- [ ] **Swagger** endpoints documentés\n- [ ] **README module** créé/mis à jour\n\n---\n\n## Integration avec DEV_AGENT\n\n### Workflow Complet\n\n```\n1. DEV_AGENT reçoit tâche (ex: TASK-P2-007)\n   ↓\n2. DEV_AGENT lit DEV_WORKFLOW.md (9 étapes)\n   ↓\n3. DEV_AGENT invoque Backend Dev Skill (ce skill)\n   ↓\n4. Backend Dev Skill fournit :\n   - Patterns 3-tiers\n   - Templates code\n   - Références documentation\n   ↓\n5. DEV_AGENT implémente selon patterns\n   ↓\n6. DEV_AGENT génère rapport (.agent/Reports/PHASE_X/)\n   ↓\n7. Go/No-Go Validator valide (invoque TEST_AGENT)\n```\n\n**Ce skill ne fait PAS** :\n- ❌ Workflow développement (c'est DEV_WORKFLOW.md)\n- ❌ Git operations (c'est DEV_WORKFLOW.md)\n- ❌ Génération rapports (c'est DEV_AGENT)\n- ❌ Validation (c'est Go/No-Go Validator)\n\n**Ce skill fait** :\n- ✅ Fournit patterns techniques\n- ✅ Référence documentation\n- ✅ Templates code\n- ✅ Best practices backend\n\n---\n\n## References\n\n### Agents & Workflows\n- `.claude/.agent/Tasks/DEV_AGENT.md` - Agent développement\n- `.claude/.agent/SOP/DEV_WORKFLOW.md` - Workflow 9 étapes\n- `.claude/.agent/SOP/CODE_STANDARDS.md` - Standards code\n- `.claude/.agent/SOP/TEST_WORKFLOW.md` - Tests\n\n### Documentation Backend\n- `.github/docs-internal/Documentations/Backend/API_REFERENCE.md`\n- `.github/docs-internal/Documentations/Backend/ARCHITECTURE.md`\n- `.github/docs-internal/Documentations/Backend/DATABASE_SCHEMA.md`\n- `.github/docs-internal/Documentations/Backend/ERROR_HANDLING.md`\n- `.github/docs-internal/Documentations/Backend/AUTHENTICATION.md`\n\n### Templates\n- `templates/endpoint_template.py` - Route FastAPI\n- `templates/service_template.py` - Service métier\n- `templates/repository_template.py` - Repository DB\n\n### Sources (Règle 0)\n1. `database/schema.sql` - Schéma DB\n2. `packages/backend/.env` - Configuration\n3. `.github/docs-internal/Documentations/Backend/` - Documentation\n4. `packages/backend/app/` - Code existant\n\n---\n\n## Success Criteria\n\nUne implémentation backend est réussie si :\n- ✅ Architecture 3-tiers strictement respectée\n- ✅ Pydantic validation complète\n- ✅ Error handling RFC 7807\n- ✅ RBAC implémenté\n- ✅ Tests coverage >85%\n- ✅ 0 erreurs lint/type\n- ✅ Documentation Swagger complète\n- ✅ Sources (Règle 0) vérifiées\n\n---\n\n## Example Usage\n\n**Scenario** : DEV_AGENT implémente TASK-P2-007 (Endpoint création déclaration)\n\n**DEV_AGENT actions** :\n1. Lit TASK-P2-007 définition\n2. Invoque Backend Dev Skill\n3. Backend Dev Skill retourne :\n   - Pattern Route (template endpoint)\n   - Pattern Service (template service)\n   - Pattern Repository (template repository)\n   - Référence `.github/docs-internal/Documentations/Backend/API_REFERENCE.md`\n4. DEV_AGENT implémente selon patterns :\n   - `app/api/v1/declarations.py` (route)\n   - `app/services/declaration_service.py` (service)\n   - `app/database/repositories/declaration_repository.py` (repository)\n5. DEV_AGENT écrit tests (>85% coverage)\n6. DEV_AGENT génère rapport\n\n**Backend Dev Skill n'a PAS** :\n- ❌ Créé fichiers (c'est DEV_AGENT)\n- ❌ Exécuté tests (c'est TEST_AGENT via Go/No-Go)\n- ❌ Généré rapport (c'est DEV_AGENT)\n\n**Backend Dev Skill a** :\n- ✅ Fourni patterns/templates\n- ✅ Référencé documentation\n- ✅ Guidé architecture\n\n---\n\n**Skill created by:** TaxasGE Backend Team  \n**Date:** 2025-10-31  \n**Version:** 2.0.0  \n**Status:** ✅ READY FOR USE\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "KouemouSah",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/222217803?v=4",
      "license": "NOASSERTION",
      "lastCommit": "2026-01-04T13:00:20Z",
      "createdAt": "2025-07-22T11:32:35Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:06.985Z"
  },
  {
    "id": "supabase-skill",
    "source": {
      "type": "github-search",
      "repo": "boutchaz/agritech",
      "path": ".claude/skills/supabase-skill/SKILL.MD",
      "url": "https://github.com/boutchaz/agritech/blob/main/.claude/skills/supabase-skill/SKILL.MD"
    },
    "frontmatter": {},
    "content": "# Supabase Skill for Agritech Project\n\n## Core Principle\n\n**ALWAYS consolidate all database changes in `project/supabase/migrations/00000000000000_schema.sql`**\n\nThis is a single-file migration approach where all schema changes (tables, functions, triggers, RLS policies, etc.) are maintained in one comprehensive migration file.\n\n## Project Structure\n\n```\nproject/\n├── supabase/\n│   └── migrations/\n│       └── 00000000000000_schema.sql  # Single consolidated schema file\n└── src/\n    └── ... (React frontend)\n```\n\n## Migration Strategy\n\n### Single File Approach\n- All database schema changes go into `00000000000000_schema.sql`\n- This file contains the complete, idempotent database schema\n- All CREATE statements use `IF NOT EXISTS`, `OR REPLACE`, or `DO $$ BEGIN ... EXCEPTION ... END $$` patterns\n- The file is self-contained and can be run multiple times safely\n\n### Idempotency Requirements\n- **Tables**: Use `CREATE TABLE IF NOT EXISTS`\n- **Functions**: Use `CREATE OR REPLACE FUNCTION`\n- **Types/ENUMs**: Use `DO $$ BEGIN ... EXCEPTION WHEN duplicate_object THEN null; END $$`\n- **Indexes**: Use `CREATE INDEX IF NOT EXISTS`\n- **Triggers**: Use `DROP TRIGGER IF EXISTS` before `CREATE TRIGGER`\n- **Policies**: Use `DROP POLICY IF EXISTS` before `CREATE POLICY`\n\n## Schema Organization\n\nThe schema file is organized into sections:\n\n1. **Extensions** - PostGIS, UUID generation\n2. **Helper Functions** - Utility functions like `update_updated_at_column()`\n3. **ENUM Types** - All custom types (quote_status, invoice_status, etc.)\n4. **Core Tables** - Organizations, users, profiles\n5. **Farm Management** - Farms, parcels\n6. **Billing Cycle** - Quotes, sales orders, purchase orders\n7. **Accounting** - Accounts, journal entries, invoices, payments\n8. **Workers & Tasks** - Workers, tasks, work units\n9. **Harvest & Delivery** - Harvest records, deliveries\n10. **Inventory & Stock** - Items, warehouses, stock entries\n11. **Satellite Data** - Satellite indices, processing jobs\n12. **Analyses & Reports** - Soil analyses, reports\n13. **RLS Policies** - Row Level Security policies\n14. **Triggers** - Database triggers\n15. **Data Seeding** - Initial data setup\n\n## RLS (Row Level Security) Patterns\n\n### Helper Function\nAll RLS policies use the `is_organization_member()` helper function:\n\n```sql\nCREATE OR REPLACE FUNCTION is_organization_member(p_organization_id UUID)\nRETURNS BOOLEAN\nLANGUAGE sql\nSECURITY DEFINER\nSTABLE\nSET search_path = public\nAS $$\n  SELECT EXISTS (\n    SELECT 1 \n    FROM public.organization_users \n    WHERE user_id = auth.uid() \n      AND organization_id = p_organization_id\n      AND is_active = true\n  );\n$$;\n```\n\n### Policy Naming Convention\n- Read: `org_read_{table_name}`\n- Write/Insert: `org_write_{table_name}`\n- Update: `org_update_{table_name}`\n- Delete: `org_delete_{table_name}`\n- All operations: `org_access_{table_name}` (for child tables)\n\n### Policy Pattern\n```sql\nDROP POLICY IF EXISTS \"org_read_{table}\" ON {table};\nCREATE POLICY \"org_read_{table}\" ON {table}\n  FOR SELECT USING (\n    is_organization_member(organization_id)\n  );\n```\n\n## Common Patterns\n\n### Organization-Scoped Tables\nAll main tables include `organization_id` and RLS policies:\n```sql\nCREATE TABLE IF NOT EXISTS {table_name} (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,\n  -- other columns\n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n### Updated_at Triggers\nFor tables with `updated_at` columns:\n```sql\nDROP TRIGGER IF EXISTS trg_{table}_updated_at ON {table};\nCREATE TRIGGER trg_{table}_updated_at\n  BEFORE UPDATE ON {table}\n  FOR EACH ROW\n  EXECUTE FUNCTION update_updated_at_column();\n```\n\n### Number Generation Functions\nUse helper functions for generating sequential numbers:\n- `generate_quote_number(p_organization_id UUID)`\n- `generate_sales_order_number(p_organization_id UUID)`\n- `generate_invoice_number(p_organization_id UUID, p_invoice_type invoice_type)`\n\n## When Making Changes\n\n### Adding a New Table\n1. Add table definition in appropriate section\n2. Add indexes\n3. Enable RLS: `ALTER TABLE IF EXISTS {table} ENABLE ROW LEVEL SECURITY;`\n4. Add RLS policies (read, write, update, delete)\n5. Add `updated_at` trigger if needed\n6. Add to appropriate section comment\n\n### Modifying Existing Table\n1. Use `ALTER TABLE IF EXISTS` for schema changes\n2. Ensure idempotency (check if column/index exists first)\n3. Update related RLS policies if needed\n4. Update triggers if needed\n\n### Adding Functions\n1. Use `CREATE OR REPLACE FUNCTION`\n2. Add `SECURITY DEFINER` if it needs to bypass RLS\n3. Set `SET search_path = public` for security\n4. Add `GRANT EXECUTE` statements\n5. Add comments with `COMMENT ON FUNCTION`\n\n### Adding RLS Policies\n1. Always `DROP POLICY IF EXISTS` first\n2. Use consistent naming: `org_{operation}_{table}`\n3. Use `is_organization_member()` for organization-scoped tables\n4. For child tables, check parent table membership\n\n## Data Seeding\n\nInitial data setup is done in the same file:\n- Default currencies\n- Default roles\n- User profile synchronization\n- Organization setup\n- Subscription setup\n\nUse `DO $$ ... END $$` blocks for conditional seeding.\n\n## Testing Changes\n\nBefore committing:\n1. Verify idempotency - run migration twice, should succeed both times\n2. Check RLS policies - ensure users can only access their organization's data\n3. Verify foreign keys - ensure all references are valid\n4. Test triggers - ensure they fire correctly\n\n## Important Notes\n\n- **Never create separate migration files** - always update `00000000000000_schema.sql`\n- **Always use idempotent statements** - migrations should be safe to run multiple times\n- **RLS is enabled on all tables** - ensure policies are created for every table\n- **Use SECURITY DEFINER carefully** - only for functions that need to bypass RLS\n- **Maintain section organization** - keep related items together\n- **Add comments** - document complex logic and important decisions\n\n## Common Commands\n\n### Apply Migration\n```bash\ncd project\nnpm run db:migrate\n```\n\n### Generate TypeScript Types\n```bash\nnpm run db:generate-types\n```\n\n### Reset Database (Local)\n```bash\nnpm run db:reset\n```\n\n## File Location\n\nAll schema changes must be made in:\n```\nproject/supabase/migrations/00000000000000_schema.sql\n```\n\n**Remember: This is the single source of truth for the database schema.**\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "boutchaz",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/15620015?v=4",
      "lastCommit": "2026-01-04T12:36:52Z",
      "createdAt": "2025-09-06T13:29:39Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:07.624Z"
  },
  {
    "id": "deploy",
    "source": {
      "type": "github-search",
      "repo": "WellFitCommunity/WellFit-Community-Daily-Complete",
      "path": ".claude/skills/deploy/skill.md",
      "url": "https://github.com/WellFitCommunity/WellFit-Community-Daily-Complete/blob/main/.claude/skills/deploy/skill.md"
    },
    "frontmatter": {},
    "content": "# Deployment Checklist Skill\n\n## Purpose\nComprehensive pre-deployment validation to ensure zero-downtime deployments for production and the Methodist Hospital demo (Dec 5th, 2025).\n\n## What This Skill Does\n\nValidates all deployment prerequisites across multiple layers:\n1. **Code Quality** - Linting, types, tests\n2. **Database Migrations** - All migrations applied\n3. **Environment Variables** - Required secrets configured\n4. **Edge Functions** - Supabase functions deployed\n5. **MCP Server** - Claude MCP server operational\n6. **Security** - GPG signing, encryption keys\n7. **HIPAA Compliance** - Audit logs, RLS policies\n8. **Performance** - Bundle size, load times\n9. **Monitoring** - Alerts configured\n\n## Deployment Validation Steps\n\n### Step 1: Code Quality Validation\n```bash\n# Run complete validation suite\nnpm run lint && npm run typecheck && npm test\n```\n\n**Success criteria:**\n- ✅ 0 linting errors\n- ✅ 0 TypeScript errors\n- ✅ 625+ tests passing\n- ✅ 0 critical warnings\n\n### Step 2: Database Migrations Check\n\nVerify all migrations are applied:\n```bash\n# Check for unapplied migrations\nnpx supabase db pull\nnpx supabase migration list\n```\n\n**Validate:**\n- All migrations in `supabase/migrations/` are applied\n- No pending migrations\n- Schema matches production\n\n**Critical tables to verify:**\n- `profiles`\n- `medications`\n- `encounters`\n- `fhir_*` tables\n- `phi_access_logs`\n- `audit_logs`\n- `ai_skill_config`\n\n### Step 3: Environment Variables Check\n\nVerify all required environment variables are set:\n\n**Frontend (.env):**\n```bash\n# Required variables\nREACT_APP_SUPABASE_URL\nREACT_APP_SUPABASE_ANON_KEY\nREACT_APP_HCAPTCHA_SITE_KEY\nREACT_APP_PHI_ENCRYPTION_KEY\n```\n\n**Supabase Secrets:**\n```bash\n# Check secrets are configured\nnpx supabase secrets list\n```\n\n**Required secrets:**\n- `ANTHROPIC_API_KEY`\n- `SUPABASE_SERVICE_ROLE_KEY`\n- `MAILERSEND_API_TOKEN` (if email enabled)\n\n### Step 4: Edge Functions Deployment\n\nVerify Supabase Edge Functions are deployed:\n```bash\n# List deployed functions\nnpx supabase functions list\n```\n\n**Required functions:**\n- `mcp-claude-server` (Claude MCP integration)\n- `ai-billing-suggester` (if AI skills enabled)\n- `ai-readmission-predictor` (if AI skills enabled)\n\n**Test function health:**\n```bash\n# Test MCP server\ncurl -X POST https://YOUR_PROJECT.supabase.co/functions/v1/mcp-claude-server \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_ANON_KEY\" \\\n  -d '{\"method\":\"tools/list\"}'\n```\n\n### Step 5: MCP Server Validation\n\nTest Claude MCP server is operational:\n\n**Check:**\n1. MCP server responds to health check\n2. Available tools: `analyze-text`, `generate-suggestion`, `summarize`\n3. Prompt caching enabled\n4. De-identification working\n5. Audit logging to `claude_usage_logs`\n\n**Test query:**\n```sql\n-- Verify recent MCP usage\nSELECT * FROM claude_usage_logs\nWHERE request_type LIKE 'mcp_%'\nORDER BY created_at DESC\nLIMIT 5;\n```\n\n### Step 6: Security Validation\n\nVerify security controls:\n\n**GPG Commit Signing:**\n```bash\ngit config --get commit.gpgsign  # Should return \"true\"\ngit config --get user.signingkey  # Should return key ID\n```\n\n**Encryption Keys:**\n- PHI encryption key configured\n- Master encryption key secured\n- Backup encryption tested\n\n**RLS Policies:**\n```sql\n-- Count RLS-enabled tables\nSELECT COUNT(*) FROM pg_tables\nWHERE schemaname = 'public'\n  AND rowsecurity = true;\n-- Should be 80+\n```\n\n### Step 7: HIPAA Compliance Check\n\nRun HIPAA compliance scan:\n- No PHI in logs\n- All audit logging active\n- Encryption enabled\n- RLS policies on all PHI tables\n\nReference: Run `/security-scan` command\n\n### Step 8: Performance Validation\n\nCheck bundle size and performance:\n\n```bash\n# Build production bundle\nnpm run build\n\n# Check bundle size\ndu -sh build/static/js/*.js | sort -h\n```\n\n**Targets:**\n- Total bundle size: < 2 MB\n- Main chunk: < 500 KB\n- Vendor chunk: < 800 KB\n- Load time (3G): < 3 seconds\n\n### Step 9: Monitoring & Alerts\n\nVerify monitoring is configured:\n\n**Database Monitoring:**\n```sql\n-- Check security monitoring dashboard\nSELECT * FROM security_monitoring_dashboard;\n```\n\n**Alerts configured for:**\n- Critical security events\n- Failed login attempts (>5)\n- PHI access anomalies\n- Database errors\n- API failures\n\n### Step 10: Backup Verification\n\nVerify backups are working:\n\n**Check:**\n- Last backup timestamp < 24 hours\n- Backup restoration tested (quarterly)\n- Encryption keys backed up\n- Code signed commits in git history\n\n## Methodist Hospital Demo Checklist\n\n**Additional checks for Dec 5th demo:**\n\n### Demo-Specific Features\n- [ ] FHIR integration working\n- [ ] Epic sync functional\n- [ ] Medication Cabinet AI working\n- [ ] Care gap detection active\n- [ ] Quality metrics dashboard ready\n- [ ] Guardian Agent operational\n- [ ] White-label branding configured\n\n### Demo Data\n- [ ] Test patient accounts created\n- [ ] Sample encounters loaded\n- [ ] Medications pre-populated\n- [ ] Care plans generated\n- [ ] Quality metrics showing\n\n### Demo Environment\n- [ ] Production-like environment\n- [ ] HTTPS enabled\n- [ ] Fast load times\n- [ ] Mobile responsive\n- [ ] No console errors\n\n## Output Format\n\n```\n🚀 DEPLOYMENT READINESS CHECK\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[1/9] Code Quality Validation...\n✅ Linting passed (0 errors)\n✅ Type checking passed (0 errors)\n✅ Tests passed (627 passing)\n\n[2/9] Database Migrations...\n✅ All migrations applied (143 total)\n✅ Schema matches production\n\n[3/9] Environment Variables...\n✅ All 4 frontend variables configured\n✅ All 3 Supabase secrets configured\n\n[4/9] Edge Functions Deployment...\n✅ MCP server deployed (v1.2.3)\n✅ AI billing suggester deployed\n✅ AI readmission predictor deployed\n\n[5/9] MCP Server Health...\n✅ MCP server responding\n✅ 3 tools available\n✅ Prompt caching active\n\n[6/9] Security Validation...\n✅ GPG signing enabled (Key: D1578B97AFE4D408)\n✅ PHI encryption key configured\n✅ RLS enabled on 87 tables\n\n[7/9] HIPAA Compliance...\n✅ No PHI logging violations\n✅ Audit logging active\n✅ All security controls passing\n\n[8/9] Performance Check...\n✅ Bundle size: 1.4 MB (target: <2 MB)\n✅ Main chunk: 423 KB\n✅ Load time: 2.1s (target: <3s)\n\n[9/9] Monitoring & Alerts...\n✅ Security dashboard active\n✅ Alerts configured (5 critical rules)\n✅ Last backup: 6 hours ago\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n✅ DEPLOYMENT READY\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSummary:\n  ✅ Code Quality: Passing\n  ✅ Database: 143 migrations applied\n  ✅ Security: All controls active\n  ✅ Performance: 1.4 MB bundle\n  ✅ Monitoring: Configured\n\n🟢 SAFE TO DEPLOY TO PRODUCTION\n\nNext Steps:\n  1. Create deployment tag: git tag v1.2.3\n  2. Push to production branch\n  3. Monitor deployment logs\n  4. Run post-deployment smoke tests\n```\n\n## Failure Output Format\n\n```\n🚀 DEPLOYMENT READINESS CHECK\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[1/9] Code Quality Validation...\n✅ Linting passed\n✅ Type checking passed\n❌ Tests failed (3 failures)\n\nFailed Tests:\n  ✗ MedicationService › should encrypt PHI\n  ✗ FhirService › should sync with Epic\n  ✗ BillingService › should calculate CCM time\n\n[2/9] Database Migrations...\n⚠️ WARNING: 2 unapplied migrations\n\nUnapplied:\n  - 20251116000000_add_demo_features.sql\n  - 20251116120000_update_rls_policies.sql\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n❌ DEPLOYMENT BLOCKED\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nCritical Issues:\n  ❌ 3 failing tests\n  ⚠️ 2 unapplied migrations\n\n🔴 DO NOT DEPLOY\n\nRequired Actions:\n  1. Fix failing tests\n  2. Apply pending migrations: npx supabase db push\n  3. Re-run deployment check\n  4. Get approval before deploying\n```\n\n## When to Use This Skill\n\n**Pre-Deployment:**\n- Before every production deployment\n- Before staging deployments\n- Before demo environment setup\n\n**Methodist Demo:**\n- Nov 30th - Final validation\n- Dec 1st - Demo environment check\n- Dec 4th - Pre-demo validation\n- Dec 5th - Morning of demo\n\n**Regular Schedule:**\n- Weekly deployments\n- After major features\n- After security updates\n\n## Rollback Plan\n\nIf deployment check fails after deployment:\n\n1. **Immediate actions:**\n   - Revert to previous version\n   - Restore database backup\n   - Notify team\n\n2. **Investigation:**\n   - Review deployment logs\n   - Check error reports\n   - Run this skill again\n\n3. **Fix and redeploy:**\n   - Address root cause\n   - Re-run deployment check\n   - Deploy with approval\n\n## Notes for AI Agent\n\n- Run ALL 9 steps (don't skip any)\n- Block deployment if ANY critical check fails\n- Warnings are okay, errors are not\n- Show detailed failure reasons\n- Provide clear remediation steps\n- Track deployment readiness score\n- Suggest rollback plan if needed\n- Cross-reference with Methodist demo date\n",
    "github": {
      "stars": 1,
      "forks": 0,
      "owner": "WellFitCommunity",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/207092865?v=4",
      "license": "MIT",
      "lastCommit": "2026-01-04T03:56:13Z",
      "createdAt": "2025-04-17T00:08:32Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:08.290Z"
  },
  {
    "id": "notion-tasks",
    "source": {
      "type": "github-search",
      "repo": "oaknational/oak-ai-lesson-assistant",
      "path": ".claude/skills/notion-tasks/SKILL.md",
      "url": "https://github.com/oaknational/oak-ai-lesson-assistant/blob/main/.claude/skills/notion-tasks/SKILL.md"
    },
    "frontmatter": {
      "name": "notion-tasks",
      "description": "Query and manage Oak AI squad tasks and sprints in Notion. Use when checking current sprint, finding tasks by status, updating task information, or understanding team priorities and workload.",
      "allowed-tools": "mcp__notionApi__API-post-database-query, mcp__notionApi__API-retrieve-a-page, mcp__notionApi__API-get-block-children"
    },
    "content": "# Notion Task Management\n\n## Setup\n\nDatabase IDs are stored in `config.local.md` (not checked into git). See `config.local.md.example` for the required format.\n\n## Finding Current Sprint Tasks\n\n1. **Find current sprint**: Query sprints database with filter:\n\n   ```json\n   { \"property\": \"Sprint status\", \"status\": { \"equals\": \"Current\" } }\n   ```\n\n2. **Get sprint ID** from the result (e.g., `13d26cc4-e1b1-8108-9fb3-d74429e520c6`)\n\n3. **Query tasks** with filter:\n   ```json\n   { \"property\": \"Sprint\", \"relation\": { \"contains\": \"SPRINT_ID\" } }\n   ```\n\n## Efficient Querying with filter_properties\n\n- **Essential fields**: `[\"title\", \"pPPj\", \"FH@y\", \"RIV<\", \"pIdo\"]`\n  - These map to: Task, Status, Who, Type, ID\n- **Use filter_properties**: Reduces response size significantly\n- **Page size**: Can use 50-100 when filtering properties\n\n**Example query:**\n\n```json\n{\n  \"database_id\": \"TASKS_DB_ID\",\n  \"filter\": {\n    \"property\": \"Sprint\",\n    \"relation\": { \"contains\": \"SPRINT_ID\" }\n  },\n  \"filter_properties\": [\"title\", \"pPPj\", \"FH@y\", \"RIV<\", \"pIdo\"],\n  \"page_size\": 50\n}\n```\n\n## Common Task Filters\n\n- **In progress**: `{\"property\": \"Status\", \"status\": {\"equals\": \"In progress\"}}`\n- **Ready tasks**: `{\"property\": \"Status\", \"status\": {\"equals\": \"Ready\"}}`\n- **Ready for review**: `{\"property\": \"Status\", \"status\": {\"equals\": \"Ready for review\"}}`\n- **Current sprint tasks**: First get current sprint ID, then filter by Sprint relation\n\n## Task Status Meanings\n\n- **Ready**: Available to pick up and start development\n- **Backlog**: Not ready for development yet, needs planning/refinement\n- **Candidate**: Being evaluated, not ready for development\n- **Triage**: Needs assessment before development\n- **In progress**: Currently being worked on\n- **Ready for review**: Code complete, awaiting review\n- **Merged**: Code merged to main branch\n- **Done**: Task completed\n- **Blocked**: Cannot proceed due to dependencies\n",
    "github": {
      "stars": 29,
      "forks": 3,
      "owner": "oaknational",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/63889701?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-18T18:53:27Z",
      "createdAt": "2024-08-21T08:30:05Z",
      "topics": [
        "ai",
        "education",
        "mit-license"
      ]
    },
    "scrapedAt": "2026-01-04T13:03:08.954Z"
  },
  {
    "id": "website-builder",
    "source": {
      "type": "github-search",
      "repo": "Manuelvillarvieites/local-studios-website",
      "path": ".claude/skills/website-builder/Skill.md",
      "url": "https://github.com/Manuelvillarvieites/local-studios-website/blob/main/.claude/skills/website-builder/Skill.md"
    },
    "frontmatter": {
      "name": "website-builder",
      "description": "Complete SEO-optimized website creation workflow orchestrator. Coordinates SEO analysis (keywords, competitors), sitemap planning, shadcn component selection, theme application, and content optimization. Use when building websites, creating landing pages, designing web interfaces, optimizing for search engines, planning site architecture, or requesting homepage/website builds with SEO considerations.",
      "version": "1.0.0",
      "dependencies": "sitemap-analyst agent, shadcn-ui-blocks skill, shadcn-ui-theme skill, shadcn-website-builder agent, WebFetch",
      "allowed-tools": "Read,Write,Edit,Bash,WebFetch"
    },
    "content": "# SEO-Optimized Website Builder\n\nOrchestrates complete website creation workflow: SEO analysis → sitemap planning → component selection → theme application → SEO integration → verification.\n\n---\n\n## 5-Step Workflow\n\n### Step 1: SEO Analysis\n**Create SEO research foundation (if nothing specified by user):**\n\n1. Create `/planning/seo/` directory\n2. Analyze target website (if provided via WebFetch):\n   - Extract current SEO metadata\n   - Identify primary/secondary keywords\n   - Analyze page structure, headings, content\n3. Create `/planning/seo/analysis.md`:\n   - Current state assessment\n   - SEO audit findings\n   - Key opportunities\n4. Create `/planning/seo/keywords.md`:\n   - Primary keywords (search volume, difficulty)\n   - Secondary keywords\n   - Long-tail variations\n   - Keyword mapping to pages\n5. Create `/planning/seo/competitors.md`:\n   - Top 3-5 competitors\n   - Their keyword focus\n   - Content gaps you can fill\n   - Unique positioning strategy\n\n**Use WebFetch when user provides target URL:**\n```\nFetch: <user-provided-url>\nPrompt: \"Extract SEO metadata, primary keywords, page structure, content themes\"\n```\n\n### Step 2: Sitemap Creation\n**Delegate to sitemap-analyst agent:**\n\nCall sitemap-analyst with:\n- Business goals (from user or inferred)\n- Target audience personas\n- Primary conversion objectives\n- Use only sections from shadcn-ui-blocks skill\n- Constraints/preferences\n\nAgent outputs `/planning/sitemap.yaml` with:\n- Page hierarchy (path, title, priority)\n- Purpose & SEO focus per page\n- CMS collections (if applicable)\n- Internal linking strategy\n- User journey mapping\n\nExample invocation:\n```\nUse sitemap-analyst agent:\nContext: [Business goals, audience, conversion targets]\nOutput destination: /planning/sitemap.yaml\n```\n\n### Step 3: Website Build\n**Select & configure components:**\n\n**IMPORTANT:** The sitemap-analyst from Step 2 decides the pages and structure. You only implement the design and SEO aspects here.\n\n1. Review sitemap structure\n2. For each page type, use **shadcn-ui-blocks** skill:\n   - Homepage → hero1-50 + feature + pricing/cta\n   - Feature pages → feature1-266 (largest category)\n   - Pricing → pricing1-35\n   - Testimonials → testimonial1-28\n   - FAQ → faq1-16\n   - Blog → blog1-22 + blogpost1-6\n   - Contact → contact1-17\n   - Footer → footer1-21\n   - Navigation → navbar1-16\n\n3. Use **shadcn-ui-theme** skill:\n   - Select theme matching brand\n   - Apply CSS variables\n   - Test light/dark modes\n   - Verify accessibility\n\n4. Coordinate with **shadcn-website-builder agent**:\n   - Provide sitemap + component selections\n   - Get built pages with responsive layouts\n   - Review and refine\n\n### Step 4: SEO Integration\n**Optimize for search engines:**\n\n1. **Meta Tags**:\n   - Title (50-60 chars): Primary keyword + brand\n   - Description (150-160 chars): Call-to-action + benefit\n   - Canonical URLs (prevent duplicates)\n   - Open Graph (social sharing)\n\n2. **Structured Data**:\n   - JSON-LD Schema.org markup\n   - Organization schema (homepage)\n   - BreadcrumbList (navigation)\n   - Product schema (if applicable)\n   - FAQPage schema (FAQ sections)\n\n3. **Content Optimization**:\n   - H1: Single, primary keyword\n   - H2-H4: Secondary keywords, natural flow\n   - Image alt text: Descriptive, keyword-relevant\n   - Internal links: Anchor text with keywords\n   - Meta descriptions: Unique per page\n\n4. **Performance Optimization**:\n   - Core Web Vitals (LCP, FID, CLS)\n   - Image optimization (next/image component)\n   - CSS/JS minification\n   - Mobile responsiveness verification\n   - Page speed analysis\n\n5. **Technical SEO**:\n   - XML sitemap: `/sitemap.xml` list of all pages\n   - robots.txt: Control crawler access\n   - Canonical tags: Prevent duplicate content\n   - Mobile-first indexing: Responsive design\n   - No broken links (404 audit)\n\n### Step 5: Verification\n**Test implementation:**\n\n1. **Page Testing**:\n   - Load all pages in browser\n   - Verify responsive on mobile/tablet/desktop\n   - Check navigation, links, forms work\n   - Validate HTML (W3C validator)\n\n2. **SEO Verification**:\n   - Meta tags present and unique (view source)\n   - Structured data valid (schema.org validator)\n   - Robots.txt accessible\n   - Sitemap.xml valid XML\n   - Page titles/descriptions follow best practices\n\n3. **Performance Check**:\n   - Google PageSpeed Insights score\n   - Core Web Vitals status\n   - Mobile usability test\n   - Accessibility audit (WCAG 2.1 AA)\n\n4. **Content Audit**:\n   - Keyword presence verified (tool or manual)\n   - H1-H6 hierarchy correct\n   - Image alt text present\n   - Internal links functioning\n   - CTA buttons visible and clickable\n\n---\n\n## SEO Best Practices\n\n### Meta Tags\n```html\n<head>\n  <title>Primary Keyword | Brand (50-60 chars)</title>\n  <meta name=\"description\" content=\"Action-oriented description with secondary keyword (150-160 chars)\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  <link rel=\"canonical\" href=\"https://example.com/page\">\n  <meta property=\"og:title\" content=\"Social title\">\n  <meta property=\"og:description\" content=\"Social description\">\n  <meta property=\"og:image\" content=\"Social image URL\">\n</head>\n```\n\n### Structured Data (JSON-LD)\n```json\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"Organization\",\n  \"name\": \"Brand Name\",\n  \"url\": \"https://example.com\",\n  \"logo\": \"https://example.com/logo.png\",\n  \"sameAs\": [\n    \"https://twitter.com/brand\",\n    \"https://linkedin.com/company/brand\"\n  ]\n}\n```\n\n### Content Structure\n- **H1**: One per page, primary keyword\n- **H2-H4**: Secondary keywords, logical hierarchy\n- **Paragraphs**: 2-4 sentences, natural keyword integration\n- **Images**: Descriptive alt text, WebP format, lazy loading\n- **Internal Links**: 3-5 contextually relevant per page\n\n### Core Web Vitals Targets\n- **Largest Contentful Paint (LCP)**: < 2.5 seconds\n- **First Input Delay (FID)**: < 100ms (or Interaction to Next Paint INP < 200ms)\n- **Cumulative Layout Shift (CLS)**: < 0.1\n\n### Mobile SEO\n- Responsive design (no fixed widths)\n- Touch-friendly buttons (44x44px minimum)\n- Readable font size (16px base minimum)\n- Avoid intrusive interstitials\n- Fast mobile page load\n\n---\n\n## Keywords to Trigger This Skill\n\nPrimary: website, SEO, landing page, web design, sitemap, homepage, site builder, web development\nSecondary: website creation, website build, web page design, search optimization, site structure, information architecture, web layout, digital presence\n\n---\n\n## Integration Summary\n\n| Component | Purpose | Link |\n|-----------|---------|------|\n| sitemap-analyst | Site structure & IA | Use when designing page hierarchy |\n| shadcn-ui-blocks | 959 pre-built components | Use for visual pages |\n| shadcn-ui-theme | 17 color themes | Use for design system |\n| shadcn-website-builder | Implementation agent | Coordinates build phase |\n| WebFetch | Target site analysis | Use when user provides URL |\n\n---\n\n## Example Request Triggers\n\n- \"Build me a website with SEO\"\n- \"Create a landing page optimized for search\"\n- \"Design a website for my SaaS product\"\n- \"Help me structure my site for better rankings\"\n- \"Build a homepage that converts and ranks\"\n- \"Create a website with proper site architecture\"\n- \"Design a landing page with sitemap planning\"\n\n---\n\n## Output Artifacts\n\nAfter workflow completion, user receives:\n\n1. `/planning/seo/analysis.md` - SEO research summary\n2. `/planning/seo/keywords.md` - Keyword strategy & targeting\n3. `/planning/seo/competitors.md` - Competitive analysis\n4. `/planning/sitemap.yaml` - Site structure (YAML format)\n5. Website code with:\n   - Responsive shadcn components\n   - Applied theme\n   - Optimized meta tags\n   - Structured data\n   - Performance-tuned images\n   - Internal linking strategy\n\n---\n\n**Note:** This is an orchestrating skill. It delegates analysis to sitemap-analyst and building to shadcn-website-builder + component skills. Your role is coordinating the workflow, not implementing every step directly.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "Manuelvillarvieites",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/107494014?v=4",
      "lastCommit": "2025-11-06T12:21:38Z",
      "createdAt": "2025-11-06T12:21:33Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:09.636Z"
  },
  {
    "id": "api-endpoint-guardian",
    "source": {
      "type": "github-search",
      "repo": "mkelam/PDFLab",
      "path": ".claude/skills/api-endpoint-guardian.SKILL.md",
      "url": "https://github.com/mkelam/PDFLab/blob/main/.claude/skills/api-endpoint-guardian.SKILL.md"
    },
    "frontmatter": {
      "name": "api-endpoint-guardian",
      "description": "Prevents API endpoint failures through systematic design validation and error handling review. Use when adding new endpoints, debugging API errors, implementing authentication, or preparing for production deployment. Catches missing validation, inconsistent error responses, authentication gaps, rate limiting issues, and documentation problems."
    },
    "content": "# API Endpoint Guardian\n\n**Mission:** Ensure API endpoints are secure, consistent, well-documented, and production-ready through offensive validation and best practice enforcement.\n\n## Activation Triggers\n\n- Adding new API endpoints\n- API design reviews\n- Authentication/authorization issues\n- Error handling inconsistencies\n- Rate limiting implementation\n- API documentation needs\n- Client integration errors\n- Production API failures\n\n## Framework Awareness\n\nThis skill understands:\n- **Express.js** - Routing, middleware, error handling\n- **TypeScript** - Type safety for requests/responses\n- **REST** - Resource naming, HTTP methods, status codes\n- **Authentication** - JWT, session, OAuth patterns\n- **Validation** - Request/response schemas\n\n## Scan Methodology\n\n### 1. Initial Context Gathering\n\n**Ask if not provided:**\n- \"Show me the new endpoint code (controller + route)\"\n- \"What's the endpoint purpose?\" (CRUD, action, query)\n- \"Who can access this?\" (public, authenticated, admin)\n- \"What request/response formats are expected?\"\n- \"Are there rate limits or quotas?\"\n\n### 2. Critical Endpoint Scan\n\nExecute ALL checks in this section.\n\n#### 🔴 CRITICAL: Route Definition & HTTP Method\n\n**Historical Failure:** Used GET for delete action, browser prefetch deleted data\n\n**Scan for:**\n- [ ] HTTP method matches action (GET=read, POST=create, PUT/PATCH=update, DELETE=delete)\n- [ ] Resource naming follows REST conventions (`/api/users/:id` not `/api/get-user`)\n- [ ] Plural nouns for collections (`/api/batch/upload` → `/api/batches`)\n- [ ] Nested resources show relationships (`/api/users/:userId/batches`)\n- [ ] No verbs in URL (action in HTTP method, not path)\n\n**Red flags:**\n```typescript\n// ❌ WRONG METHOD - Deletes on GET\nrouter.get('/api/delete-user/:id', deleteUser)\n\n// ❌ WRONG METHOD - Creates on GET\nrouter.get('/api/create-job', createJob)\n\n// ❌ VERB IN URL - Should be POST /api/users\nrouter.post('/api/create-user', createUser)\n\n// ❌ SINGULAR NOUN - Should be /api/batches\nrouter.post('/api/batch/upload', uploadBatch)\n\n// ❌ INCONSISTENT - Should nest under /api/batches/:id\nrouter.get('/api/batch-status/:id', getBatchStatus)\n```\n\n**Optimization:**\n```typescript\n// ✅ CORRECT - Idempotent GET\nrouter.get('/api/batches/:id', getBatchStatus)\n\n// ✅ CORRECT - POST for creation\nrouter.post('/api/batches', uploadBatch)\n\n// ✅ CORRECT - DELETE for deletion\nrouter.delete('/api/batches/:id', cancelBatch)\n\n// ✅ CORRECT - Nested resource\nrouter.get('/api/users/:userId/batches', getUserBatches)\n\n// ✅ CORRECT - Resource naming\nrouter.get('/api/batches/:id/download', downloadBatchZip)\n```\n\n#### 🔴 CRITICAL: Authentication & Authorization\n\n**Historical Failure:** Forgot auth middleware, anyone could delete users\n\n**Scan for:**\n- [ ] Authentication middleware applied to protected routes\n- [ ] Authorization checks for user-specific resources\n- [ ] Admin-only routes have role/permission checks\n- [ ] Public routes explicitly marked (no middleware = intentional?)\n- [ ] User ID from token matches resource owner\n- [ ] Guest access clearly defined\n\n**Authentication Patterns:**\n```typescript\n// ❌ MISSING AUTH - Anyone can delete\nrouter.delete('/api/batches/:id', cancelBatch)\n\n// ❌ NO OWNERSHIP CHECK - User A can delete User B's batch\nexport const cancelBatch = async (req: Request, res: Response) => {\n  const batch = await BatchJob.findByPk(req.params.id)\n  await batch.destroy()  // No check if req.user.id === batch.user_id\n}\n\n// ✅ AUTH REQUIRED\nrouter.delete('/api/batches/:id', authMiddleware, cancelBatch)\n\n// ✅ OWNERSHIP VERIFICATION\nexport const cancelBatch = async (req: Request, res: Response) => {\n  const batch = await BatchJob.findByPk(req.params.id)\n\n  if (!batch) {\n    return res.status(404).json({ error: 'Batch not found' })\n  }\n\n  if (batch.user_id !== req.user!.id) {\n    return res.status(403).json({ error: 'Access denied' })\n  }\n\n  await batch.destroy()\n}\n\n// ✅ ADMIN ONLY\nrouter.get('/api/admin/users', authMiddleware, requireAdmin, getUsers)\n\n// ✅ PUBLIC (Explicitly documented)\n// No auth required - PayFast webhook\nrouter.post('/api/payfast/webhook', payfastWebhook)\n```\n\n#### 🔴 CRITICAL: CORS Configuration (PRODUCTION DEPLOYMENT MANDATORY)\n\n**Historical Failure:** v1.1.1 hotfix (Nov 9, 2025) - CORS missing production domain caused 100% authentication failure for 15 minutes\n\n**Production Lesson Learned**: Deployed to production without including production domain (https://pdflab.pro) in CORS origins. All API calls from frontend were blocked with \"Not allowed by CORS\" error. Required emergency hotfix v1.1.1.\n\n**⚠️ MANDATORY: CORS Pre-Deployment Checklist (NON-NEGOTIABLE)**\n\n**Before ANY Production Deployment:**\n```bash\n# Step 1: List ALL CORS origins in server.ts\n□ localhost:3000 (development)\n□ localhost:3002 (alternative dev port)\n□ https://pdflab.pro (PRODUCTION - MANDATORY)\n□ http://pdflab.pro (production HTTP fallback)\n□ https://staging.pdflab.pro (staging - if exists)\n\n# Step 2: Verify CORS configuration code\n□ Check CORS_ORIGIN environment variable includes production domain\n□ Verify CORS middleware applied BEFORE routes\n□ Test CORS preflight OPTIONS requests\n□ Check CORS credentials setting matches frontend\n\n# Step 3: Test from production domain BEFORE deployment\n□ Use browser DevTools → Network tab\n□ Check for \"Access-Control-Allow-Origin\" header in response\n□ Verify no CORS errors in console\n□ Test API call from https://pdflab.pro\n\n# Step 4: Production verification (AFTER deployment)\n□ Open https://pdflab.pro in browser\n□ Try login from production domain\n□ Check Network tab for CORS errors\n□ Verify API calls succeed (not 500 errors)\n```\n\n**Real Production Issue (Nov 9, 2025):**\n```typescript\n// ❌ BROKEN: Missing production domain (v1.1.0)\nconst corsOrigins = process.env['CORS_ORIGIN']?.split(',') || [\n  'http://localhost:3000',\n  'http://localhost:3002'\n  // Missing: 'https://pdflab.pro'\n]\n\napp.use(cors({\n  origin: (origin, callback) => {\n    if (corsOrigins.includes(origin) || process.env.NODE_ENV === 'development') {\n      callback(null, true)\n    } else {\n      callback(new Error('Not allowed by CORS'))  // ← Blocks production!\n    }\n  }\n}))\n\n// Result: 100% authentication failure in production\n// Error: \"Error: Not allowed by CORS\" (500 status)\n// Impact: All users blocked for 15 minutes until v1.1.1 hotfix\n\n// ✅ FIXED: Production domains included (v1.1.1)\nconst corsOrigins = process.env['CORS_ORIGIN']?.split(',') || [\n  'http://localhost:3000',\n  'http://localhost:3002',\n  'https://pdflab.pro',      // ← MANDATORY for production\n  'http://pdflab.pro'        // ← HTTP fallback\n]\n\napp.use(cors({\n  origin: (origin, callback) => {\n    if (corsOrigins.includes(origin)) {\n      callback(null, true)\n    } else {\n      console.warn(`[CORS] Blocked request from origin: ${origin}`)\n      callback(new Error('Not allowed by CORS'))\n    }\n  },\n  credentials: true  // If using cookies/auth headers\n}))\n\n// Result: Production working, authentication successful\n```\n\n**CORS Testing Commands:**\n```bash\n# Test CORS from production domain\ncurl -H \"Origin: https://pdflab.pro\" \\\n     -H \"Access-Control-Request-Method: POST\" \\\n     -H \"Access-Control-Request-Headers: Content-Type,Authorization\" \\\n     -X OPTIONS \\\n     https://pdflab.pro/api/auth/login\n\n# Expected response headers:\nAccess-Control-Allow-Origin: https://pdflab.pro\nAccess-Control-Allow-Methods: GET,POST,PUT,DELETE\nAccess-Control-Allow-Headers: Content-Type,Authorization\nAccess-Control-Allow-Credentials: true\n\n# If missing → CORS NOT CONFIGURED CORRECTLY\n```\n\n#### 🔴 CRITICAL: Express Trust Proxy (PRODUCTION DEPLOYMENT MANDATORY)\n\n**Historical Failure:** v1.1.1 hotfix (Nov 9, 2025) - Trust proxy not enabled caused rate limiter ValidationError\n\n**Production Lesson Learned**: Production uses Nginx reverse proxy, but Express `trust proxy` setting was false. This caused express-rate-limit to throw ValidationError because X-Forwarded-For header was set but Express didn't trust it. Rate limiting broken in production.\n\n**⚠️ MANDATORY: Trust Proxy Configuration (NON-NEGOTIABLE for Production)**\n\n**Before ANY Deployment Behind Reverse Proxy (Nginx/Apache/CloudFlare):**\n```typescript\n// Step 1: Enable trust proxy IMMEDIATELY after creating Express app\nconst app = express()\nconst PORT = parseInt(process.env.PORT || '3006')\n\n// ✅ CRITICAL: Enable trust proxy for production (behind Nginx)\napp.set('trust proxy', true)  // ← MANDATORY for reverse proxy deployments\n\n// Step 2: Verify rate limiting works\nconst apiLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000,\n  max: 100,\n  standardHeaders: true,\n  legacyHeaders: false,\n  // Now works correctly with X-Forwarded-For header\n})\n\napp.use('/api/', apiLimiter)\n```\n\n**Real Production Issue (Nov 9, 2025):**\n```typescript\n// ❌ BROKEN: Trust proxy not enabled (v1.1.0)\nconst app = express()\n// Missing: app.set('trust proxy', true)\n\n// Rate limiter fails with:\n// ValidationError: The 'X-Forwarded-For' header is set but the Express\n// 'trust proxy' setting is false\n\nconst apiLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000,\n  max: 100\n})\napp.use('/api/', apiLimiter)  // ← Throws ValidationError in production\n\n// Result: Rate limiting broken, misleading client IPs in logs\n// Nginx sets X-Forwarded-For header, but Express doesn't trust it\n\n// ✅ FIXED: Trust proxy enabled (v1.1.1)\nconst app = express()\napp.set('trust proxy', true)  // ← ADDED - Now trusts Nginx headers\n\nconst apiLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000,\n  max: 100\n})\napp.use('/api/', apiLimiter)  // ← Now works correctly\n\n// Result: Rate limiting working, correct client IPs logged\n```\n\n**Trust Proxy Checklist:**\n```bash\n# Step 1: Identify if behind reverse proxy\n□ Using Nginx? → Requires trust proxy\n□ Using Apache? → Requires trust proxy\n□ Using CloudFlare? → Requires trust proxy\n□ Using AWS ALB/ELB? → Requires trust proxy\n□ Direct to internet? → Do NOT enable trust proxy\n\n# Step 2: Verify X-Forwarded headers\ncurl -I https://pdflab.pro/api/health\n# Check for: X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Host\n\n# Step 3: Test rate limiting works\nfor i in {1..10}; do\n  curl https://pdflab.pro/api/health\ndone\n# Should work without ValidationError\n\n# Step 4: Verify correct client IPs in logs\ndocker logs pdflab-backend-prod | grep \"IP:\"\n# Should show real client IPs, not proxy IP\n```\n\n**Production Deployment Checklist (CORS + Trust Proxy):**\n- [ ] CORS origins include production domain(s)\n- [ ] Trust proxy enabled if behind Nginx/Apache/CloudFlare\n- [ ] Test API call from production domain BEFORE deployment\n- [ ] Verify rate limiting works in production\n- [ ] Check browser DevTools Network tab for CORS errors\n- [ ] Monitor logs for \"Not allowed by CORS\" warnings\n- [ ] Verify X-Forwarded-For headers trusted correctly\n\n#### 🔴 CRITICAL: Request Validation\n\n**Historical Failure:** Missing file validation allowed 10GB upload, crashed server\n\n**Scan for:**\n- [ ] Required fields validated (401 if missing)\n- [ ] Data types validated (string, number, enum)\n- [ ] File uploads validated (size, type, count)\n- [ ] Enum values validated against allowed list\n- [ ] Input sanitization for XSS/injection\n- [ ] Array/object structure validation\n\n**Validation Patterns:**\n```typescript\n// ❌ NO VALIDATION - Accepts anything\nexport const uploadBatch = async (req: Request, res: Response) => {\n  const files = req.files\n  const operationType = req.body.operation_type\n  // No checks - could be undefined, wrong type, etc.\n}\n\n// ✅ COMPREHENSIVE VALIDATION\nexport const uploadBatch = async (req: Request, res: Response) => {\n  // 1. File validation\n  const files = req.files as Express.Multer.File[]\n  if (!files || files.length === 0) {\n    return res.status(400).json({\n      error: 'No files uploaded',\n      message: 'Please provide at least one file'\n    })\n  }\n\n  // 2. Enum validation\n  const operation_type = req.body.operation_type as BatchOperationType\n  if (!Object.values(BatchOperationType).includes(operation_type)) {\n    return res.status(400).json({\n      error: 'Invalid operation type',\n      message: 'Operation type must be one of: convert, compress, merge'\n    })\n  }\n\n  // 3. Count validation\n  const maxBatchSize = req.user!.getMaxBatchSize()\n  if (files.length > maxBatchSize) {\n    return res.status(400).json({\n      error: 'Too many files',\n      message: `Your ${req.user!.plan} plan supports up to ${maxBatchSize} files`,\n      upgrade_required: true\n    })\n  }\n\n  // 4. File size validation\n  const maxFileSize = req.user!.getMaxFileSize()\n  const oversizedFile = files.find(file => file.size > maxFileSize)\n  if (oversizedFile) {\n    return res.status(413).json({\n      error: 'File too large',\n      file_name: oversizedFile.originalname,\n      file_size: oversizedFile.size,\n      max_file_size: maxFileSize\n    })\n  }\n}\n\n// ✅ TYPE VALIDATION with Zod (recommended)\nimport { z } from 'zod'\n\nconst uploadBatchSchema = z.object({\n  operation_type: z.enum(['convert', 'compress', 'merge']),\n  batch_name: z.string().optional(),\n  output_format: z.enum(['pptx', 'docx', 'xlsx', 'png']).optional(),\n  compression_level: z.enum(['good', 'recommended', 'extreme']).optional()\n})\n\nexport const uploadBatch = async (req: Request, res: Response) => {\n  // Validate with Zod\n  const validation = uploadBatchSchema.safeParse(req.body)\n  if (!validation.success) {\n    return res.status(400).json({\n      error: 'Validation failed',\n      details: validation.error.errors\n    })\n  }\n\n  const { operation_type, batch_name } = validation.data\n}\n```\n\n#### 🟡 HIGH: Error Response Consistency\n\n**Historical Issue:** Some endpoints returned strings, some objects, broke client parsing\n\n**Scan for:**\n- [ ] Consistent error format across all endpoints\n- [ ] HTTP status codes match error type (400=bad request, 401=unauthorized, 403=forbidden, 404=not found, 500=server error)\n- [ ] Error messages are user-friendly\n- [ ] Error responses include helpful context\n- [ ] Stack traces NOT exposed in production\n\n**Error Response Standard:**\n```typescript\n// ❌ INCONSISTENT\nres.status(400).send('Bad request')  // String\nres.status(404).json({ error: 'Not found' })  // Object\nres.status(500).json({ message: 'Error', code: 500 })  // Different format\n\n// ✅ CONSISTENT - Always same shape\ninterface ErrorResponse {\n  error: string           // Short error type\n  message: string         // User-friendly message\n  details?: any          // Additional context\n  upgrade_required?: boolean\n  cta?: { text: string, url: string }\n}\n\n// Example usage:\nres.status(400).json({\n  error: 'Invalid file type',\n  message: 'Only PDF files are supported',\n  details: { uploaded_type: 'image/jpeg', supported_types: ['application/pdf'] }\n})\n\nres.status(401).json({\n  error: 'Authentication required',\n  message: 'Please log in to access this feature',\n  cta: { text: 'Log In', url: '/login' }\n})\n\nres.status(413).json({\n  error: 'File too large',\n  message: `Your ${user.plan} plan supports files up to ${maxSizeMB}MB`,\n  upgrade_required: true,\n  cta: { text: 'Upgrade Plan', url: '/pricing' }\n})\n```\n\n**Status Code Guide:**\n- `200 OK` - Success with response body\n- `201 Created` - Resource created (return resource)\n- `204 No Content` - Success with no response body\n- `400 Bad Request` - Validation error\n- `401 Unauthorized` - Not authenticated\n- `403 Forbidden` - Authenticated but not authorized\n- `404 Not Found` - Resource doesn't exist\n- `409 Conflict` - Resource conflict (duplicate email)\n- `413 Payload Too Large` - File/request too big\n- `422 Unprocessable Entity` - Semantic validation error\n- `429 Too Many Requests` - Rate limit exceeded\n- `500 Internal Server Error` - Unexpected server error\n- `503 Service Unavailable` - Temporarily unavailable\n\n#### 🟡 HIGH: Response Format Consistency\n\n**Scan for:**\n- [ ] Success responses follow consistent structure\n- [ ] List endpoints include pagination metadata\n- [ ] Timestamps in ISO 8601 format\n- [ ] IDs are strings (UUIDs) not numbers\n- [ ] Nested objects use consistent naming (snake_case or camelCase)\n\n**Response Patterns:**\n```typescript\n// ✅ SINGLE RESOURCE\nres.status(200).json({\n  batch_id: '123e4567-e89b-12d3-a456-426614174000',\n  batch_name: 'My Batch',\n  status: 'processing',\n  progress: 60,\n  created_at: '2025-11-09T10:30:00Z'\n})\n\n// ✅ LIST WITH PAGINATION\nres.status(200).json({\n  batches: [\n    { batch_id: '...', batch_name: '...' },\n    { batch_id: '...', batch_name: '...' }\n  ],\n  pagination: {\n    page: 1,\n    limit: 20,\n    total: 45,\n    total_pages: 3\n  }\n})\n\n// ✅ CREATION RESPONSE (201)\nres.status(201).json({\n  message: 'Batch created successfully',\n  batch_id: '123e4567-e89b-12d3-a456-426614174000',\n  status: 'pending',\n  conversion_job_ids: ['...', '...'],\n  created_at: '2025-11-09T10:30:00Z'\n})\n```\n\n#### 🟠 MEDIUM: Rate Limiting\n\n**Scan for:**\n- [ ] Rate limits applied to expensive operations\n- [ ] Different limits for auth vs guest users\n- [ ] Rate limit headers returned (X-RateLimit-Limit, X-RateLimit-Remaining)\n- [ ] 429 status code when rate limited\n- [ ] Retry-After header included\n\n**Rate Limiting Patterns:**\n```typescript\nimport rateLimit from 'express-rate-limit'\n\n// ✅ GLOBAL RATE LIMIT\nconst apiLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // Limit each IP to 100 requests per windowMs\n  message: {\n    error: 'Too many requests',\n    message: 'Please try again later',\n    retry_after: 900 // seconds\n  },\n  standardHeaders: true, // Return rate limit info in headers\n  legacyHeaders: false\n})\n\napp.use('/api/', apiLimiter)\n\n// ✅ ENDPOINT-SPECIFIC LIMIT\nconst uploadLimiter = rateLimit({\n  windowMs: 60 * 60 * 1000, // 1 hour\n  max: 10, // 10 uploads per hour\n  message: {\n    error: 'Upload limit exceeded',\n    message: 'You can upload 10 batches per hour on your plan',\n    upgrade_required: true\n  }\n})\n\nrouter.post('/api/batches', authMiddleware, uploadLimiter, uploadBatch)\n\n// ✅ USER-BASED RATE LIMIT\nconst getUserRateLimit = (user: User) => {\n  switch (user.plan) {\n    case 'free': return 10\n    case 'starter': return 50\n    case 'pro': return 200\n    case 'enterprise': return 1000\n  }\n}\n```\n\n#### 🟠 MEDIUM: Documentation\n\n**Scan for:**\n- [ ] JSDoc comments on all public endpoints\n- [ ] Request/response types documented\n- [ ] Authentication requirements documented\n- [ ] Example requests included\n- [ ] Error responses documented\n\n**Documentation Pattern:**\n```typescript\n/**\n * Upload multiple files for batch processing\n *\n * @route POST /api/batches\n * @access Private (requires authentication)\n * @ratelimit 10 uploads per hour\n *\n * @body {File[]} files - 2-50 PDF files (plan-dependent)\n * @body {string} operation_type - 'convert' | 'compress' | 'merge'\n * @body {string} [batch_name] - Optional custom name\n * @body {string} [output_format] - Required if operation_type='convert'\n * @body {string} [compression_level] - Required if operation_type='compress'\n *\n * @returns {201} Batch created successfully\n * @returns {400} Validation error\n * @returns {401} Not authenticated\n * @returns {413} File too large\n * @returns {429} Rate limit exceeded\n *\n * @example\n * const formData = new FormData()\n * formData.append('files', file1)\n * formData.append('files', file2)\n * formData.append('operation_type', 'convert')\n * formData.append('output_format', 'pptx')\n *\n * fetch('/api/batches', {\n *   method: 'POST',\n *   headers: { 'Authorization': `Bearer ${token}` },\n *   body: formData\n * })\n */\nexport const uploadBatch = async (req: Request, res: Response) => {\n  // Implementation\n}\n```\n\n### 3. Production Pre-Flight Checklist\n\nBefore deploying new endpoints:\n\n**Security:**\n- [ ] Authentication required for protected endpoints\n- [ ] Authorization checks for user-specific resources\n- [ ] Input validation prevents injection attacks\n- [ ] File uploads validated and sanitized\n- [ ] Secrets not logged or returned in responses\n- [ ] CORS configured correctly\n\n**Performance:**\n- [ ] Database queries optimized (use EXPLAIN)\n- [ ] Pagination for list endpoints\n- [ ] Rate limiting applied\n- [ ] Large operations run async (batch processing)\n- [ ] Caching strategy defined\n\n**Reliability:**\n- [ ] Error handling for all failure modes\n- [ ] Graceful degradation if dependencies fail\n- [ ] Timeout handling for long operations\n- [ ] Retry logic for transient failures\n- [ ] Rollback strategy for partial failures\n\n**Monitoring:**\n- [ ] Logging at info/warn/error levels\n- [ ] Metrics tracked (request count, duration, errors)\n- [ ] Alerting configured for error rates\n- [ ] Performance baselines established\n\n### 4. Common API Errors & Fixes\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"Cannot set headers after sent\" | Calling `res.json()` twice | Use `return res.json()` to exit function |\n| \"User is not authenticated\" | Missing auth middleware | Add `authMiddleware` to route |\n| \"Cannot read property 'id' of undefined\" | `req.user` undefined | Check `req.user` exists before accessing |\n| \"Validation error\" | Missing required field | Add validation before processing |\n| \"Too many requests\" | Rate limit exceeded | Implement exponential backoff on client |\n| \"CORS error\" | Origin not allowed | Add origin to CORS whitelist |\n\n### 5. API Versioning Strategy\n\n**When to version:**\n- Breaking changes to request/response format\n- Removing fields\n- Changing data types\n- Renaming endpoints\n\n**Versioning Patterns:**\n```typescript\n// ✅ URL VERSIONING (Recommended for REST)\nrouter.post('/api/v1/batches', uploadBatch)\nrouter.post('/api/v2/batches', uploadBatchV2)  // Breaking changes\n\n// ✅ HEADER VERSIONING\napp.use((req, res, next) => {\n  const version = req.headers['api-version'] || 'v1'\n  req.apiVersion = version\n  next()\n})\n\n// ✅ DEPRECATION WARNING\nres.set('X-API-Deprecated', 'true')\nres.set('X-API-Sunset', '2026-01-01')\nres.json({\n  warning: 'This endpoint is deprecated. Use /api/v2/batches instead.',\n  sunset_date: '2026-01-01'\n})\n```\n\n## Output Format: Auto-Scan Report\n\n```\n═══════════════════════════════════════════════\n🛡️ API ENDPOINT GUARDIAN - SCAN RESULTS\n═══════════════════════════════════════════════\n\n📊 SCAN SCOPE\n• Endpoint: POST /api/batch/upload\n• Controller: uploadBatchFiles\n• Authentication: Required\n• Rate Limit: None configured\n\n🚨 CRITICAL FINDINGS: 2\n1. Authentication not verified in controller\n   • Risk: Bypass possible if middleware removed\n   • Fix: Add `if (!req.user) return res.status(401)...`\n\n2. No file count validation\n   • Risk: Upload 1000 files, crash server\n   • Fix: Check files.length against plan limits\n\n⚠️  HIGH PRIORITY: 3\n1. Inconsistent error responses\n   • Some return strings, some objects\n   • Fix: Standardize to { error, message, details }\n\n2. No rate limiting\n   • Risk: API abuse, DDoS vulnerability\n   • Fix: Add uploadLimiter middleware\n\n3. Missing request validation\n   • operation_type not validated against enum\n   • Fix: Use Zod schema validation\n\n💡 OPTIMIZATIONS: 4\n1. Add JSDoc documentation\n2. Return 201 Created instead of 200 OK\n3. Include batch_id in response headers (Location)\n4. Add pagination to batch history endpoint\n\n═══════════════════════════════════════════════\nAPI DESIGN SCORE: 7/10\n═══════════════════════════════════════════════\n✅ HTTP method correct (POST for creation)\n✅ Authentication middleware applied\n⚠️  Missing ownership verification\n⚠️  No rate limiting\n❌ Request validation incomplete\n❌ Error responses inconsistent\n\nPRODUCTION READINESS: BLOCKED\nCritical issues must be resolved before deployment\n\nNEXT ACTIONS:\n1. Add file count validation\n2. Verify req.user exists in controller\n3. Standardize error response format\n4. Add rate limiting middleware\n\n═══════════════════════════════════════════════\n```\n\n## Quick Reference: Middleware Order\n\n```typescript\n// ✅ CORRECT ORDER\nrouter.post(\n  '/api/batches',\n  cors(),                          // 1. CORS first\n  apiLimiter,                      // 2. Rate limiting\n  authMiddleware,                  // 3. Authentication\n  requirePermission('batch.create'), // 4. Authorization\n  upload.array('files', 50),       // 5. File upload\n  validateRequest(uploadSchema),   // 6. Validation\n  uploadBatch                      // 7. Controller\n)\n\n// ❌ WRONG ORDER - Auth after validation (security gap)\nrouter.post('/api/batches',\n  validateRequest(uploadSchema),  // Processes untrusted input first!\n  authMiddleware,\n  uploadBatch\n)\n```\n\n## Key Principles\n\n1. **Fail fast** - Validate early, return errors immediately\n2. **Consistent errors** - Always same format\n3. **Auth first** - Check authentication before anything else\n4. **Validate everything** - Never trust client input\n5. **Document thoroughly** - Future you will thank you\n6. **Version carefully** - Breaking changes need new version\n7. **Monitor everything** - You can't fix what you can't see\n\n## When to Escalate\n\n- Complex authorization rules (RBAC, ABAC)\n- GraphQL endpoint design\n- Websocket API patterns\n- Microservices communication\n- API gateway configuration\n- OAuth/SSO integration\n",
    "github": {
      "stars": 1,
      "forks": 0,
      "owner": "mkelam",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/130832145?v=4",
      "lastCommit": "2025-12-21T18:09:06Z",
      "createdAt": "2025-10-29T11:23:14Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:12.082Z"
  },
  {
    "id": "code-quality-fix",
    "source": {
      "type": "github-search",
      "repo": "DarkMonkDev/WitchCityRope",
      "path": ".claude/skills/code-quality-fix/SKILL.md",
      "url": "https://github.com/DarkMonkDev/WitchCityRope/blob/main/.claude/skills/code-quality-fix/SKILL.md"
    },
    "frontmatter": {},
    "content": "#!/bin/bash\n# code-quality-fix skill - Automated code quality fixes\n# Purpose: Auto-fix ESLint errors, format with Prettier, organize imports\n# Performance: ~3-7 seconds for full codebase, ~2 seconds for changed files\n# Exit codes: 0 = fixes applied successfully, 1 = manual fixes required\n\n# ============================================================================\n# SKILL: Code Quality Fix\n# ============================================================================\n#\n# WHAT THIS DOES:\n# Automatically fixes code quality issues including:\n# - ESLint auto-fixable errors and warnings\n# - Prettier formatting issues\n# - Import statement organization\n# - Trailing whitespace and newlines\n#\n# WHEN TO USE:\n# - After completing development session (cleanup)\n# - Before creating a pull request\n# - After merging branches (resolve conflicts)\n# - When quality checks fail with auto-fixable issues\n# - As part of code refactoring\n#\n# WHO USES THIS:\n# - react-developer (cleanup after feature development)\n# - backend-developer (fix TypeScript issues)\n# - All developers (before committing)\n# - code-reviewer agent (suggest auto-fix for PRs)\n#\n# INTEGRATION:\n# - Complements code-quality-check skill (run check first, then fix)\n# - Used after merge conflicts to fix formatting\n# - Referenced by development workflow guides\n#\n# ============================================================================\n\nset -e\n\n# Color codes for output\nRED='\\033[0;31m'\nYELLOW='\\033[1;33m'\nGREEN='\\033[0;32m'\nBLUE='\\033[0;34m'\nCYAN='\\033[0;36m'\nNC='\\033[0m' # No Color\n\necho \"\"\necho -e \"${BLUE}╔════════════════════════════════════════════════════════════════╗${NC}\"\necho -e \"${BLUE}║           CODE QUALITY AUTO-FIX - WitchCityRope                ║${NC}\"\necho -e \"${BLUE}╚════════════════════════════════════════════════════════════════╝${NC}\"\necho \"\"\n\n# Parse options\nCHANGED_ONLY=false\nDRY_RUN=false\nVERIFY_AFTER=true\n\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        --changed-only)\n            CHANGED_ONLY=true\n            shift\n            ;;\n        --dry-run)\n            DRY_RUN=true\n            shift\n            ;;\n        --no-verify)\n            VERIFY_AFTER=false\n            shift\n            ;;\n        *)\n            echo -e \"${RED}Unknown option: $1${NC}\"\n            echo \"Usage: bash code-quality-fix.md [--changed-only] [--dry-run] [--no-verify]\"\n            exit 1\n            ;;\n    esac\ndone\n\nSTART_TIME=$(date +%s)\nFIXES_APPLIED=0\nMANUAL_FIXES_NEEDED=0\n\n# ============================================================================\n# STEP 1: Environment Check\n# ============================================================================\n\necho -e \"${CYAN}1️⃣  Checking environment...${NC}\"\n\n# Check if we're in the right directory\nif [ ! -f \"package.json\" ]; then\n    echo -e \"${RED}   ❌ Error: Not in project root (package.json not found)${NC}\"\n    exit 1\nfi\n\n# Check if node_modules exists\nif [ ! -d \"node_modules\" ]; then\n    echo -e \"${RED}   ❌ Error: node_modules not found. Run: npm install${NC}\"\n    exit 1\nfi\n\n# Verify tools are available\nif ! command -v npx &> /dev/null; then\n    echo -e \"${RED}   ❌ Error: npx not found. Install Node.js${NC}\"\n    exit 1\nfi\n\necho -e \"${GREEN}   ✅ Environment ready${NC}\"\n\nif [ \"$DRY_RUN\" = true ]; then\n    echo -e \"${YELLOW}   ⚠️  DRY RUN MODE: No changes will be made${NC}\"\nfi\n\necho \"\"\n\n# ============================================================================\n# STEP 2: Determine File Scope\n# ============================================================================\n\necho -e \"${CYAN}2️⃣  Determining files to fix...${NC}\"\n\nif [ \"$CHANGED_ONLY\" = true ]; then\n    # Only fix git-modified files\n    CHANGED_FILES=$(git diff --name-only --diff-filter=ACM | grep -E '\\.(ts|tsx|js|jsx)$' || echo \"\")\n\n    if [ -z \"$CHANGED_FILES\" ]; then\n        echo -e \"${GREEN}   ✅ No changed TypeScript/JavaScript files${NC}\"\n        echo \"\"\n        exit 0\n    fi\n\n    FILE_COUNT=$(echo \"$CHANGED_FILES\" | wc -l)\n    echo -e \"${BLUE}   Scope: Changed files only (${FILE_COUNT} files)${NC}\"\n\n    # Show first few files\n    echo \"$CHANGED_FILES\" | head -5 | while read file; do\n        echo -e \"   - $file\"\n    done\n    if [ $FILE_COUNT -gt 5 ]; then\n        echo -e \"   ... and $((FILE_COUNT - 5)) more\"\n    fi\nelse\n    echo -e \"${BLUE}   Scope: Full codebase${NC}\"\n    echo -e \"${BLUE}   Pattern: **/*.{ts,tsx,js,jsx}${NC}\"\nfi\n\necho \"\"\n\n# ============================================================================\n# STEP 3: Run ESLint Auto-Fix\n# ============================================================================\n\necho -e \"${CYAN}3️⃣  Running ESLint auto-fix...${NC}\"\n\nif [ \"$DRY_RUN\" = true ]; then\n    # Dry run: show what would be fixed\n    if [ \"$CHANGED_ONLY\" = true ] && [ -n \"$CHANGED_FILES\" ]; then\n        echo \"$CHANGED_FILES\" | xargs npx eslint --fix-dry-run --format=compact 2>&1 | tee /tmp/eslint-dry-run.txt\n    else\n        npx eslint . --ext .ts,.tsx,.js,.jsx --fix-dry-run --format=compact 2>&1 | tee /tmp/eslint-dry-run.txt\n    fi\n\n    FIXABLE=$(grep -c \"fixable\" /tmp/eslint-dry-run.txt || echo \"0\")\n    echo -e \"${YELLOW}   ⚠️  DRY RUN: ${FIXABLE} issues would be auto-fixed${NC}\"\nelse\n    # Actually fix issues\n    if [ \"$CHANGED_ONLY\" = true ] && [ -n \"$CHANGED_FILES\" ]; then\n        echo \"$CHANGED_FILES\" | xargs npx eslint --fix --format=compact 2>&1 | tee /tmp/eslint-fix.txt\n    else\n        npx eslint . --ext .ts,.tsx,.js,.jsx --fix --format=compact 2>&1 | tee /tmp/eslint-fix.txt\n    fi\n\n    # Count fixed issues\n    FIXED=$(grep -c \"fixed\" /tmp/eslint-fix.txt || echo \"0\")\n    REMAINING=$(grep -c \"problem\" /tmp/eslint-fix.txt || echo \"0\")\n\n    if [ \"$FIXED\" -gt 0 ]; then\n        echo -e \"${GREEN}   ✅ Auto-fixed ${FIXED} ESLint issues${NC}\"\n        FIXES_APPLIED=$((FIXES_APPLIED + FIXED))\n    fi\n\n    if [ \"$REMAINING\" -gt 0 ]; then\n        echo -e \"${YELLOW}   ⚠️  ${REMAINING} issues require manual fixes${NC}\"\n        MANUAL_FIXES_NEEDED=$((MANUAL_FIXES_NEEDED + REMAINING))\n    fi\n\n    if [ \"$FIXED\" -eq 0 ] && [ \"$REMAINING\" -eq 0 ]; then\n        echo -e \"${GREEN}   ✅ No ESLint issues found${NC}\"\n    fi\nfi\n\necho \"\"\n\n# ============================================================================\n# STEP 4: Run Prettier Formatting\n# ============================================================================\n\necho -e \"${CYAN}4️⃣  Running Prettier formatting...${NC}\"\n\nif [ \"$DRY_RUN\" = true ]; then\n    # Dry run: check what would be formatted\n    if [ \"$CHANGED_ONLY\" = true ] && [ -n \"$CHANGED_FILES\" ]; then\n        PRETTIER_FILES=$(echo \"$CHANGED_FILES\" | xargs npx prettier --list-different || echo \"\")\n    else\n        PRETTIER_FILES=$(npx prettier --list-different \"**/*.{ts,tsx,js,jsx,json,md}\" || echo \"\")\n    fi\n\n    if [ -n \"$PRETTIER_FILES\" ]; then\n        PRETTIER_COUNT=$(echo \"$PRETTIER_FILES\" | wc -l)\n        echo -e \"${YELLOW}   ⚠️  DRY RUN: ${PRETTIER_COUNT} files would be formatted${NC}\"\n    else\n        echo -e \"${GREEN}   ✅ All files already formatted${NC}\"\n    fi\nelse\n    # Actually format files\n    if [ \"$CHANGED_ONLY\" = true ] && [ -n \"$CHANGED_FILES\" ]; then\n        echo \"$CHANGED_FILES\" | xargs npx prettier --write 2>&1 > /tmp/prettier-write.txt\n    else\n        npx prettier --write \"**/*.{ts,tsx,js,jsx,json,md}\" 2>&1 > /tmp/prettier-write.txt\n    fi\n\n    FORMATTED=$(grep -c \"ms\" /tmp/prettier-write.txt || echo \"multiple\")\n    echo -e \"${GREEN}   ✅ Formatted files with Prettier${NC}\"\n    FIXES_APPLIED=$((FIXES_APPLIED + 1))\nfi\n\necho \"\"\n\n# ============================================================================\n# STEP 5: Show Changes Made\n# ============================================================================\n\nif [ \"$DRY_RUN\" = false ]; then\n    echo -e \"${CYAN}5️⃣  Reviewing changes...${NC}\"\n\n    # Show git diff statistics\n    if git diff --stat 2>/dev/null | head -15 | tee /tmp/git-diff-stat.txt; then\n        MODIFIED_FILES=$(grep -c \"file changed\\|files changed\" /tmp/git-diff-stat.txt || echo \"0\")\n        if [ \"$MODIFIED_FILES\" -gt 0 ]; then\n            echo \"\"\n            echo -e \"${BLUE}   ${MODIFIED_FILES} files modified${NC}\"\n            echo -e \"${YELLOW}   Review with: ${CYAN}git diff${NC}\"\n        else\n            echo -e \"${GREEN}   No changes needed (files already clean)${NC}\"\n        fi\n    else\n        echo -e \"${GREEN}   No changes made${NC}\"\n    fi\n\n    echo \"\"\nfi\n\n# ============================================================================\n# STEP 6: Verify Fixes (Re-run Quality Check)\n# ============================================================================\n\nif [ \"$VERIFY_AFTER\" = true ] && [ \"$DRY_RUN\" = false ]; then\n    echo -e \"${CYAN}6️⃣  Verifying fixes...${NC}\"\n    echo \"\"\n\n    # Run code-quality-check skill to verify\n    if [ \"$CHANGED_ONLY\" = true ]; then\n        bash /.claude/skills/code-quality-check.md --quick --changed-only\n    else\n        bash /.claude/skills/code-quality-check.md --quick\n    fi\n\n    VERIFY_EXIT=$?\n\n    if [ $VERIFY_EXIT -eq 0 ]; then\n        echo -e \"${GREEN}   ✅ All quality checks now pass!${NC}\"\n    elif [ $VERIFY_EXIT -eq 2 ]; then\n        echo -e \"${YELLOW}   ⚠️  Quality checks pass with warnings${NC}\"\n    else\n        echo -e \"${YELLOW}   ⚠️  Some issues require manual fixes${NC}\"\n        MANUAL_FIXES_NEEDED=$((MANUAL_FIXES_NEEDED + 1))\n    fi\n\n    echo \"\"\nfi\n\n# ============================================================================\n# STEP 7: Summary Report\n# ============================================================================\n\nEND_TIME=$(date +%s)\nDURATION=$((END_TIME - START_TIME))\n\necho \"\"\necho -e \"${BLUE}╔════════════════════════════════════════════════════════════════╗${NC}\"\necho -e \"${BLUE}║                  AUTO-FIX SUMMARY                              ║${NC}\"\necho -e \"${BLUE}╚════════════════════════════════════════════════════════════════╝${NC}\"\necho \"\"\n\nif [ \"$DRY_RUN\" = true ]; then\n    echo -e \"${YELLOW}🔍 DRY RUN COMPLETE (No changes made)${NC}\"\n    echo -e \"${YELLOW}   Re-run without --dry-run to apply fixes${NC}\"\n    EXIT_CODE=0\nelif [ $MANUAL_FIXES_NEEDED -eq 0 ]; then\n    echo -e \"${GREEN}✅ STATUS: ALL ISSUES FIXED${NC}\"\n    echo -e \"${GREEN}   Fixes applied: ${FIXES_APPLIED}${NC}\"\n    echo -e \"${GREEN}   Manual fixes needed: 0${NC}\"\n    EXIT_CODE=0\nelse\n    echo -e \"${YELLOW}⚠️  STATUS: PARTIALLY FIXED${NC}\"\n    echo -e \"${GREEN}   Fixes applied: ${FIXES_APPLIED}${NC}\"\n    echo -e \"${YELLOW}   Manual fixes needed: ${MANUAL_FIXES_NEEDED}${NC}\"\n    EXIT_CODE=1\nfi\n\necho \"\"\necho -e \"${CYAN}Performance:${NC}\"\necho -e \"   Duration: ${DURATION}s\"\necho -e \"   Mode: $([ \"$DRY_RUN\" = true ] && echo \"Dry run\" || echo \"Apply fixes\")\"\necho -e \"   Scope: $([ \"$CHANGED_ONLY\" = true ] && echo \"Changed files\" || echo \"Full codebase\")\"\necho \"\"\n\n# ============================================================================\n# STEP 8: Next Steps\n# ============================================================================\n\nif [ $MANUAL_FIXES_NEEDED -gt 0 ]; then\n    echo -e \"${YELLOW}📋 Manual Fixes Required:${NC}\"\n    echo \"\"\n    echo -e \"1. Review remaining issues:\"\n    echo -e \"   ${CYAN}bash /.claude/skills/code-quality-check.md${NC}\"\n    echo \"\"\n    echo -e \"2. Fix issues that couldn't be auto-fixed\"\n    echo \"\"\n    echo -e \"3. Re-run this skill to verify:\"\n    echo -e \"   ${CYAN}bash /.claude/skills/code-quality-fix.md${NC}\"\n    echo \"\"\nfi\n\nif [ \"$DRY_RUN\" = false ] && [ $FIXES_APPLIED -gt 0 ]; then\n    echo -e \"${YELLOW}💡 Recommended Actions:${NC}\"\n    echo \"\"\n    echo -e \"1. Review the changes made:\"\n    echo -e \"   ${CYAN}git diff${NC}\"\n    echo \"\"\n    echo -e \"2. Test your application:\"\n    echo -e \"   ${CYAN}npm run dev${NC}\"\n    echo -e \"   ${CYAN}npm test${NC}\"\n    echo \"\"\n    echo -e \"3. Commit the fixes:\"\n    echo -e \"   ${CYAN}git add .${NC}\"\n    echo -e \"   ${CYAN}git commit -m \\\"Fix: Auto-fix code quality issues\\\"${NC}\"\n    echo \"\"\nfi\n\nif [ \"$DRY_RUN\" = true ]; then\n    echo -e \"${YELLOW}📋 To Apply Fixes:${NC}\"\n    echo \"\"\n    if [ \"$CHANGED_ONLY\" = true ]; then\n        echo -e \"   ${CYAN}bash /.claude/skills/code-quality-fix.md --changed-only${NC}\"\n    else\n        echo -e \"   ${CYAN}bash /.claude/skills/code-quality-fix.md${NC}\"\n    fi\n    echo \"\"\nfi\n\necho -e \"${BLUE}════════════════════════════════════════════════════════════════${NC}\"\necho \"\"\n\n# ============================================================================\n# TROUBLESHOOTING GUIDE\n# ============================================================================\n#\n# COMMON ISSUES:\n#\n# 1. \"Some issues cannot be auto-fixed\"\n#    - Expected: Not all ESLint rules are auto-fixable\n#    - Solution: Fix remaining issues manually\n#    - Check: ESLint output for specific error messages\n#\n# 2. \"Changes break application\"\n#    - Review: Use git diff to see what changed\n#    - Test: Run npm run dev and npm test\n#    - Revert: git checkout . (if needed)\n#\n# 3. \"Prettier conflicts with ESLint\"\n#    - Install: eslint-config-prettier (disables conflicting rules)\n#    - Add: \"prettier\" to .eslintrc.json extends array\n#\n# 4. \"Too many changes at once\"\n#    - Use: --changed-only flag for incremental fixes\n#    - Use: --dry-run to preview changes first\n#\n# 5. \"Verification fails after fix\"\n#    - Some issues require manual fixes\n#    - Run: bash /.claude/skills/code-quality-check.md\n#    - Fix: Remaining issues manually\n#\n# USAGE EXAMPLES:\n#\n# Fix all auto-fixable issues:\n#   bash /.claude/skills/code-quality-fix.md\n#\n# Fix changed files only:\n#   bash /.claude/skills/code-quality-fix.md --changed-only\n#\n# Preview fixes without applying:\n#   bash /.claude/skills/code-quality-fix.md --dry-run\n#\n# Fix without verification:\n#   bash /.claude/skills/code-quality-fix.md --no-verify\n#\n# WHAT GETS FIXED:\n#\n# ESLint auto-fixable rules:\n# - Missing semicolons\n# - Quote style (single vs double)\n# - Spacing issues\n# - Import ordering\n# - Unused imports\n# - Trailing commas\n#\n# Prettier formatting:\n# - Indentation\n# - Line breaks\n# - Maximum line length\n# - Bracket spacing\n# - Arrow function parentheses\n#\n# WHAT REQUIRES MANUAL FIX:\n#\n# - Type errors\n# - Logic errors\n# - Complex ESLint violations\n# - Security vulnerabilities\n# - Unused variables (must decide if needed)\n# - Missing function returns\n#\n# INTEGRATION:\n#\n# Typical workflow:\n# 1. Develop feature\n# 2. Run: bash /.claude/skills/code-quality-fix.md\n# 3. Review: git diff\n# 4. Test: npm test\n# 5. Commit: git commit\n#\n# Before PR:\n# 1. bash /.claude/skills/code-quality-fix.md\n# 2. bash /.claude/skills/code-quality-check.md\n# 3. Fix any remaining issues\n# 4. Create PR\n#\n# ============================================================================\n\nexit $EXIT_CODE\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "DarkMonkDev",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/75149771?v=4",
      "lastCommit": "2025-12-15T04:02:53Z",
      "createdAt": "2025-06-28T17:15:19Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:12.720Z"
  },
  {
    "id": "database",
    "source": {
      "type": "github-search",
      "repo": "rubys/showcase",
      "path": ".claude/skills/database/SKILL.md",
      "url": "https://github.com/rubys/showcase/blob/main/.claude/skills/database/SKILL.md"
    },
    "frontmatter": {
      "name": "database",
      "description": "Database management commands and the bin/run utility for executing Ruby scripts against specific event databases. Use when the user needs to query, manipulate, or run scripts against existing databases without starting a Rails server."
    },
    "content": "# Database Management\n\n## Standard Database Commands\n\n```bash\n# Prepare database (creates and runs migrations)\nbin/rails db:prepare\n\n# Run migrations\nbin/rails db:migrate\n\n# Load seed data\nbin/rails db:seed\n\n# Load fixtures (for test data)\nbin/rails db:fixtures:load\n```\n\n## Running Scripts Against Existing Databases\n\nThe `bin/run` command allows you to execute Ruby scripts against any database in the project.\n\n### Basic Usage\n\n```bash\n# Run a script file against a specific database\nbin/run db/2025-boston.sqlite3 path/to/script.rb\n\n# Evaluate Ruby code directly against a database\nbin/run db/2025-boston.sqlite3 -e \"puts Event.current.name\"\n\n# Run against test database (automatically loads fixtures)\nbin/run test -e \"puts Person.count\"\n```\n\n### Common Query Examples\n\n```bash\n# Count heats\nbin/run db/2025-boston.sqlite3 -e \"Heat.count\"\n\n# Get all student names\nbin/run db/2025-boston.sqlite3 -e \"Person.where(type: 'Student').pluck(:name)\"\n\n# List studios with person counts\nbin/run db/2025-boston.sqlite3 -e \"Studio.all.map { |s| [s.name, s.people.count] }\"\n```\n\n### How bin/run Works\n\nThe script automatically sets up:\n- `RAILS_APP_DB` environment variable from database filename\n- `RAILS_STORAGE` path for Active Storage files\n- For test database: runs `db:prepare` and loads fixtures\n\nThis allows you to query and manipulate any event database without starting a full Rails server.\n",
    "github": {
      "stars": 34,
      "forks": 6,
      "owner": "rubys",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/4815?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-24T21:15:09Z",
      "createdAt": "2022-04-06T12:34:32Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:13.350Z"
  },
  {
    "id": "navigator",
    "source": {
      "type": "github-search",
      "repo": "rubys/showcase",
      "path": ".claude/skills/navigator/SKILL.md",
      "url": "https://github.com/rubys/showcase/blob/main/.claude/skills/navigator/SKILL.md"
    },
    "frontmatter": {
      "name": "navigator",
      "description": "Working with the Navigator Go submodule for web server fixes and enhancements. Use when deployment plans require Navigator changes, config parsing issues arise, or new routing/proxy behavior is needed."
    },
    "content": "# Navigator Submodule Development\n\n## Why Navigator is a Submodule\n\nNavigator is included as a Git submodule because showcase routinely needs Navigator fixes to implement deployment plans. Changes are tested in showcase context before being pushed upstream.\n\n**Location**: `navigator/` (Git submodule)\n**Language**: Go\n**Purpose**: Multi-tenant web server with framework independence\n\n## Project Structure\n\n```\nnavigator/\n├── cmd/navigator/            # Production Navigator implementation\n├── internal/                  # Modular packages\n│   ├── config/               # Configuration loading and parsing\n│   ├── server/               # HTTP handling, routing, static files\n│   ├── auth/                 # Authentication (htpasswd)\n│   ├── process/              # Web app lifecycle management\n│   └── proxy/                # Reverse proxy and Fly-Replay\n└── docs/                     # MkDocs documentation\n```\n\n## Critical Architecture: Configuration Flow\n\nUnderstanding this flow is essential for fixing config-related bugs:\n\n1. **YAML file** (user-facing config)\n2. **YAMLConfig struct** in `types.go` (mirrors YAML structure with yaml tags)\n3. **ConfigParser** in `parser.go` (converts YAML to internal format)\n4. **Config struct** in `types.go` (optimized internal representation)\n\n**Common Bug Pattern**: When adding config fields, developers often:\n- ✅ Add field to `Config` struct (internal)\n- ❌ FORGET to add field to `YAMLConfig` struct\n- ❌ FORGET to add parsing logic in `parser.go`\n- Result: YAML config is valid but silently ignored!\n\n## Request Flow in Handler (Order Matters!)\n\n```go\n// From internal/server/handler.go ServeHTTP()\n1. Health checks      // BEFORE everything (bypasses auth)\n2. Authentication     // EARLY check (before routing)\n3. Rewrites/redirects\n4. CGI scripts\n5. Reverse proxies\n6. Static files\n7. Maintenance mode\n8. Web app proxy      // Tenant routing\n```\n\n**Security Critical**: Health checks MUST come before auth. Auth MUST come before tenant routing.\n\n## Common Development Tasks\n\n### Adding a New Config Option\n\n**Example: Health Check Config (Recent Fix)**\n\n**Step 1**: Add to YAMLConfig in `internal/config/types.go`:\n```go\ntype YAMLConfig struct {\n    Server struct {\n        // ... other fields\n        HealthCheck HealthCheckConfig `yaml:\"health_check\"` // ← Must have yaml tag\n    } `yaml:\"server\"`\n}\n```\n\n**Step 2**: Add to internal Config (if different structure needed):\n```go\ntype Config struct {\n    Server struct {\n        // ... other fields\n        HealthCheck HealthCheckConfig\n    }\n}\n```\n\n**Step 3**: Add parsing logic in `internal/config/parser.go`:\n```go\nfunc (p *ConfigParser) parseServerConfig() {\n    // ... other parsing\n    p.config.Server.HealthCheck = p.yamlConfig.Server.HealthCheck // ← CRITICAL\n}\n```\n\n**Step 4**: Add tests in `internal/config/parser_test.go`:\n```go\nfunc TestConfigParser_ParseHealthCheck(t *testing.T) {\n    yamlConfig := YAMLConfig{}\n    yamlConfig.Server.HealthCheck = HealthCheckConfig{\n        Path: \"/up\",\n        Response: &HealthCheckResponse{Status: 200, Body: \"OK\"},\n    }\n\n    parser := NewConfigParser(&yamlConfig)\n    config, err := parser.Parse()\n\n    if config.Server.HealthCheck.Path != \"/up\" {\n        t.Errorf(\"HealthCheck not parsed correctly\")\n    }\n}\n```\n\n**Step 5**: Add integration tests in `internal/server/handler_test.go` (or appropriate file).\n\n### Adding Handler Behavior\n\n**Example: Health Check Handler (Recent Fix)**\n\n```go\n// Add early in ServeHTTP - order matters!\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    // Health checks BEFORE authentication\n    if h.config.Server.HealthCheck.Path != \"\" && r.URL.Path == h.config.Server.HealthCheck.Path {\n        h.handleHealthCheck(recorder, r)\n        return  // Stop processing\n    }\n\n    // Authentication check comes next\n    // ... rest of handler\n}\n```\n\n**Integration Test (Security Critical)**:\n```go\nfunc TestHandler_ServeHTTP_HealthCheckBeforeAuth(t *testing.T) {\n    cfg := &config.Config{}\n    cfg.Server.HealthCheck = config.HealthCheckConfig{\n        Path: \"/up\",\n        Response: &config.HealthCheckResponse{Status: 200, Body: \"OK\"},\n    }\n    cfg.Auth.Enabled = true  // Enable auth\n\n    basicAuth := &auth.BasicAuth{}\n    handler := CreateTestHandler(cfg, nil, basicAuth, nil)\n\n    req := httptest.NewRequest(\"GET\", \"/up\", nil)\n    // NO auth credentials provided\n    recorder := httptest.NewRecorder()\n    handler.ServeHTTP(recorder, req)\n\n    // MUST succeed without auth - critical security requirement\n    if recorder.Code != 200 {\n        t.Errorf(\"Health check should bypass auth\")\n    }\n}\n```\n\n## Testing Requirements\n\n### Test Checklist\n- [ ] **Config parser tests** (`internal/config/parser_test.go`)\n  - YAML unmarshaling works\n  - Parsing logic copies fields correctly\n  - Default values applied\n  - Edge cases (empty, nil)\n\n- [ ] **Integration tests** (appropriate `*_test.go`)\n  - Full request/response through `ServeHTTP`\n  - Security implications (especially auth bypass scenarios)\n  - Edge cases and error conditions\n\n### Running Tests\n\n```bash\n# All tests\ngo test ./...\n\n# Specific package\ngo test -v ./internal/config/\ngo test -v ./internal/server/\n\n# With coverage\ngo test -cover ./...\n\n# Pre-commit validation (CI requirements)\ngofmt -s -l . && \\\ngolangci-lint run && \\\ngo vet ./... && \\\ngo test -race -cover -timeout=3m ./... && \\\ngo build ./cmd/navigator-refactored && \\\necho \"✓ All CI checks passed!\"\n```\n\n## Recent Fixes (Nov 2024)\n\n### Fix 1: CGI reload_config Not Parsed\n**Problem**: `reload_config` field in CGI scripts ignored\n**Cause**: Field missing from `YAMLConfig.Server.CGIScripts`\n**Fix**: Added field to `CGIScriptConfig` struct\n**Lesson**: Always check YAMLConfig has all fields with yaml tags\n\n### Fix 2: Health Checks Not Working\n**Problem**: `/up` returned 401 (auth) or started index tenant unnecessarily\n**Cause**:\n- `YAMLConfig.Server` missing `HealthCheck` field\n- `parseServerConfig()` not copying health check config\n\n**Fix**:\n```go\n// types.go - Added to YAMLConfig.Server\nHealthCheck HealthCheckConfig `yaml:\"health_check\"`\n\n// parser.go - Added to parseServerConfig()\np.config.Server.HealthCheck = p.yamlConfig.Server.HealthCheck\n```\n\n**Test Coverage**: 270 lines added\n- Parser tests: YAML → Config transformation\n- Handler tests: before auth, different paths, custom headers\n- Security test: health checks bypass authentication\n\n## Common Bug Patterns\n\n### Pattern 1: Config Parser Forgot to Copy Field\n```go\n// ❌ BUG: Field exists in YAMLConfig but not copied\ntype YAMLConfig struct {\n    Server struct {\n        HealthCheck HealthCheckConfig `yaml:\"health_check\"`\n    }\n}\n\nfunc (p *ConfigParser) parseServerConfig() {\n    // ... other fields copied\n    // ❌ FORGOT: p.config.Server.HealthCheck = p.yamlConfig.Server.HealthCheck\n}\n\n// ✅ FIX: Add the copy statement\np.config.Server.HealthCheck = p.yamlConfig.Server.HealthCheck\n```\n\n**How to catch**: Write parser tests that verify field is copied.\n\n### Pattern 2: Missing yaml Tag\n```go\n// ❌ BUG: No yaml tag, field won't unmarshal\ntype ServerConfig struct {\n    Listen string  // Won't unmarshal from YAML\n}\n\n// ✅ FIX: Add yaml tag\ntype ServerConfig struct {\n    Listen string `yaml:\"listen\"`\n}\n```\n\n### Pattern 3: Wrong Handler Order\n```go\n// ❌ BUG: Auth check after tenant routing (can be bypassed!)\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    h.handleWebAppProxy(w, r)  // Routes first\n    if !h.auth.CheckAuth(r) {  // Auth never runs!\n        return\n    }\n}\n\n// ✅ FIX: Correct order\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    // 1. Health checks (before everything)\n    if h.config.Server.HealthCheck.Path != \"\" { /* ... */ }\n\n    // 2. Authentication (EARLY)\n    isPublic := auth.ShouldExcludeFromAuth(r.URL.Path, h.config)\n    needsAuth := h.auth.IsEnabled() && !isPublic\n    if needsAuth && !h.auth.CheckAuth(r) {\n        h.auth.RequireAuth(recorder)\n        return\n    }\n\n    // 3. Then routing\n    h.handleWebAppProxy(w, r)\n}\n```\n\n**How to catch**: Write integration tests that verify security (auth bypass scenarios).\n\n## Deployment Workflow\n\n1. **Rebase submodule on main before making changes**:\n```bash\ncd navigator\n\n# Fetch latest from remote\ngit fetch origin\n\n# Rebase on main (creates a clean linear history)\ngit rebase origin/main\n\n# If conflicts, resolve them and continue\n# git rebase --continue\n```\n\n2. **Work in Navigator submodule**:\n```bash\n# Make changes, run tests\ngo test ./...\ngolangci-lint run\ngit add -A\ngit commit -m \"Fix: health check parsing\"\n```\n\n3. **Test in showcase context**:\n```bash\ncd ..  # Back to showcase root\n# Test Navigator fix with showcase deployment\n```\n\n4. **Push to both repos**:\n```bash\n# Push Navigator changes\ncd navigator\ngit push origin HEAD:refs/heads/main\n\n# Update submodule reference in showcase\ncd ..\ngit add navigator\ngit commit -m \"Update navigator: fix health check parsing\"\ngit push\n```\n\n**Note**: If submodule is in detached HEAD state (common after `git submodule update`), the rebase step ensures you're building on the latest main branch before making changes.\n\n## Quick Reference Commands\n\n```bash\n# Build Navigator\ncd navigator\ngo build -o bin/navigator cmd/navigator\n\n# Or use make\nmake build\n\n# Run with config\n./bin/navigator config/navigator.yml\n\n# Reload config (SIGHUP)\nkill -HUP $(cat /tmp/navigator.pid)\n\n# Debug config loading\nLOG_LEVEL=debug ./bin/navigator config/navigator.yml\n```\n\n## When to Fix Navigator\n\nYou need Navigator changes when:\n- ✅ Deployment plan requires new config options\n- ✅ New routing/proxy behavior needed\n- ✅ Authentication/security enhancements required\n- ✅ Config not being parsed correctly (check YAMLConfig + parser)\n- ✅ Request flow order needs adjustment\n\n**Pattern**: Fix Navigator first, test in showcase, then continue deployment plan.\n\n## Essential Files\n\n- **`navigator/CLAUDE.md`** - Comprehensive development guide (read first!)\n- **`internal/config/types.go`** - All config structures (YAMLConfig + Config)\n- **`internal/config/parser.go`** - YAML → Config conversion\n- **`internal/server/handler.go`** - Main HTTP request routing\n\n## Getting Help\n\n1. Read `navigator/CLAUDE.md` for comprehensive guide\n2. Check `navigator/docs/` for user documentation\n3. Search tests for similar patterns\n4. Review Git history for \"Fix:\" commits\n5. Run with `LOG_LEVEL=debug` to see what's happening\n",
    "github": {
      "stars": 34,
      "forks": 6,
      "owner": "rubys",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/4815?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-24T21:15:09Z",
      "createdAt": "2022-04-06T12:34:32Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:14.021Z"
  },
  {
    "id": "clickup-workflow",
    "source": {
      "type": "github-search",
      "repo": "SOELexicon/clickup_framework",
      "path": ".claude/skills/workflow/skill.md",
      "url": "https://github.com/SOELexicon/clickup_framework/blob/main/.claude/skills/workflow/skill.md"
    },
    "frontmatter": {
      "name": "clickup-workflow",
      "description": "Guide for ClickUp + Git workflow - when and where to use commands",
      "tags": "[clickup, workflow, git, best-practices]"
    },
    "content": "# ClickUp + Git Workflow Guide\n\nThis guide shows the recommended workflow for using ClickUp commands (`cum`) alongside Git.\n\n## Installation\n\nBefore starting, ensure the ClickUp Framework is installed:\n\n```bash\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n```\n\nAnd set your API token:\n\n```bash\nexport CLICKUP_API_TOKEN=\"your_token_here\"\n# Add to ~/.bashrc or ~/.zshrc for persistence\n```\n\n## Starting a Task\n\n1. **View your assigned tasks:**\n   ```bash\n   cum a\n   ```\n\n2. **Set current task:**\n   ```bash\n   cum set task <task_id>\n   ```\n\n3. **View task details:**\n   ```bash\n   cum d <task_id>\n   ```\n\n4. **Update task status to \"In Development\":**\n   ```bash\n   cum tss <task_id> \"In Development\"\n   ```\n\n## During Development\n\n### Managing Dependencies\n\nIf your task depends on other tasks:\n\n```bash\n# Make current task wait for another task\ncum tad current --waiting-on <other_task_id>\n\n# Make current task block another task\ncum tad current --blocking <blocked_task_id>\n```\n\n### Linking Related Tasks\n\n```bash\n# Link related tasks together\ncum tal current <related_task_id>\n```\n\n### Adding Comments\n\n```bash\n# Add progress updates\ncum ca current \"Started working on feature X\"\n\n# Add comment from file\ncum ca current --comment-file progress_notes.md\n```\n\n### Viewing Context\n\n```bash\n# Check your current context\ncum show\n\n# View task with comments\ncum d current --show-comments 5\n```\n\n## Git Workflow\n\n### Creating Commits\n\n1. **Make your changes**\n2. **Stage and commit:**\n   ```bash\n   git add .\n   git commit -m \"Descriptive message\"\n   ```\n\n3. **Add comment to ClickUp task:**\n   ```bash\n   cum ca current \"Committed: <commit_message>\"\n   ```\n\n### Pushing Changes\n\n```bash\n# Push to remote\ngit push -u origin <branch-name>\n```\n\n## Completing a Task\n\n1. **Run tests (if applicable)**\n2. **Update subtasks if any:**\n   ```bash\n   cum tss <subtask_id> \"Complete\"\n   ```\n\n3. **Update main task:**\n   ```bash\n   cum tss current \"Complete\"\n   ```\n\n4. **Add final comment:**\n   ```bash\n   cum ca current \"Task completed and pushed to branch <branch-name>\"\n   ```\n\n5. **Create PR if needed:**\n   ```bash\n   gh pr create --title \"Title\" --body \"Description\"\n   ```\n\n## Common Command Aliases\n\n| Command | Alias | Description |\n|---------|-------|-------------|\n| `assigned` | `a` | View assigned tasks |\n| `detail` | `d` | View task details |\n| `hierarchy` | `h` `ls` `l` | View task hierarchy |\n| `set_current` | `set` | Set current context |\n| `show_current` | `show` | Show current context |\n| `task_set_status` | `tss` | Set task status |\n| `task_create` | `tc` | Create new task |\n| `task_update` | `tu` | Update task |\n| `task_add_dependency` | `tad` | Add dependency |\n| `task_remove_dependency` | `trd` | Remove dependency |\n| `task_add_link` | `tal` | Link tasks |\n| `task_remove_link` | `trl` | Unlink tasks |\n| `comment_add` | `ca` | Add comment |\n| `comment_list` | `cl` | List comments |\n\n## Quick Examples\n\n### Example 1: Starting a new task\n```bash\n# View assigned tasks\ncum a\n\n# Set current task\ncum set task 86c6e0q12\n\n# Update status\ncum tss current \"In Development\"\n\n# Add initial comment\ncum ca current \"Starting work on this task\"\n```\n\n### Example 2: Completing with dependencies\n```bash\n# Mark dependency complete\ncum tss 86c6e0q16 \"Complete\"\n\n# Remove dependency\ncum trd current --waiting-on 86c6e0q16\n\n# Mark current task complete\ncum tss current \"Complete\"\n```\n\n### Example 3: Creating a subtask\n```bash\n# Create subtask with parent (no --list needed)\ncum tc \"Subtask name\" --parent <parent_task_id> --status \"In Development\"\n```\n\n### Example 4: Daily standup workflow\n```bash\n# Check your tasks\ncum a\n\n# View task hierarchy with comments\ncum h current --show-comments 3\n\n# Update progress\ncum ca current \"Today: Completed authentication module, blocked on API review\"\n```\n\n## Pro Tips\n\n1. **Always use `cum show`** to verify your current context before running commands with \"current\"\n2. **Use `cum d <task_id> --show-comments 5`** to see recent comments and context\n3. **Add comments frequently** to track progress: `cum ca current \"message\"`\n4. **Use dependencies** (`cum tad`) to enforce task order and prevent premature status changes\n5. **Use links** (`cum tal`) for related tasks that don't have strict ordering\n6. **Set default assignee** in context to auto-assign new tasks\n7. **Use `--description-file` and `--comment-file`** for longer content from files\n8. **Leverage short codes** - `cum h` is faster than `cum hierarchy`\n\n## Context Management Tips\n\n```bash\n# Set up your default working context once\ncum set workspace <workspace_id>\ncum set list <list_id>\ncum set assignee <your_user_id>\n\n# Now all commands use these defaults\ncum tc \"Quick task\"  # Auto-uses current list and assignee\ncum h current        # Shows current list hierarchy\ncum a                # Shows tasks for current assignee\n```\n\n## Integration with CI/CD\n\nAdd ClickUp updates to your CI/CD pipeline:\n\n```yaml\n# .github/workflows/deploy.yml\n- name: Update ClickUp Task\n  run: |\n    pip install git+https://github.com/SOELexicon/clickup_framework.git\n    cum ca $TASK_ID \"Deployed to production: ${{ github.sha }}\"\n  env:\n    CLICKUP_API_TOKEN: ${{ secrets.CLICKUP_API_TOKEN }}\n    TASK_ID: ${{ github.event.inputs.task_id }}\n```\n\n---\n\nFor complete command reference: See the CLI Reference skill or run `/cum` in Claude Code.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "SOELexicon",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/4205001?v=4",
      "lastCommit": "2025-12-30T21:37:58Z",
      "createdAt": "2025-11-08T00:31:11Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:14.796Z"
  },
  {
    "id": "example-skill",
    "source": {
      "type": "github-search",
      "repo": "phil65/agentpool",
      "path": ".claude/skills/example-skill/SKILL.md",
      "url": "https://github.com/phil65/agentpool/blob/main/.claude/skills/example-skill/SKILL.md"
    },
    "frontmatter": {
      "name": "Example Skill",
      "description": "A simple example skill that demonstrates the basic structure and functionality. Use when you need to show how skills work or test skill loading."
    },
    "content": "# Example Skill\n\n## Overview\n\nThis is an example skill that demonstrates the basic structure of a Claude Code Skill. It shows how to organize instructions, provide examples, and reference additional resources.\n\n## Quick Start\n\nWhen using this skill, follow these steps:\n\n1. **Identify the task**: Determine what specific functionality is needed\n2. **Check resources**: Look for additional files in the skill directory\n3. **Follow patterns**: Use the examples provided as templates\n\n## Examples\n\n### Basic Usage\n```python\n# Example of how this skill might be used\ndef example_function():\n    print(\"This is an example from the skill\")\n    return \"skill_result\"\n```\n\n### Advanced Usage\n```bash\n# Command line example\necho \"Running skill example\"\n```\n\n## Additional Resources\n\nThis skill may contain additional files:\n- `helpers.py`: Utility functions\n- `templates/`: Example templates\n- `docs/`: Extended documentation\n\n## Best Practices\n\n- Always validate inputs\n- Provide clear error messages  \n- Document any assumptions\n- Test functionality before deployment",
    "github": {
      "stars": 41,
      "forks": 13,
      "owner": "phil65",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/110931?v=4",
      "license": "MIT",
      "lastCommit": "2026-01-04T08:30:55Z",
      "createdAt": "2024-12-05T12:50:17Z",
      "topics": [
        "acp",
        "agent",
        "ai",
        "litellm",
        "llm",
        "llm-agent",
        "mcp",
        "pydantic",
        "pydantic-ai",
        "yaml"
      ]
    },
    "scrapedAt": "2026-01-04T13:03:15.416Z"
  },
  {
    "id": "nextjs-16",
    "source": {
      "type": "github-search",
      "repo": "putto11262002/jimmodel",
      "path": ".claude/skills/nextjs-16/skill.md",
      "url": "https://github.com/putto11262002/jimmodel/blob/main/.claude/skills/nextjs-16/skill.md"
    },
    "frontmatter": {},
    "content": "# Next.js 16 Skill\n\n**Use this skill when working with Next.js 16-specific features, breaking changes, and modern patterns.**\n\nThis skill captures Next.js 16-specific conventions and critical breaking changes. Assumes familiarity with Next.js fundamentals (App Router, Server Components, etc.).\n\n---\n\n## Documentation Lookup\n\n**When you need Next.js documentation:**\n\n1. Use the `mcp__context7__get-library-docs` tool with library ID: `/vercel/next.js`\n2. Start with **2000 tokens**, increase if information not found\n3. Specify a focused `topic` parameter (e.g., \"caching\", \"routing\", \"server actions\")\n\n```typescript\n// Example: Look up Next.js caching documentation\nmcp__context7__get-library-docs({\n  context7CompatibleLibraryID: \"/vercel/next.js\",\n  tokens: 2000,\n  topic: \"caching\"\n})\n```\n\n---\n\n## Breaking Changes (CRITICAL)\n\n### 1. Async Request APIs\n\n**All request-scoped APIs are now async and require `await`:**\n\n```typescript\n// ❌ OLD (Next.js 15 and earlier)\nexport default function Page({ params, searchParams }) {\n  const id = params.id;\n  const query = searchParams.query;\n}\n\n// ✅ NEW (Next.js 16)\nexport default async function Page({ params, searchParams }) {\n  const { id } = await params;\n  const { query } = await searchParams;\n}\n```\n\n**Applies to:**\n- `params` (dynamic route segments)\n- `searchParams` (URL query parameters)\n- `cookies()` from `next/headers`\n- `headers()` from `next/headers`\n- `draftMode()` from `next/headers`\n\n```typescript\n// ✅ Correct usage\nimport { cookies, headers, draftMode } from 'next/headers';\n\nexport async function MyServerComponent() {\n  const cookieStore = await cookies();\n  const headersList = await headers();\n  const draft = await draftMode();\n}\n```\n\n### 2. Parallel Routes: Explicit `default.js` Required\n\nAll parallel route slots MUST have a `default.js` file:\n\n```\napp/\n├── @modal/\n│   ├── default.js    ← Required!\n│   └── login/page.js\n└── page.js\n```\n\n```typescript\n// app/@modal/default.js\nexport default function Default() {\n  return null;\n}\n```\n\n### 3. `proxy.ts` Replaces `middleware.ts`\n\nThe `middleware.ts` filename is deprecated. Use `proxy.ts`:\n\n```typescript\n// ✅ proxy.ts (new)\nexport function proxy(request: NextRequest) {\n  // Middleware logic\n}\n\n// ❌ middleware.ts (deprecated)\nexport function middleware(request: NextRequest) {\n  // Old approach\n}\n```\n\n### 4. `revalidateTag()` Requires `cacheLife`\n\n```typescript\n// ❌ OLD\nrevalidateTag('posts');\n\n// ✅ NEW\nrevalidateTag('posts', {\n  cacheLife: 'default' // or 'aggressive', 'minimal'\n});\n```\n\n### 5. Images Configuration\n\n- `minimumCacheTTL` default: 60s → **4 hours**\n- `images.domains` deprecated → use `images.remotePatterns`\n\n```typescript\n// next.config.ts\nexport default {\n  images: {\n    remotePatterns: [\n      {\n        protocol: 'https',\n        hostname: 'example.com',\n        pathname: '/images/**',\n      },\n    ],\n  },\n};\n```\n\n---\n\n## New Features\n\n### Cache Components (`\"use cache\"`)\n\nOpt-in caching for pages, components, and functions:\n\n```typescript\n\"use cache\";\nexport async function CachedComponent() {\n  const data = await fetch('/api/data');\n  return <div>{data}</div>;\n}\n```\n\n**Note:** Detailed caching patterns are documented separately.\n\n---\n\n## React 19 Patterns\n\nNext.js 16 ships with React 19.2. Key features:\n\n### View Transitions API\n\n```typescript\n\"use client\";\nimport { useTransition } from 'react';\n\nexport function NavigationLink({ href, children }) {\n  const [isPending, startTransition] = useTransition();\n\n  const navigate = () => {\n    startTransition(() => {\n      // Trigger navigation with view transition\n      document.startViewTransition(() => {\n        // Navigation logic\n      });\n    });\n  };\n\n  return <button onClick={navigate}>{children}</button>;\n}\n```\n\n### `useEffectEvent()` Hook\n\nSeparates event handlers from reactive dependencies:\n\n```typescript\n\"use client\";\nimport { useEffectEvent } from 'react';\n\nfunction Component({ url, onChange }) {\n  const onUpdate = useEffectEvent(onChange);\n\n  useEffect(() => {\n    // onUpdate won't trigger re-runs when onChange changes\n    fetch(url).then(data => onUpdate(data));\n  }, [url]); // Only re-run when url changes\n}\n```\n\n### `<Activity>` Component\n\nFor loading states and suspense boundaries:\n\n```typescript\nimport { Activity } from 'react';\n\nexport default function Page() {\n  return (\n    <Activity fallback={<Loading />}>\n      <AsyncContent />\n    </Activity>\n  );\n}\n```\n\n---\n\n## Removed Features\n\n- AMP support\n- `next lint` command (use `eslint` directly)\n- `serverRuntimeConfig` / `publicRuntimeConfig`\n- `next/legacy/image`\n- `experimental.turbopack` config (moved to top-level)\n\n---\n\n## Requirements\n\n- **Node.js:** 20.9+\n- **TypeScript:** 5.1+\n- **React:** 19.x\n\n---\n\n## Resources\n\n- [Next.js 16 Blog](https://nextjs.org/blog/next-16) - Full breaking changes and migration details\n- [Next.js Docs via Context7](/) - Use library ID: `/vercel/next.js`\n\n---\n\n## Project-Specific Conventions\n\n<!-- Document your project's Next.js patterns here -->\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "putto11262002",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/85939355?v=4",
      "lastCommit": "2026-01-03T10:51:06Z",
      "createdAt": "2025-11-04T07:19:35Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:16.213Z"
  },
  {
    "id": "render-page",
    "source": {
      "type": "github-search",
      "repo": "rubys/showcase",
      "path": ".claude/skills/render-page/SKILL.md",
      "url": "https://github.com/rubys/showcase/blob/main/.claude/skills/render-page/SKILL.md"
    },
    "frontmatter": {
      "name": "render-page",
      "description": "Use this skill to inspect what a Rails page currently displays, extract HTML content, or verify rendering WITHOUT starting a dev server. Useful for understanding page output before making changes, debugging views, searching for content, or testing that pages work correctly. Provides scripts/render.rb for quick page inspection and HTML extraction."
    },
    "content": "# Render Pages Without Starting a Server\n\nUse the `scripts/render.rb` tool to verify page rendering, extract HTML, or search content in rendered pages.\n\n### Basic Usage\n\nThe script accepts the database either as an argument or via RAILS_APP_DB environment variable:\n\n```bash\n# Pass database as argument\nscripts/render.rb db/2025-alexandria-80-s-neon-nights.sqlite3 /people\n\n# Or just the database name\nscripts/render.rb 2025-alexandria-80-s-neon-nights /people\n\n# Or use environment variable\nRAILS_APP_DB=2025-alexandria-80-s-neon-nights scripts/render.rb /people\n```\n\nCommon operations:\n\n```bash\n# Check if pages render successfully\nscripts/render.rb 2025-alexandria-80-s-neon-nights --check /people /heats /solos\n\n# Show summary with page sizes\nscripts/render.rb 2025-alexandria-80-s-neon-nights /people /heats\n\n# Get full HTML output (single page only)\nscripts/render.rb 2025-alexandria-80-s-neon-nights --html /solos\n\n# Search for specific content in rendered pages\nscripts/render.rb 2025-alexandria-80-s-neon-nights --search \"Solos\" /solos\n```\n\n### Script Options\n\n- `--check` - Only check if page renders (exit 0 on success, 1 on failure)\n- `--html` - Output full HTML content (works with single path only)\n- `--search TEXT` - Search for specific text in rendered output\n- `--verbose, -v` - Show detailed information\n- `--help, -h` - Show help message\n\n## Common Workflows\n\n### 1. Verify Pages Render Successfully\n\nTest multiple pages at once and see their sizes:\n\n```bash\nscripts/render.rb db/DATABASE.sqlite3 /people /heats /solos\n```\n\nFor CI/CD pipelines, use `--check` mode (silent, exit code indicates success):\n\n```bash\nscripts/render.rb db/DATABASE.sqlite3 --check /people /heats /solos\necho $?  # 0 = all succeeded, 1 = at least one failed\n```\n\n### 2. Search for Content in Rendered Pages\n\nVerify specific content appears in a page:\n\n```bash\nscripts/render.rb db/DATABASE.sqlite3 --search \"Rhythm Solos\" /solos\n# Output: ✓ /solos - 'Rhythm Solos' found\n```\n\n### 3. Extract HTML for Analysis\n\nSave rendered HTML to a file for inspection:\n\n```bash\nscripts/render.rb db/DATABASE.sqlite3 --html /heats > heats.html\n```\n\n## Advanced: Custom Scripts with Rails API\n\nFor complex custom logic, write a Ruby script using Rails' routing API directly:\n\n```ruby\n# custom_test.rb\nenv = { \"PATH_INFO\" => '/heats', \"REQUEST_METHOD\" => \"GET\" }\ncode, headers, response = Rails.application.routes.call(env)\n\nif code == 200\n  html = response.body.force_encoding('utf-8')\n  puts \"Success: #{html.length} bytes\"\nelse\n  puts \"Error: #{code}\"\n  exit 1\nend\n```\n\nRun with `bin/run db/DATABASE.sqlite3 custom_test.rb`\n\nSee `lib/tasks/prerender.rake` for a production example of this technique.\n",
    "github": {
      "stars": 34,
      "forks": 6,
      "owner": "rubys",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/4815?v=4",
      "license": "MIT",
      "lastCommit": "2025-12-24T21:15:09Z",
      "createdAt": "2022-04-06T12:34:32Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:16.834Z"
  },
  {
    "id": "hatena-blog-markdown-extractor",
    "source": {
      "type": "github-search",
      "repo": "shoji-sws/hatena-md-skill",
      "path": ".claude/skills/hatena-md/skill.md",
      "url": "https://github.com/shoji-sws/hatena-md-skill/blob/main/.claude/skills/hatena-md/skill.md"
    },
    "frontmatter": {
      "name": "\"hatena-blog-markdown-extractor\"",
      "description": "\"Extract content from Hatena Blog article URLs and convert to Markdown format with frontmatter. Preserves images as absolute URLs and removes sidebars, headers, footers, and ads.\""
    },
    "content": "# Hatena Blog Markdown Extractor\n\nはてなブログの記事URLからコンテンツをMarkdown形式で抽出するスキルです。\n\n## 機能\n\n- はてなブログの記事URLから本文を抽出\n- HTML → Markdown変換\n- 画像URLをフルパス(絶対パス)で保持\n- サイドバー、ヘッダー、フッター、広告などの不要な要素を自動除去\n- フロントマター付きMarkdownファイルを生成\n\n## 使用方法\n\n```bash\n# 単一のURLから抽出\n/hatena-md <記事URL>\n\n# カスタム出力ディレクトリを指定\n/hatena-md <記事URL> <出力ディレクトリ>\n```\n\n## 例\n\n```bash\n# デフォルトの出力先 (./hatena-posts/) に保存\n/hatena-md https://www.simple-web-system.work/entry/2024/10/17/223212\n\n# カスタム出力先に保存\n/hatena-md https://www.simple-web-system.work/entry/2024/10/17/223212 ./my-posts\n```\n\n## 出力形式\n\n抽出されたMarkdownファイルには以下のフロントマターが付きます:\n\n```markdown\n---\ntitle: \"記事タイトル\"\nurl: https://www.simple-web-system.work/entry/2024/10/17/223212\nextracted: 2025-10-27T09:46:00.000Z\n---\n\n記事本文...\n```\n\n## 出力先\n\n- デフォルト: `./hatena-posts/`\n- ファイル名: 記事タイトル.md (特殊文字は除去・置換されます)\n\n## 注意事項\n\n- Node.jsプロジェクトとしてセットアップされている必要があります\n- 実行前に `npm install` と `npm run build` を実行してください\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "shoji-sws",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/32668181?v=4",
      "lastCommit": "2025-10-27T14:05:03Z",
      "createdAt": "2025-10-27T14:04:47Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:17.466Z"
  },
  {
    "id": "wechat-ops",
    "source": {
      "type": "github-search",
      "repo": "garychai/wechat-operation",
      "path": ".claude/skills/wechat-ops/skill.md",
      "url": "https://github.com/garychai/wechat-operation/blob/main/.claude/skills/wechat-ops/skill.md"
    },
    "frontmatter": {},
    "content": "# 微信公众号运营 Skill\n\n这是一个微信公众号运营技能，允许你通过自然语言管理微信公众号，包括创建文章草稿、发布文章和分析数据。\n\n## 可用功能\n\n### 1. 草稿管理\n- **创建草稿**: 创建新的文章草稿\n- **查看草稿列表**: 列出所有草稿\n- **查看草稿详情**: 获取特定草稿的详细信息\n- **删除草稿**: 删除指定的草稿\n- **获取草稿总数**: 查询草稿箱中的草稿总数\n\n### 2. 文章发布\n- **发布文章**: 将草稿发布到公众号\n- **查看发布状态**: 检查文章发布状态\n- **查看已发布文章**: 列出已发布的文章\n- **获取文章详情**: 获取已发布文章的详细信息\n- **删除已发布文章**: 删除已发布的文章\n\n### 3. 数据分析\n- **分析文章数据**: 查看文章的阅读量、分享数等统计数据\n- **自定义日期范围分析**: 分析指定日期范围内的数据\n\n### 4. 素材管理\n- **上传图片**: 上传图片作为封面或文章素材\n\n## 使用方法\n\n当用户请求执行微信公众号相关操作时，使用Python脚本调用相应的功能。\n\n### 工具路径\nPython脚本位于: `/home/user/wechat-operation/src/wechat_operations.py`\n\n### 配置文件\n需要配置文件: `/home/user/wechat-operation/config.json`\n\n配置文件格式:\n```json\n{\n  \"app_id\": \"你的微信公众号AppID\",\n  \"app_secret\": \"你的微信公众号AppSecret\"\n}\n```\n\n## 命令示例\n\n### 草稿管理\n\n#### 创建草稿\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py draft create \\\n  --title \"文章标题\" \\\n  --content \"文章内容或文件路径\" \\\n  --author \"作者名\" \\\n  --thumb \"封面图片素材ID\"\n```\n\n#### 列出草稿\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py draft list --offset 0 --count 20\n```\n\n#### 删除草稿\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py draft delete MEDIA_ID\n```\n\n### 发布管理\n\n#### 发布文章\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py publish submit MEDIA_ID\n```\n\n#### 查看发布状态\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py publish status PUBLISH_ID\n```\n\n#### 列出已发布文章\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py publish list --offset 0 --count 20\n```\n\n### 数据分析\n\n#### 分析最近N天的数据\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py analytics --days 7\n```\n\n#### 分析指定日期范围\n```bash\npython3 /home/user/wechat-operation/src/wechat_operations.py analytics \\\n  --begin 2024-01-01 \\\n  --end 2024-01-07\n```\n\n## 处理用户请求的指南\n\n当用户提出以下类型的请求时，使用此技能：\n\n### 1. 创建和发布文章\n**用户**: \"帮我写一篇关于XX的公众号文章并发布\"\n**处理步骤**:\n1. 根据用户需求生成文章内容\n2. 使用 `draft create` 创建草稿\n3. 获取返回的 media_id\n4. 使用 `publish submit` 发布文章\n5. 向用户报告发布状态\n\n### 2. 管理草稿\n**用户**: \"查看我的草稿列表\" 或 \"删除某个草稿\"\n**处理步骤**:\n1. 使用 `draft list` 查看草稿\n2. 根据需要使用 `draft delete` 删除草稿\n\n### 3. 数据分析\n**用户**: \"分析一下我最近的文章表现\" 或 \"看看上周文章的阅读量\"\n**处理步骤**:\n1. 使用 `analytics` 命令获取数据\n2. 解析返回的JSON数据\n3. 用易读的方式向用户展示关键指标（阅读量、分享数等）\n\n## 注意事项\n\n1. **首次使用**: 确保用户已经配置了 `config.json` 文件\n2. **素材ID**: 创建文章时如果需要封面图，需要先上传图片获取 thumb_media_id\n3. **内容格式**: 文章内容支持HTML格式\n4. **API限制**: 注意微信API的调用频率限制\n5. **日期范围**: 数据统计API有日期范围限制（如阅读数据最大3天）\n\n## 错误处理\n\n当API返回错误时，检查返回的 `errcode` 和 `errmsg` 字段，向用户提供清晰的错误说明。\n\n常见错误码:\n- `40001`: access_token 过期或无效\n- `40007`: 非法的 media_id\n- `45009`: 接口调用超过限制\n- `47001`: POST 数据格式错误\n- `85064`: 找不到草稿\n\n## API文档参考\n\n基于微信公众号官方API:\n- 草稿箱管理: https://developers.weixin.qq.com/doc/offiaccount/Draft_Box/Add_draft.html\n- 发布能力: https://developers.weixin.qq.com/doc/offiaccount/Publish/Publish.html\n- 数据统计: https://developers.weixin.qq.com/doc/offiaccount/Analytics/Analytics_API.html\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "garychai",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/564722?v=4",
      "lastCommit": "2025-10-21T05:40:12Z",
      "createdAt": "2025-10-21T04:21:32Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:18.295Z"
  },
  {
    "id": "qa-auditor",
    "source": {
      "type": "github-search",
      "repo": "Kandil7/prprompts-flutter-generator",
      "path": ".claude/skills/automation/qa-auditor/skill.md",
      "url": "https://github.com/Kandil7/prprompts-flutter-generator/blob/main/.claude/skills/automation/qa-auditor/skill.md"
    },
    "frontmatter": {},
    "content": "# QA Auditor Automation Skill\n\n## Skill Overview\n\nYou are an expert QA auditor specializing in comprehensive Flutter application audits for production readiness, compliance certification, and enterprise deployment. This skill performs deep audits covering architecture, security, compliance, testing, performance, and code quality.\n\n**What This Skill Does:**\n- Comprehensive audit across 6 categories (architecture, security, compliance, testing, performance, code quality)\n- Validates compliance with HIPAA, PCI-DSS, GDPR, SOC2, COPPA, FERPA\n- Generates certification-ready audit reports\n- Identifies production blockers and critical issues\n- Provides actionable remediation steps\n- Issues pass/fail certification based on threshold\n\n**Execution Time:** 10-20 minutes\n\n---\n\n## Step 1: Architecture Audit\n\n### 1.1 Clean Architecture Validation\n\n**Check:**\n- ✅ Proper layer separation (domain/data/presentation)\n- ✅ Dependency flow (presentation → domain ← data)\n- ✅ No circular dependencies\n- ✅ Repository pattern correctly implemented\n- ✅ Use case single responsibility\n\n**Score: /25 points**\n\n---\n\n## Step 2: Security Audit\n\n### 2.1 Authentication Security\n- JWT verification (not signing)\n- Password security (no storage)\n- Session management\n- MFA implementation (if applicable)\n\n### 2.2 Data Protection\n- Encryption at rest (FlutterSecureStorage)\n- HTTPS enforcement\n- API key protection\n- Sensitive data handling\n\n### 2.3 Compliance-Specific Checks\n\n**HIPAA:**\n- PHI encryption (AES-256-GCM)\n- Audit logging\n- Session timeout (15 min)\n- No PHI in logs\n\n**PCI-DSS:**\n- No card storage\n- Tokenization\n- Only last 4 digits displayed\n- SAQ validation\n\n**GDPR:**\n- Data export capability\n- Right to deletion\n- Consent management\n- Privacy policy\n\n**Score: /25 points**\n\n---\n\n## Step 3: Testing Audit\n\n### 3.1 Coverage Analysis\n```bash\nflutter test --coverage\ngenhtml coverage/lcov.info -o coverage/html\n```\n\n**Requirements:**\n- Domain: 90%+\n- Data: 80%+\n- Presentation: 60%+\n- Overall: 70%+\n\n### 3.2 Test Quality\n- Unit tests: Complete with mocks\n- Widget tests: User interactions\n- Integration tests: Complete flows\n- No empty tests\n- All assertions present\n\n**Score: /20 points**\n\n---\n\n## Step 4: Compliance Audit\n\n**Per Standard (HIPAA, PCI-DSS, GDPR, etc.):**\n\n✅ Required controls implemented\n✅ Documentation complete\n✅ Audit trail enabled\n✅ Security measures in place\n✅ User consent obtained\n✅ Data minimization applied\n\n**Score: /15 points**\n\n---\n\n## Step 5: Performance Audit\n\n### 5.1 App Performance\n- Launch time < 2 seconds\n- Screen transitions < 300ms\n- API calls timeout properly\n- Memory leaks checked\n- Frame rate 60fps\n\n### 5.2 Network Performance\n- Caching implemented\n- Offline mode (if required)\n- Retry logic\n- Error handling\n\n**Score: /10 points**\n\n---\n\n## Step 6: Code Quality Audit\n\n### 6.1 Static Analysis\n```bash\nflutter analyze\n```\n\n**Requirements:**\n- Zero errors\n- Zero warnings\n- Minimal info messages\n\n### 6.2 Code Standards\n- Naming conventions\n- Documentation\n- TODOs addressed\n- Dead code removed\n\n**Score: /5 points**\n\n---\n\n## Step 7: Generate Audit Report\n\n### 7.1 Calculate Overall Score\n\n```\nOverall Score =\n  Architecture (25%) +\n  Security (25%) +\n  Testing (20%) +\n  Compliance (15%) +\n  Performance (10%) +\n  Code Quality (5%)\n\nPass/Fail:\n- Pass: Score >= fail_threshold\n- Fail: Score < fail_threshold OR critical_issues > 0\n```\n\n### 7.2 Create Audit Report\n\n```markdown\n# QA Audit Report\n\n**Date:** {{date}}\n**Audit Type:** {{audit_type}}\n**Auditor:** QA Auditor Skill v1.0.0\n\n---\n\n## Overall Result: {{PASS|FAIL}}\n\n**Overall Score:** {{score}}/100\n\n**Threshold:** {{fail_threshold}}\n**Status:** {{score >= fail_threshold ? \"✅ PASS\" : \"❌ FAIL\"}}\n\n---\n\n## Scores by Category\n\n| Category | Score | Weight | Status |\n|----------|-------|--------|--------|\n| Architecture | {{arch_score}}/25 | 25% | {{status}} |\n| Security | {{sec_score}}/25 | 25% | {{status}} |\n| Testing | {{test_score}}/20 | 20% | {{status}} |\n| Compliance | {{comp_score}}/15 | 15% | {{status}} |\n| Performance | {{perf_score}}/10 | 10% | {{status}} |\n| Code Quality | {{qual_score}}/5 | 5% | {{status}} |\n\n---\n\n## Critical Issues: {{critical_count}}\n\n{{#if critical_issues}}\n❌ **PRODUCTION BLOCKER:** Critical issues must be fixed before deployment.\n\n{{#each critical_issues}}\n### {{@index}}. {{this.title}}\n- **Category:** {{this.category}}\n- **Severity:** 🔴 CRITICAL\n- **Impact:** {{this.impact}}\n- **File:** {{this.file}}:{{this.line}}\n- **Issue:** {{this.description}}\n- **Fix:** {{this.remediation}}\n- **Compliance Impact:** {{this.compliance_impact}}\n{{/each}}\n{{else}}\n✅ No critical issues found.\n{{/if}}\n\n---\n\n## Compliance Summary\n\n{{#each compliance_standards}}\n### {{this.name}} Compliance\n\n**Overall:** {{this.score}}/100 ({{this.status}})\n\n**Requirements Met:** {{this.requirements_met}}/{{this.total_requirements}}\n\n**Controls:**\n{{#each this.controls}}\n- {{this.passed ? \"✅\" : \"❌\"}} {{this.name}}: {{this.description}}\n{{/each}}\n\n{{#if this.violations}}\n**Violations:**\n{{#each this.violations}}\n- {{this.severity}} {{this.description}} ({{this.control_id}})\n  - Remediation: {{this.remediation}}\n{{/each}}\n{{/if}}\n{{/each}}\n\n---\n\n## Detailed Findings\n\n### Architecture ({{arch_score}}/25)\n\n✅ **Strengths:**\n- Clean Architecture structure properly implemented\n- Layer separation maintained\n- Repository pattern used correctly\n\n⚠️ **Issues:**\n{{#each arch_issues}}\n- {{this.description}} ({{this.severity}})\n{{/each}}\n\n### Security ({{sec_score}}/25)\n\n{{#if sec_score >= 20}}\n✅ **Excellent security posture**\n{{else if sec_score >= 15}}\n⚠️ **Acceptable with improvements needed**\n{{else}}\n❌ **Security issues require immediate attention**\n{{/if}}\n\n**Findings:**\n{{#each sec_findings}}\n- {{this.description}} ({{this.severity}})\n{{/each}}\n\n### Testing ({{test_score}}/20)\n\n**Coverage:** {{coverage}}%\n**Tests:** {{test_count}} total\n\n{{#if test_score >= 16}}\n✅ **Well-tested application**\n{{else}}\n⚠️ **Test coverage below recommended levels**\n{{/if}}\n\n**Missing Tests:**\n{{#each missing_tests}}\n- {{this.file}} ({{this.current_coverage}}% coverage)\n{{/each}}\n\n---\n\n## Recommendations\n\n### Immediate Actions (Before Production)\n\n{{#each immediate_actions}}\n{{@index}}. **{{this.title}}**\n   - Priority: {{this.priority}}\n   - Effort: {{this.effort}}\n   - Fix: {{this.fix}}\n{{/each}}\n\n### Short-term Improvements\n\n{{#each short_term_improvements}}\n{{@index}}. {{this.description}}\n{{/each}}\n\n### Long-term Enhancements\n\n{{#each long_term_enhancements}}\n{{@index}}. {{this.description}}\n{{/each}}\n\n---\n\n## Certification\n\n{{#if audit_passed}}\n✅ **PASSED QA AUDIT**\n\nThis application has passed the QA audit with a score of {{score}}/100.\n\n**Certification Details:**\n- Audit Date: {{date}}\n- Audit Type: {{audit_type}}\n- Compliance Standards: {{compliance_standards}}\n- Valid Until: {{expiration_date}} (90 days)\n\n**Certificate Generated:** {{certificate_path}}\n\n**Auditor Signature:**\nQA Auditor Skill v1.0.0\n🤖 Generated with Claude Code\n\n{{else}}\n❌ **FAILED QA AUDIT**\n\nThis application did not pass the QA audit.\n\n**Reasons for Failure:**\n{{#each failure_reasons}}\n- {{this}}\n{{/each}}\n\n**Required Actions:**\n1. Fix all critical issues\n2. Address compliance violations\n3. Re-run audit after fixes\n\n**To Re-Audit:**\n```bash\n@claude use skill automation/qa-auditor\n```\n{{/if}}\n\n---\n\n**Report Generated:** {{timestamp}}\n**Report Path:** {{report_path}}\n\n🤖 Generated with Claude Code\n```\n\n---\n\n## Skill Completion\n\n```markdown\n✅ QA Audit Complete!\n\n**Result:** {{PASS|FAIL}}\n**Score:** {{score}}/100\n**Critical Issues:** {{critical_count}}\n**Report:** {{report_path}}\n\n{{#if audit_passed}}\n🎉 Application is production-ready!\n{{else}}\n⚠️  Fix issues and re-audit before production deployment.\n{{/if}}\n```\n\n---\n\n**End of Skill Execution**\n",
    "github": {
      "stars": 3,
      "forks": 1,
      "owner": "Kandil7",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/105888370?v=4",
      "license": "MIT",
      "lastCommit": "2025-11-27T20:21:32Z",
      "createdAt": "2025-10-15T17:35:57Z",
      "topics": [
        "bloc",
        "claude-code",
        "clean-architecture",
        "compliance",
        "flutter",
        "gdpr",
        "gemini",
        "hipaa",
        "pci-dss",
        "prprompts",
        "qwen"
      ]
    },
    "scrapedAt": "2026-01-04T13:03:19.079Z"
  },
  {
    "id": "clickup-setup-guide",
    "source": {
      "type": "github-search",
      "repo": "SOELexicon/clickup_framework",
      "path": ".claude/skills/setup-guide/skill.md",
      "url": "https://github.com/SOELexicon/clickup_framework/blob/main/.claude/skills/setup-guide/skill.md"
    },
    "frontmatter": {
      "name": "clickup-setup-guide",
      "description": "Complete setup and installation guide for the ClickUp Framework CLI with troubleshooting",
      "tags": "[clickup, installation, setup, configuration]"
    },
    "content": "# ClickUp Framework Setup Guide\n\nComplete guide to installing and configuring the ClickUp Framework CLI.\n\n## Prerequisites\n\n- Python 3.8 or higher\n- pip (Python package installer)\n- Git (for installation from repository)\n- ClickUp API Token ([Get one here](https://app.clickup.com/settings/apps))\n\n## Installation\n\n### Method 1: Install from GitHub (Recommended)\n\nInstall the latest version directly from GitHub:\n\n```bash\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n**Flags explained:**\n- `--upgrade`: Upgrade if already installed\n- `--force-reinstall`: Force reinstall even if up-to-date\n- `git+https://...`: Install from Git repository\n\n### Method 2: Install in Virtual Environment (Recommended for Development)\n\n```bash\n# Create virtual environment\npython -m venv venv\n\n# Activate it\nsource venv/bin/activate  # On Linux/Mac\n# or\nvenv\\Scripts\\activate     # On Windows\n\n# Install the framework\npip install git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n### Method 3: Development Installation\n\nIf you're developing the framework itself:\n\n```bash\n# Clone the repository\ngit clone https://github.com/SOELexicon/clickup_framework.git\ncd clickup_framework\n\n# Install in editable mode\npip install -e .\n\n# Use the module directly\npython -m clickup_framework.cli --help\n```\n\n## Configuration\n\n### 1. Get Your ClickUp API Token\n\n1. Go to [ClickUp Settings > Apps](https://app.clickup.com/settings/apps)\n2. Click \"Generate\" under \"API Token\"\n3. Copy your token (it looks like: `pk_12345678_ABCDEFGHIJKLMNOPQRSTUVWXYZ`)\n\n### 2. Set Environment Variable\n\n**Linux/Mac (Temporary - current session only):**\n```bash\nexport CLICKUP_API_TOKEN=\"pk_your_token_here\"\n```\n\n**Linux/Mac (Permanent - add to ~/.bashrc or ~/.zshrc):**\n```bash\necho 'export CLICKUP_API_TOKEN=\"pk_your_token_here\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n**Windows (PowerShell):**\n```powershell\n$env:CLICKUP_API_TOKEN=\"pk_your_token_here\"\n\n# Or permanently:\n[System.Environment]::SetEnvironmentVariable('CLICKUP_API_TOKEN', 'pk_your_token_here', 'User')\n```\n\n### 3. Verify Installation\n\n```bash\n# Check if command is available\ncum --version\n# or\nclickup --version\n\n# Test with demo mode (no token required)\ncum demo\n\n# Test with your token\ncum show\n```\n\n### 4. Set Up Context\n\nSet your default workspace and list for easier command usage:\n\n```bash\n# Set workspace (find ID in ClickUp URL or use cum demo)\ncum set workspace <workspace_id>\n\n# Set default list\ncum set list <list_id>\n\n# Set default assignee (your user ID)\ncum set assignee <user_id>\n\n# Verify context\ncum show\n```\n\n## Optional: Enable Tab Completion\n\n### Bash\n\n```bash\n# Install argcomplete if not already installed\npip install argcomplete\n\n# Add to ~/.bashrc\necho 'eval \"$(register-python-argcomplete cum)\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n### Zsh\n\n```bash\n# Install argcomplete if not already installed\npip install argcomplete\n\n# Add to ~/.zshrc\necho 'eval \"$(register-python-argcomplete cum)\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n## Verification Checklist\n\nRun these commands to verify everything is working:\n\n```bash\n# 1. Command is available\nwhich cum\n# Should show: /usr/local/bin/cum or similar\n\n# 2. Token is set\necho $CLICKUP_API_TOKEN\n# Should show: pk_...\n\n# 3. Can connect to API\ncum show\n# Should show: Current context (even if empty)\n\n# 4. Can view tasks\ncum a\n# Should show: Your assigned tasks\n\n# 5. Context is working\ncum set workspace <workspace_id>\ncum show\n# Should show: Current workspace ID\n```\n\n## Troubleshooting\n\n### Issue: `command not found: cum`\n\n**Solution 1:** Reinstall the package\n```bash\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n**Solution 2:** Use the module directly\n```bash\npython -m clickup_framework.cli --help\n```\n\n**Solution 3:** Check your PATH\n```bash\n# Find where pip installs executables\npip show clickup_framework | grep Location\n\n# Add to PATH if needed (in ~/.bashrc or ~/.zshrc)\nexport PATH=\"$PATH:$HOME/.local/bin\"\n```\n\n### Issue: `401 Unauthorized` or API errors\n\n**Causes:**\n- Token not set\n- Token expired or invalid\n- Token doesn't have required permissions\n\n**Solution:**\n```bash\n# Check if token is set\necho $CLICKUP_API_TOKEN\n\n# If empty, set it\nexport CLICKUP_API_TOKEN=\"pk_your_token_here\"\n\n# Test the token\ncum show\n\n# If still failing, regenerate token in ClickUp settings\n```\n\n### Issue: Permission denied or pip warnings\n\n**Symptom:**\n```\nWARNING: The directory '/root/.cache/pip' or its parent directory is not owned...\nWARNING: Running pip as the 'root' user...\n```\n\n**Solution:** Use a virtual environment (recommended)\n```bash\npython -m venv venv\nsource venv/bin/activate\npip install git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n### Issue: Python version too old\n\n**Symptom:**\n```\nERROR: Package requires Python '>=3.8'\n```\n\n**Solution:** Upgrade Python\n```bash\n# Ubuntu/Debian\nsudo apt update\nsudo apt install python3.10\n\n# Mac (using Homebrew)\nbrew install python@3.10\n\n# Verify\npython3 --version\n```\n\n### Issue: SSL Certificate errors\n\n**Symptom:**\n```\nSSL: CERTIFICATE_VERIFY_FAILED\n```\n\n**Solution:**\n```bash\n# Update certificates (Mac)\n/Applications/Python\\ 3.x/Install\\ Certificates.command\n\n# Or upgrade certifi\npip install --upgrade certifi\n\n# Or temporarily disable (not recommended for production)\npip install --trusted-host pypi.org --trusted-host files.pythonhosted.org git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n### Issue: Commands are slow\n\n**Causes:**\n- Network latency\n- Large task lists\n- Complex hierarchies\n\n**Solutions:**\n```bash\n# Use minimal preset for faster rendering\ncum h current --preset minimal\n\n# Filter to specific status\ncum fil current --status \"in progress\"\n\n# Disable colors if terminal is slow\ncum ansi disable\n\n# Use flat view instead of hierarchy\ncum f current\n```\n\n## Updating\n\n### Update to Latest Version\n\n```bash\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n### Check Current Version\n\n```bash\ncum --version\n```\n\n### Update in Virtual Environment\n\n```bash\n# Activate venv\nsource venv/bin/activate\n\n# Update\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n## Uninstallation\n\n```bash\n# Uninstall the package\npip uninstall clickup_framework\n\n# Remove configuration (optional)\nrm ~/.clickup_context.json\n\n# Remove environment variable (remove from ~/.bashrc or ~/.zshrc)\n# Then reload shell:\nsource ~/.bashrc\n```\n\n## Docker Installation (Advanced)\n\nCreate a Dockerfile for containerized usage:\n\n```dockerfile\nFROM python:3.10-slim\n\n# Install ClickUp Framework\nRUN pip install --no-cache-dir git+https://github.com/SOELexicon/clickup_framework.git\n\n# Set token via environment\nENV CLICKUP_API_TOKEN=\"\"\n\n# Default command\nCMD [\"cum\", \"--help\"]\n```\n\nBuild and run:\n\n```bash\n# Build\ndocker build -t clickup-cli .\n\n# Run (pass token)\ndocker run -e CLICKUP_API_TOKEN=\"pk_your_token\" clickup-cli cum a\n```\n\n## Next Steps\n\nAfter installation:\n\n1. **Set up context:** Run `cum set workspace <id>` and `cum set list <id>`\n2. **View your tasks:** Run `cum a` to see assigned tasks\n3. **Learn the workflow:** See the Workflow skill or run `/workflow` in Claude Code\n4. **Explore commands:** Run `/cum` for complete CLI reference\n5. **Enable tab completion:** Follow the optional steps above\n\n## Getting Help\n\n```bash\n# General help\ncum --help\n\n# Command-specific help\ncum <command> --help\n\n# Demo mode (no token required)\ncum demo\n\n# Show current context\ncum show\n```\n\n## Resources\n\n- **GitHub Repository:** https://github.com/SOELexicon/clickup_framework\n- **ClickUp API Docs:** https://clickup.com/api\n- **Get API Token:** https://app.clickup.com/settings/apps\n\n---\n\nFor command reference: See the CLI Reference skill or run `/cum` in Claude Code.\nFor workflow guide: See the Workflow skill or run `/workflow` in Claude Code.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "SOELexicon",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/4205001?v=4",
      "lastCommit": "2025-12-30T21:37:58Z",
      "createdAt": "2025-11-08T00:31:11Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:19.851Z"
  },
  {
    "id": "load-to-cert",
    "source": {
      "type": "github-search",
      "repo": "pauljbernard/datamig",
      "path": ".claude/skills/load-to-cert/skill.md",
      "url": "https://github.com/pauljbernard/datamig/blob/main/.claude/skills/load-to-cert/skill.md"
    },
    "frontmatter": {},
    "content": "# Load to CERT Skill\n\nYou are a data loading specialist operating autonomously. Your mission: Load validated, anonymized data into CERT databases with transaction safety and constraint management.\n\n## Input Parameters\n\n- `district_id`: District whose data to load\n- `input_dir`: Directory with validated data (default: `data/anonymized/{district_id}`)\n- `loading_order`: Topologically sorted load order (default: from `data/analysis/extraction-order.json`)\n- `strategy`: Loading strategy (default: \"insert\") - options: insert, upsert, merge\n\n## Autonomous Execution Plan\n\n### 1. Pre-Load Validation\n\nCRITICAL: Verify validation passed before loading:\n\n```python\n# Load validation report\nvalidation_report = read_json(f'{input_dir}/validation-report.json')\n\nif validation_report['overall_status'] == 'FAILED':\n    raise Exception(\"Validation FAILED - cannot load data to CERT\")\n\nif validation_report['summary']['total_errors'] > 0:\n    raise Exception(f\"{validation_report['summary']['total_errors']} validation errors - cannot proceed\")\n\nlog_info(f\"Validation status: {validation_report['overall_status']}\")\nlog_info(f\"Warnings: {validation_report['summary']['total_warnings']}\")\n```\n\n### 2. Check CERT Connectivity\n\nTest connection to all CERT databases:\n\n```python\ndef check_cert_connectivity():\n    \"\"\"Test all CERT database connections.\"\"\"\n    cert_stores = {\n        'ids': 'postgres-ids-cert',\n        'hcp1': 'postgres-hcp1-cert',\n        'hcp2': 'postgres-hcp2-cert',\n        'adb': 'postgres-adb-cert',\n        'sp': 'neo4j-sp-cert'\n    }\n\n    for store, mcp_server in cert_stores.items():\n        try:\n            result = mcp_query(mcp_server, \"SELECT 1\")\n            log_info(f\"✓ {store} CERT connection OK\")\n        except Exception as e:\n            raise Exception(f\"Cannot connect to {store} CERT: {e}\")\n```\n\n### 3. Estimate Load Time\n\n```python\ndef estimate_load_time(input_dir):\n    \"\"\"Estimate how long load will take.\"\"\"\n    total_records = 0\n    for file_path in glob(f\"{input_dir}/**/*.parquet\", recursive=True):\n        df = pd.read_parquet(file_path)\n        total_records += len(df)\n\n    # Loading: ~30,000 records/minute (conservative)\n    minutes = total_records / 30000\n    return round(minutes / 60, 1)  # hours\n```\n\n### 4. (Optional) Create CERT Backup\n\n```python\ndef backup_cert_before_load(district_id):\n    \"\"\"\n    Optionally create backup of CERT before loading.\n    WARNING: This requires significant disk space.\n    \"\"\"\n    if os.getenv('ENABLE_CERT_BACKUP') == 'true':\n        log_info(\"Creating CERT backup...\")\n        # Implementation depends on database tools (pg_dump, etc.)\n        # Skipped for autonomous execution unless explicitly enabled\n    else:\n        log_info(\"CERT backup skipped (not enabled)\")\n```\n\n### 5. Load PostgreSQL Stores (IDS, HCP1, HCP2, ADB)\n\nLoad in topological order to respect FK constraints:\n\n```python\ndef load_postgres_store(store_name, input_dir, loading_order, strategy):\n    \"\"\"\n    Load data to a PostgreSQL CERT store.\n    \"\"\"\n    mcp_server = f\"postgres-{store_name}-cert\"\n    loaded_tables = []\n\n    # Start transaction\n    begin_transaction(mcp_server)\n\n    try:\n        for table_name in loading_order[store_name]:\n            file_path = f\"{input_dir}/{store_name}/{table_name}.parquet\"\n\n            if not os.path.exists(file_path):\n                log_warning(f\"File not found: {file_path}, skipping\")\n                continue\n\n            # Load data\n            df = pd.read_parquet(file_path)\n            record_count = len(df)\n\n            # Execute load based on strategy\n            if strategy == 'insert':\n                load_insert(mcp_server, table_name, df)\n            elif strategy == 'upsert':\n                load_upsert(mcp_server, table_name, df)\n            elif strategy == 'merge':\n                load_merge(mcp_server, table_name, df)\n\n            log_info(f\"Loaded {record_count} records to {store_name}.{table_name}\")\n\n            loaded_tables.append({\n                'table': table_name,\n                'records': record_count,\n                'status': 'SUCCESS'\n            })\n\n        # Commit transaction\n        commit_transaction(mcp_server)\n        log_info(f\"✓ {store_name} load committed\")\n\n        return loaded_tables\n\n    except Exception as e:\n        # Rollback on any error\n        rollback_transaction(mcp_server)\n        log_error(f\"✗ {store_name} load failed, rolled back: {e}\")\n        raise\n```\n\n#### Loading Strategies\n\n**Strategy 1: INSERT (default)**\n```python\ndef load_insert(mcp_server, table_name, df):\n    \"\"\"\n    Insert new records only. Fails if records already exist.\n    \"\"\"\n    # Convert DataFrame to INSERT statements\n    for chunk in chunked(df, 1000):\n        values = []\n        for _, row in chunk.iterrows():\n            row_values = ', '.join([f\"'{v}'\" if isinstance(v, str) else str(v) for v in row])\n            values.append(f\"({row_values})\")\n\n        sql = f\"\"\"\n            INSERT INTO {table_name} ({', '.join(df.columns)})\n            VALUES {', '.join(values)}\n        \"\"\"\n\n        execute_mcp(mcp_server, sql)\n```\n\n**Strategy 2: UPSERT**\n```python\ndef load_upsert(mcp_server, table_name, df):\n    \"\"\"\n    Insert new records or update existing (PostgreSQL ON CONFLICT).\n    \"\"\"\n    pk_columns = get_primary_key_columns(table_name)\n\n    for chunk in chunked(df, 1000):\n        # Build INSERT ... ON CONFLICT UPDATE\n        sql = f\"\"\"\n            INSERT INTO {table_name} ({', '.join(df.columns)})\n            VALUES ...\n            ON CONFLICT ({', '.join(pk_columns)})\n            DO UPDATE SET\n                {', '.join([f\"{col} = EXCLUDED.{col}\" for col in df.columns if col not in pk_columns])}\n        \"\"\"\n\n        execute_mcp(mcp_server, sql)\n```\n\n**Strategy 3: MERGE**\n```python\ndef load_merge(mcp_server, table_name, df):\n    \"\"\"\n    Complex merge logic with conflict resolution.\n    \"\"\"\n    # Check which records exist\n    existing_ids = get_existing_ids(mcp_server, table_name, df)\n\n    # Split into inserts and updates\n    new_records = df[~df['id'].isin(existing_ids)]\n    update_records = df[df['id'].isin(existing_ids)]\n\n    # Insert new\n    if len(new_records) > 0:\n        load_insert(mcp_server, table_name, new_records)\n\n    # Update existing\n    if len(update_records) > 0:\n        for _, row in update_records.iterrows():\n            # Custom conflict resolution logic here\n            update_record(mcp_server, table_name, row)\n```\n\n### 6. Load Neo4j Graph Store (SP)\n\n```python\ndef load_neo4j_store(input_dir):\n    \"\"\"\n    Load graph data to Neo4j CERT.\n    \"\"\"\n    mcp_server = \"neo4j-sp-cert\"\n\n    # Load nodes\n    nodes_file = f\"{input_dir}/sp/nodes.parquet\"\n    nodes_df = pd.read_parquet(nodes_file)\n\n    # Load relationships\n    rels_file = f\"{input_dir}/sp/relationships.parquet\"\n    rels_df = pd.read_parquet(rels_file)\n\n    # Create nodes in batches\n    for label in nodes_df['labels'].unique():\n        label_nodes = nodes_df[nodes_df['labels'] == label]\n\n        for chunk in chunked(label_nodes, 1000):\n            # Build Cypher CREATE statement\n            cypher = f\"\"\"\n                UNWIND $nodes AS node\n                CREATE (n:{label})\n                SET n = node.properties\n            \"\"\"\n\n            nodes_data = chunk.to_dict('records')\n            execute_neo4j(mcp_server, cypher, {'nodes': nodes_data})\n\n    # Create relationships\n    for rel_type in rels_df['type'].unique():\n        type_rels = rels_df[rels_df['type'] == rel_type]\n\n        for chunk in chunked(type_rels, 1000):\n            cypher = f\"\"\"\n                UNWIND $rels AS rel\n                MATCH (start) WHERE id(start) = rel.start\n                MATCH (end) WHERE id(end) = rel.end\n                CREATE (start)-[r:{rel_type}]->(end)\n                SET r = rel.properties\n            \"\"\"\n\n            rels_data = chunk.to_dict('records')\n            execute_neo4j(mcp_server, cypher, {'rels': rels_data})\n\n    log_info(f\"✓ Loaded {len(nodes_df)} nodes and {len(rels_df)} relationships to Neo4j CERT\")\n```\n\n### 7. Post-Load Validation\n\nVerify data loaded correctly:\n\n```python\ndef validate_cert_load(district_id, load_manifest):\n    \"\"\"\n    Validate data in CERT matches expectations.\n    \"\"\"\n    errors = []\n\n    for store_name, tables in load_manifest.items():\n        mcp_server = f\"postgres-{store_name}-cert\" if store_name != 'sp' else 'neo4j-sp-cert'\n\n        for table_info in tables:\n            table_name = table_info['table']\n            expected_count = table_info['records']\n\n            # Query actual count in CERT\n            if store_name != 'sp':\n                result = mcp_query(mcp_server, f\"SELECT COUNT(*) FROM {table_name} WHERE district_id = '{district_id}'\")\n                actual_count = result[0]['count']\n            else:\n                # Neo4j count\n                result = execute_neo4j(mcp_server, f\"MATCH (n:{table_name} {{district_id: '{district_id}'}}) RETURN count(n) as count\")\n                actual_count = result[0]['count']\n\n            if actual_count != expected_count:\n                errors.append({\n                    'table': f\"{store_name}.{table_name}\",\n                    'expected': expected_count,\n                    'actual': actual_count,\n                    'message': f\"Record count mismatch\"\n                })\n\n    return errors\n```\n\n### 8. Re-run Integrity Checks on CERT\n\n```python\ndef verify_fk_integrity_on_cert(district_id):\n    \"\"\"\n    Re-validate FK relationships in CERT environment.\n    \"\"\"\n    # Use same validation logic as validate-integrity skill\n    # but query from CERT databases instead of Parquet files\n\n    errors = []\n\n    # Example: Verify student FK references exist in schools\n    students = mcp_query('postgres-ids-cert', f\"SELECT school_id FROM students WHERE district_id = '{district_id}'\")\n    schools = mcp_query('postgres-ids-cert', f\"SELECT id FROM schools WHERE district_id = '{district_id}'\")\n\n    student_school_ids = set(s['school_id'] for s in students)\n    school_ids = set(s['id'] for s in schools)\n\n    orphans = student_school_ids - school_ids\n\n    if orphans:\n        errors.append({\n            'check': 'fk_students_schools',\n            'orphans': len(orphans),\n            'message': 'Students reference non-existent schools'\n        })\n\n    return errors\n```\n\n### 9. Generate Load Report\n\nCreate `data/loads/{district_id}/load-report.json`:\n\n```json\n{\n  \"load_run_id\": \"load-20251106-001\",\n  \"district_id\": \"district-001\",\n  \"started_at\": \"2025-11-06T20:00:00Z\",\n  \"completed_at\": \"2025-11-06T21:45:00Z\",\n  \"duration_minutes\": 105,\n  \"overall_status\": \"SUCCESS\",\n  \"strategy\": \"insert\",\n  \"stores\": {\n    \"ids\": {\n      \"status\": \"SUCCESS\",\n      \"tables_loaded\": 45,\n      \"records_loaded\": 250000,\n      \"duration_minutes\": 32,\n      \"transaction\": \"COMMITTED\"\n    },\n    \"hcp1\": {\n      \"status\": \"SUCCESS\",\n      \"tables_loaded\": 32,\n      \"records_loaded\": 180000,\n      \"duration_minutes\": 24,\n      \"transaction\": \"COMMITTED\"\n    },\n    \"hcp2\": {\n      \"status\": \"SUCCESS\",\n      \"tables_loaded\": 28,\n      \"records_loaded\": 120000,\n      \"duration_minutes\": 18,\n      \"transaction\": \"COMMITTED\"\n    },\n    \"adb\": {\n      \"status\": \"SUCCESS\",\n      \"tables_loaded\": 52,\n      \"records_loaded\": 200000,\n      \"duration_minutes\": 28,\n      \"transaction\": \"COMMITTED\"\n    },\n    \"sp\": {\n      \"status\": \"SUCCESS\",\n      \"nodes_loaded\": 100000,\n      \"relationships_loaded\": 250000,\n      \"duration_minutes\": 3\n    }\n  },\n  \"totals\": {\n    \"tables_loaded\": 157,\n    \"records_loaded\": 750000\n  },\n  \"post_load_validation\": {\n    \"status\": \"PASSED\",\n    \"record_count_verification\": \"PASSED\",\n    \"fk_integrity_verification\": \"PASSED\",\n    \"errors\": 0\n  }\n}\n```\n\n### 10. Report Completion\n\n```\n✓ CERT Load Complete\n\nDistrict: district-001\nDuration: 1.75 hours\nStrategy: INSERT\n\nLoad Summary:\n- IDS: 250,000 records (45 tables) ✓ COMMITTED\n- HCP1: 180,000 records (32 tables) ✓ COMMITTED\n- HCP2: 120,000 records (28 tables) ✓ COMMITTED\n- ADB: 200,000 records (52 tables) ✓ COMMITTED\n- SP (Graph): 100,000 nodes, 250,000 relationships ✓ COMMITTED\n\nTotal: 750,000 records loaded successfully\n\nPost-Load Validation: PASSED\n- Record counts: ✓ All match expectations\n- FK integrity: ✓ All relationships valid in CERT\n- Data accessibility: ✓ All tables queryable\n\nCERT Environment Status: READY FOR TESTING\n\nReport: data/loads/district-001/load-report.json\n\nNext step: Generate migration report\n```\n\n## Tools Available\n\n- **MCP Servers**: All CERT database connections\n- **ETL MCP**: `load_with_constraints` tool\n- **Python Libraries**: pandas, sqlalchemy\n\n## Success Criteria\n\n- ✓ All tables loaded in dependency order\n- ✓ All transactions committed successfully\n- ✓ Record counts match source\n- ✓ FK integrity validated on CERT\n- ✓ 0 load errors\n\n## Error Handling\n\n**Load Failures**:\n- Immediate rollback of current store transaction\n- Log detailed error with table/record information\n- Stop loading remaining stores\n- Report failure status\n\n**FK Violations**:\n- Should not occur if validation passed\n- If detected, rollback and investigate\n\n**Transaction Timeout**:\n- Increase timeout for large loads\n- Consider chunking very large tables\n\nIf ANY store fails to load, entire migration is considered FAILED.\nRollback completed, CERT remains in pre-migration state.\n\nExecute autonomously. Report progress every 10 tables loaded.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "pauljbernard",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/324344?v=4",
      "lastCommit": "2025-11-06T19:39:36Z",
      "createdAt": "2025-11-06T11:54:27Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:20.773Z"
  },
  {
    "id": "cs2-analyzer",
    "source": {
      "type": "github-search",
      "repo": "mwridgway/akkr-gemini",
      "path": ".claude/skills/cs2-analyzer/skill.md",
      "url": "https://github.com/mwridgway/akkr-gemini/blob/main/.claude/skills/cs2-analyzer/skill.md"
    },
    "frontmatter": {
      "description": "Analyze CS2 demo files and generate comprehensive tactical reports"
    },
    "content": "# CS2 Demo Analyzer Skill\n\nThis skill analyzes Counter-Strike 2 demo files (.dem) and produces professional tactical analysis reports with strategic insights, player statistics, and coaching recommendations.\n\n## What This Skill Does\n\n1. **Accepts a CS2 demo file** (drag & drop or provide path)\n2. **Generates multi-format analysis**:\n   - Compact storage format for archival\n   - Digest summary for quick review\n   - Full markdown tactical report\n3. **Returns strategic insights** including:\n   - Team performance (T-side vs CT-side)\n   - Player statistics and rankings\n   - Critical round breakdowns\n   - Tactical recommendations\n   - Entry success, post-plant win rates, tempo analysis\n\n## For End Users\n\n### Simple Usage\n\nJust say:\n- \"Analyze this demo\" (after dragging .dem file)\n- \"Analyze data/raw/my-game.dem\"\n- \"Give me a tactical report for the latest demo\"\n\n### What You'll Get\n\nA comprehensive report covering:\n- **Executive Summary** - Top 3-5 strategic insights\n- **Team Analysis** - T-side and CT-side performance\n- **Critical Rounds** - Key moments that decided the game\n- **Player Highlights** - Top performers and improvement areas\n- **Tactical Recommendations** - Actionable coaching points\n\n## Requirements (One-Time Setup)\n\nThis skill requires:\n- ✅ Python 3.11+ with Poetry installed\n- ✅ Project dependencies installed (`poetry install`)\n- ✅ Directory structure: `data/raw/`, `data/compact/`, `data/digest/`, `reports/`\n\n**Setup Instructions**: See `docs/setup.md` for first-time installation.\n\n## How It Works\n\nThe skill uses a three-tier architecture:\n\n1. **Parse Demo** → Extract game data using awpy library\n2. **Generate Formats**:\n   - Compact state (storage, ~400KB)\n   - Digest (Claude-readable, <50KB)\n   - Full report (markdown)\n3. **Strategic Analysis** → Metrics calculation and tactical insights\n\n## Example Output\n\n```\n# CS2 Tactical Analysis: Dust2\n\n**Final Score**: T 15 - 13 CT\n\n## Executive Summary\n- T-side dominated with 54% round win rate\n- Exceptional 82% post-plant win rate\n- Aggressive 17.5s time-to-first-kill\n- m0NESY MVP: 1.46 K/D, 19 kills\n\n## Recommendations\n- T-Side: Maintain excellent post-plant discipline\n- CT-Side: Improve retake coordination (only 18% success)\n- Individual: mezii needs positioning work (0.6 K/D)\n```\n\n## Technical Details\n\n**Scripts Used**:\n- `generate_tactical_report.py` - Main analysis pipeline\n- `generate_digest.py` - Digest generation\n- `analyze_compact_demo.py` - Metrics calculation\n\n**Output Locations**:\n- Reports: `reports/game_analysis_[map]_[timestamp].md`\n- Digests: `data/digest/[demo]_[timestamp].digest.txt`\n- Compact: `data/compact/[demo]_[timestamp].compact.txt`\n\n**Performance**: 30-60 seconds for full match analysis (30-50 rounds)\n\n## Skill Workflow\n\nWhen invoked, this skill will:\n\n1. ✅ Validate demo file path exists\n2. ✅ Check dependencies (poetry environment)\n3. ✅ Run analysis pipeline\n4. ✅ Read generated digest for strategic insights\n5. ✅ Read full report for comprehensive details\n6. ✅ Summarize findings for user\n7. ✅ Provide report file path for download\n\n## Error Handling\n\nThe skill handles:\n- Missing demo files → Clear error message\n- Invalid demo format → awpy parsing error explanation\n- Missing dependencies → Setup instructions\n- Analysis failures → Detailed error logs\n\n## Future Enhancements\n\nPlanned features:\n- [ ] Multi-demo comparison (track player improvement over time)\n- [ ] Interactive queries (\"Show me all A-site executes\")\n- [ ] Heatmap generation (positional analysis)\n- [ ] Utility usage analysis (smoke/flash patterns)\n- [ ] Economic analysis (force buy success rates)\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "mwridgway",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/116311?v=4",
      "lastCommit": "2025-11-14T05:39:51Z",
      "createdAt": "2025-09-30T04:06:13Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:21.423Z"
  },
  {
    "id": "quantconnect",
    "source": {
      "type": "github-search",
      "repo": "derekcrosslu/CLAUDE_CODE_EXPLORE",
      "path": ".claude/skills/quantconnect/skill.md",
      "url": "https://github.com/derekcrosslu/CLAUDE_CODE_EXPLORE/blob/main/.claude/skills/quantconnect/skill.md"
    },
    "frontmatter": {
      "name": "QuantConnect",
      "description": "QuantConnect strategy development with component library (project)"
    },
    "content": "# QuantConnect Strategy Development (Component-Based)\n\nDevelop modular strategies using reusable components: `./component`\n\n## When to Load This Skill\n\n- Creating new QuantConnect strategy\n- Need to add indicators, signals, or risk management\n- Troubleshooting QC-specific issues\n\n## Component Library (Progressive Disclosure)\n\n**Use components instead of writing from scratch**. Load only what you need.\n\n### Discovery\n```bash\n# List all components\n./component list\n\n# List by category\n./component list indicators\n./component list signals\n./component list risk_management\n\n# Search by keyword\n./component search momentum\n./component search stop\n```\n\n### Integration\n```bash\n# View component code\n./component show add_rsi\n\n# Get integration guide\n./component explain add_rsi\n```\n\n**IMPORTANT: Do not read component source files directly. Use --help and explain commands.**\n\n## Available Components\n\n### Indicators (indicators/)\n- **add_rsi** - RSI indicator for overbought/oversold\n- **add_sma** - Simple Moving Average for trend detection\n\n### Signals (signals/)\n- **mean_reversion** - RSI-based mean reversion signals\n- **momentum_breakout** - SMA crossover momentum signals\n\n### Risk Management (risk_management/)\n- **stop_loss** - Fixed or trailing stop loss\n\n### Sentiment (sentiment/)\n- Future: Kalshi prediction market integration\n\n## Strategy Development Workflow\n\n### 1. Plan Strategy\n```\n1. Choose hypothesis (mean reversion, momentum, etc.)\n2. Select components needed:\n   - Indicators: RSI, SMA, MACD?\n   - Signals: Mean reversion, breakout?\n   - Risk: Stop loss, position sizing?\n```\n\n### 2. Browse Components\n```bash\n./component list\n./component explain COMPONENT\n```\n\n### 3. Build Strategy\n```python\nfrom AlgorithmImports import *\nfrom strategy_components.indicators.add_rsi import add_rsi\nfrom strategy_components.signals.mean_reversion import MeanReversionSignal\n\nclass MyStrategy(QCAlgorithm):\n    def Initialize(self):\n        # Add components\n        self.rsi = add_rsi(self, symbol=\"SPY\", period=14)\n        self.signal = MeanReversionSignal(oversold=30, overbought=70)\n        \n    def OnData(self, data):\n        # Use components\n        if self.rsi.IsReady:\n            signal = self.signal.get_signal(self.rsi.Current.Value, self.is_long)\n            # Execute trades...\n```\n\n### 4. Test Strategy\n```bash\n# Local test first (if possible)\n# Then: ./qc_backtest run --strategy strategy.py\n```\n\n## Common Patterns\n\n### Mean Reversion\n```python\n# Components: add_rsi, mean_reversion\nself.rsi = add_rsi(self, \"SPY\", period=14)\nself.signal = MeanReversionSignal(oversold=30, overbought=70)\n```\n\n### Momentum Breakout\n```python\n# Components: add_sma, momentum_breakout\nself.sma = add_sma(self, \"SPY\", period=20)\nself.signal = MomentumBreakoutSignal(volume_confirmation=True)\n```\n\n### With Stop Loss\n```python\n# Add: stop_loss component\nself.stop_loss = StopLossManager(stop_loss_pct=0.05, trailing=False)\n# Call: self.stop_loss.set_entry_price(price) after entry\n# Check: if self.stop_loss.should_exit(current_price): ...\n```\n\n## Beyond MCP Principles\n\n1. **Use component CLI, not source code**\n   - `./component list` - browse available\n   - `./component explain COMPONENT` - integration guide\n   - Don't read .py files directly\n\n2. **Progressive Disclosure**\n   - Load only components you need\n   - Don't load entire 955-line skill\n\n3. **Modular Architecture**\n   - Components are independent\n   - Mix and match as needed\n   - Reuse across strategies\n\n## Critical QC Errors (Still Important)\n\n### Error 1: SMA NoneType\n**Problem**: `self.sma.Current` is None\n**Fix**: Check `if self.sma.IsReady` before using\n\n### Error 2: Data Key Missing\n**Problem**: `KeyError` on `data[self.symbol]`\n**Fix**: Check `if data.ContainsKey(self.symbol)` first\n\n### Error 3: Warmup Issues\n**Problem**: Strategy trades during warmup\n**Fix**: `if self.IsWarmingUp: return`\n\n## Authoritative Documentation\n\n**When confused about QC API or architecture:**\n- QC Docs: https://www.quantconnect.com/docs\n- Component README: `SCRIPTS/strategy_components/README.md`\n\n**Do not guess. Use component CLI and QC docs as source of truth.**\n\n---\n\n**Context Savings**: 120 lines (vs 955 lines in old skill) = 87% reduction\n\n**Progressive Disclosure**: Use component CLI to load only what you need\n\n**Trifecta**: CLI works for humans, teams, AND agents\n\n**Beyond MCP Pattern**: Use --help and explain, not source code\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "derekcrosslu",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/3807652?v=4",
      "lastCommit": "2025-11-18T16:17:17Z",
      "createdAt": "2025-11-09T23:45:39Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:22.053Z"
  },
  {
    "id": "clickup-cli-reference",
    "source": {
      "type": "github-search",
      "repo": "SOELexicon/clickup_framework",
      "path": ".claude/skills/cli-reference/skill.md",
      "url": "https://github.com/SOELexicon/clickup_framework/blob/main/.claude/skills/cli-reference/skill.md"
    },
    "frontmatter": {
      "name": "clickup-cli-reference",
      "description": "Complete command reference for the ClickUp Framework CLI (cum) with all shortcuts, options, examples, and usage patterns",
      "tags": "[clickup, cli, reference, commands]"
    },
    "content": "# ClickUp Framework CLI Reference\n\nQuick reference for the ClickUp Framework CLI (`cum` / `clickup`)\n\n## Installation\n\n### Install from GitHub (Latest)\n```bash\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n### Verify Installation\n```bash\ncum --version\n# or\nclickup --version\n```\n\n### Required: API Token Setup\n```bash\nexport CLICKUP_API_TOKEN=\"your_token_here\"\n# Or add to ~/.bashrc or ~/.zshrc for persistence\n```\n\n## Command Invocation\n\n**In Development Environment (working in the repo):**\n```bash\npython -m clickup_framework.cli <command> [args]\n```\n\n**In Installed Environment:**\n```bash\ncum <command> [args]\n# or\nclickup <command> [args]\n```\n\n**How to determine which to use:**\n1. First check if `cum` command is available with: `which cum` or `command -v cum`\n2. If NOT available (returns empty/error), use: `python -m clickup_framework.cli`\n3. If available, use: `cum`\n\n## Quick Reference: All Short Codes\n\n**View:** `h` `ls` `l` `c` `f` `fil` `d` `st` `a`\n**Context:** `set` `show` `clear`\n**Tasks:** `tc` `tu` `td` `ta` `tua` `tss` `tsp` `tst` `tad` `trd` `tal` `trl`\n**Comments:** `ca` `cl` `cu` `cd`\n**Docs:** `dl` `dg` `dc` `du` `de` `di` `pl` `pc` `pu`\n\n## View Commands\n\n| Command | Short Codes | Usage | Description |\n|---------|-------------|-------|-------------|\n| `hierarchy` | `h` `list` `ls` `l` | `cum h <list_id\\|--all>` | Hierarchical parent-child tree view |\n| `clist` / `container` | `c` | `cum c <list_id>` | Container hierarchy (Space→Folder→List) |\n| `flat` | `f` | `cum f <list_id>` | Flat list view |\n| `filter` | `fil` | `cum fil <list_id> [--status\\|--priority\\|--tags\\|--assignee]` | Filtered task view |\n| `detail` | `d` | `cum d <task_id> [list_id]` | Detailed single task view |\n| `stats` | `st` | `cum st <list_id>` | Task statistics & distribution |\n| `assigned` | `a` | `cum a [--user-id UID] [--team-id TID]` | Your assigned tasks, sorted by difficulty |\n| `demo` | | `cum demo [--mode MODE]` | Demo mode (no API token required) |\n\n## Context Management\n\n| Command | Short Codes | Usage | Description |\n|---------|-------------|-------|-------------|\n| `set_current` | `set` | `cum set <type> <id>` | Set current workspace/list/task/assignee |\n| `show_current` | `show` | `cum show` | Display current context |\n| `clear_current` | `clear` | `cum clear [type]` | Clear context (all or specific type) |\n| `ansi` | | `cum ansi <enable\\|disable\\|status>` | Configure color output |\n\n**Context Types**: `workspace`, `space`, `folder`, `list`, `task`, `assignee`\n\n## Task Management\n\n| Command | Short Codes | Usage | Description |\n|---------|-------------|-------|-------------|\n| `task_create` | `tc` | `cum tc \"name\" --list <list_id> [options]` | Create new task (name FIRST!) |\n| `task_update` | `tu` | `cum tu <task_id> [options]` | Update task properties |\n| `task_delete` | `td` | `cum td <task_id> [--force]` | Delete task |\n| `task_assign` | `ta` | `cum ta <task_id> <user_id> [...]` | Assign users to task |\n| `task_unassign` | `tua` | `cum tua <task_id> <user_id> [...]` | Remove assignees |\n| `task_set_status` | `tss` | `cum tss <task_id> [...] <status>` | Change task status (validates subtasks) |\n| `task_set_priority` | `tsp` | `cum tsp <task_id> <priority>` | Set priority (1-4 or name) |\n| `task_set_tags` | `tst` | `cum tst <task_id> <--add\\|--remove\\|--set> <tags...>` | Manage task tags |\n| `task_add_dependency` | `tad` | `cum tad <task_id> --waiting-on\\|--blocking <task_id>` | Add dependency |\n| `task_remove_dependency` | `trd` | `cum trd <task_id> --waiting-on\\|--blocking <task_id>` | Remove dependency |\n| `task_add_link` | `tal` | `cum tal <task_id> <linked_task_id>` | Link tasks |\n| `task_remove_link` | `trl` | `cum trl <task_id> <linked_task_id>` | Unlink tasks |\n\n### Task Create Options\n\n**IMPORTANT**: Task name comes FIRST as a positional argument!\n\n```bash\ncum tc \"Task Name\" --list <list_id> [options]\ncum tc \"Task Name\" --parent <parent_id> [options]  # For subtasks\n\nOptions:\n--list LIST_ID              # List to create task in (or \"current\")\n--parent TASK_ID           # Create as subtask (no --list needed if parent provided)\n--description TEXT          # Task description (text)\n--description-file PATH     # Task description (from file)\n--status STATUS            # Initial status\n--priority {1|2|3|4|urgent|high|normal|low}\n--tags TAG [...]           # Tags to add\n--assignees USER_ID [...]  # Assign users (defaults to context assignee)\n```\n\n**Note**: `--description` and `--description-file` are mutually exclusive.\n\n### Task Update Options\n\n```bash\n--name TEXT                # Update name\n--description TEXT         # Update description (text)\n--description-file PATH    # Update description (from file)\n--status STATUS           # Update status\n--priority PRIORITY       # Update priority\n```\n\n**Note**: `--description` and `--description-file` are mutually exclusive.\n\n## Comment Management\n\n| Command | Short Codes | Usage | Description |\n|---------|-------------|-------|-------------|\n| `comment_add` | `ca` | `cum ca <task_id> \"text\" \\| --comment-file FILE` | Add comment |\n| `comment_list` | `cl` | `cum cl <task_id>` | List task comments |\n| `comment_update` | `cu` | `cum cu <comment_id> \"text\" \\| --comment-file FILE` | Update comment |\n| `comment_delete` | `cd` | `cum cd <comment_id>` | Delete comment |\n\n### Comment Options\n\n```bash\n# Add/Update comments with text or file\ncomment_text               # Direct text input\n--comment-file PATH        # Read comment text from file\n```\n\n**Note**: Comment text and `--comment-file` are mutually exclusive.\n\n## Docs & Pages\n\n| Command | Short Codes | Usage | Description |\n|---------|-------------|-------|-------------|\n| `dlist` | `dl` `doc_list` | `cum dl <workspace_id>` | List all docs |\n| `doc_get` | `dg` | `cum dg <workspace_id> <doc_id>` | Show doc details |\n| `doc_create` | `dc` | `cum dc <workspace_id> \"name\" [--pages \"name:content\" ...]` | Create doc |\n| `doc_update` | `du` | `cum du <workspace_id> <doc_id> [options]` | Update doc |\n| `doc_export` | `de` | `cum de <workspace_id> [--doc-id ID] --output-dir ./out` | Export to markdown |\n| `doc_import` | `di` | `cum di <workspace_id> ./input_dir [--nested]` | Import markdown files |\n| `page_list` | `pl` | `cum pl <workspace_id> <doc_id>` | List pages in doc |\n| `page_create` | `pc` | `cum pc <workspace_id> <doc_id> --name \"name\" [--content \"...\"]` | Create page |\n| `page_update` | `pu` | `cum pu <workspace_id> <doc_id> <page_id> [--name\\|--content]` | Update page |\n\n## Global Format Options\n\n| Option | Description |\n|--------|-------------|\n| `--preset {minimal\\|summary\\|detailed\\|full}` | Quick format preset |\n| `--colorize` / `--no-colorize` | Force colors on/off |\n| `--show-ids` | Display task IDs |\n| `--show-tags` | Display tags (default: on) |\n| `--show-descriptions` | Display descriptions |\n| `--show-dates` | Display date fields |\n| `--show-comments N` | Show N most recent comments per task (default: 5, set to 0 to hide) |\n| `--include-completed` | Include completed tasks |\n| `--no-emoji` | Hide emoji icons |\n\n## Presets\n\n| Preset | Shows | Hides |\n|--------|-------|-------|\n| `minimal` | IDs, status, priority | Tags, descriptions, dates, emojis, comments |\n| `summary` | Status, priority, tags | IDs, descriptions, dates, comments |\n| `detailed` | Status, priority, tags, descriptions, dates | IDs, comments |\n| `full` | Everything + 5 recent comments | Nothing |\n\n## Priority Values\n\n| Value | Alias |\n|-------|-------|\n| `1` | `urgent` |\n| `2` | `high` |\n| `3` | `normal` |\n| `4` | `low` |\n\n## Status Codes (3-Letter)\n\nTasks display with color-coded 3-letter status codes:\n\n- `[CLS]` Closed/Complete\n- `[OPN]` Open\n- `[PRG]` In Progress\n- `[REV]` In Review\n- `[BLK]` Blocked\n- `[TDO]` To Do\n- Custom statuses use first 3 letters\n\n## Special Keywords\n\n- `current` - Use currently set resource from context\n- Works with: task_id, list_id, workspace_id, etc.\n\n## Common Workflows\n\n```bash\n# Set up context (using short codes!)\ncum set workspace 90151898946\ncum set list 901517404278\ncum set assignee 68483025\n\n# Quick task view\ncum h current                                    # Hierarchy view\ncum a                                            # Your assigned tasks\ncum st current                                   # Task statistics\n\n# Create and manage tasks (using short codes!)\ncum tc \"New feature\" --list current              # Auto-assigned to default assignee\ncum tc \"Feature\" --list current --description-file spec.md  # With description from file\ncum tc \"Subtask\" --parent <parent_id>            # Create subtask\ncum tu current --description-file updated_spec.md    # Update from file\ncum tss current \"in progress\"                    # Set status\ncum tst current --add bug critical               # Add tags\ncum td <task_id>                                 # Delete task\n\n# Add comments (using short codes!)\ncum ca current \"Initial thoughts\"                # Add comment directly\ncum ca current --comment-file notes.txt          # Comment from file\ncum cl current                                   # List comments\n\n# Filter tasks\ncum fil current --status \"in progress\"\ncum fil current --priority urgent --view-mode flat\n\n# Work with docs (using short codes!)\ncum dl 90151898946                               # List docs\ncum dc 90151898946 \"API Docs\"                    # Create doc\ncum pc 90151898946 <doc_id> --name \"Overview\"    # Add page\ncum de 90151898946 --output-dir ./docs           # Export all docs\n```\n\n## Configuration\n\nSettings stored in `~/.clickup_context.json`:\n- Current workspace/list/task/assignee\n- ANSI color preference\n- Last updated timestamp\n\n## Tips\n\n- **Use short codes everywhere!** `cum tc` instead of `cum task_create`, `cum h` instead of `cum hierarchy`\n- Set context once with `cum set`, use `current` everywhere\n- Default assignee auto-assigns new tasks\n- `assigned` (`cum a`) command sorts by dependency difficulty\n- `demo` mode works without API token for testing\n- Use `--description-file` and `--comment-file` for longer content\n- Tab complete works with both full commands and short codes\n- Most commands support `--help` for details\n\n## Tab Completion\n\nEnable tab completion for bash/zsh (if argcomplete installed):\n```bash\neval \"$(register-python-argcomplete cum)\"\n```\n\n## Troubleshooting\n\n### Command not found: cum\n```bash\n# Reinstall the package\npip install --upgrade --force-reinstall git+https://github.com/SOELexicon/clickup_framework.git\n\n# Or use the module directly\npython -m clickup_framework.cli --help\n```\n\n### API Token Issues\n```bash\n# Check if token is set\necho $CLICKUP_API_TOKEN\n\n# Set it permanently in ~/.bashrc or ~/.zshrc\necho 'export CLICKUP_API_TOKEN=\"your_token_here\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n### Permission Errors\n```bash\n# Use virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate\npip install git+https://github.com/SOELexicon/clickup_framework.git\n```\n\n## Getting Help\n\n```bash\n# General help\ncum --help\n\n# Command-specific help\ncum <command> --help\n\n# Examples\ncum tc --help\ncum h --help\ncum tss --help\n```\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "SOELexicon",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/4205001?v=4",
      "lastCommit": "2025-12-30T21:37:58Z",
      "createdAt": "2025-11-08T00:31:11Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:22.882Z"
  },
  {
    "id": "anonymize-pii",
    "source": {
      "type": "github-search",
      "repo": "pauljbernard/datamig",
      "path": ".claude/skills/anonymize-pii/skill.md",
      "url": "https://github.com/pauljbernard/datamig/blob/main/.claude/skills/anonymize-pii/skill.md"
    },
    "frontmatter": {},
    "content": "# Anonymize PII Skill\n\nYou are a data anonymization specialist operating autonomously. Your mission: Anonymize all PII (Personally Identifiable Information) in extracted data while preserving referential integrity and data utility.\n\n## Input Parameters\n\n- `district_id`: The district whose data to anonymize\n- `input_dir`: Directory with extracted data (default: `data/staging/{district_id}`)\n- `output_dir`: Directory for anonymized data (default: `data/anonymized/{district_id}`)\n- `rules_file`: Anonymization rules (default: `config/anonymization-rules.yaml`)\n\n## Autonomous Execution Plan\n\n### 1. Load Anonymization Rules\n\nRead configuration from `config/anonymization-rules.yaml`:\n\n```yaml\nrules:\n  # Email addresses\n  - field_pattern: \".*email.*\"\n    strategy: faker\n    faker_type: email\n    preserve_domain: false\n\n  # Social Security Numbers\n  - field_pattern: \".*ssn.*|.*social_security.*\"\n    strategy: hash\n    hash_algorithm: sha256\n    salt: \"${ENV_ANONYMIZATION_SALT}\"\n\n  # Names\n  - field_pattern: \".*first_name.*|.*last_name.*|.*full_name.*\"\n    strategy: faker\n    faker_type: name\n    consistent_per_id: true\n\n  # Phone numbers\n  - field_pattern: \".*phone.*|.*mobile.*|.*telephone.*\"\n    strategy: faker\n    faker_type: phone_number\n\n  # Addresses\n  - field_pattern: \".*address.*|.*street.*|.*city.*|.*zip.*|.*postal.*\"\n    strategy: faker\n    faker_type: address\n\n  # Student/Staff IDs (preserve for FKs)\n  - field_pattern: \".*_id$\"\n    strategy: preserve\n    reason: \"Foreign key - must maintain integrity\"\n```\n\n### 2. Detect PII Fields\n\nFor each Parquet file in input directory:\n\n```python\ndef detect_pii_fields(table_name, columns, rules):\n    \"\"\"\n    Identify which columns contain PII based on rules.\n    \"\"\"\n    pii_fields = []\n\n    for column in columns:\n        for rule in rules:\n            if re.match(rule['field_pattern'], column, re.IGNORECASE):\n                pii_fields.append({\n                    'column': column,\n                    'strategy': rule['strategy'],\n                    'config': rule\n                })\n                break\n\n    return pii_fields\n```\n\n### 3. Initialize Consistency Map\n\nCritical: Same PII value must map to same anonymized value across ALL tables.\n\n```python\n# Example: \"John Doe\" in students table → \"Alice Smith\"\n#          \"John Doe\" in staff table → \"Alice Smith\" (SAME)\n\nconsistency_map = {\n    'first_names': {},   # \"John\" → \"Alice\"\n    'last_names': {},    # \"Doe\" → \"Smith\"\n    'emails': {},        # \"john@example.com\" → \"alice789@example.com\"\n    'phones': {},        # \"555-1234\" → \"555-9876\"\n    'ssns': {},          # \"123-45-6789\" → <hashed value>\n}\n```\n\n### 4. Apply Anonymization Strategies\n\n#### Strategy 1: Consistent Hashing (for SSN, sensitive IDs)\n\n```python\ndef anonymize_hash(value, salt):\n    \"\"\"\n    Hash value with salt. Same input → same output.\n    Irreversible, preserves uniqueness.\n    \"\"\"\n    import hashlib\n    return hashlib.sha256(f\"{value}{salt}\".encode()).hexdigest()[:16]\n```\n\n#### Strategy 2: Faker (for names, emails, phones, addresses)\n\n```python\nfrom faker import Faker\nfake = Faker()\n\ndef anonymize_faker(value, faker_type, consistency_map, key):\n    \"\"\"\n    Generate realistic fake data. Use consistency map to ensure\n    same value gets same fake replacement.\n    \"\"\"\n    if value in consistency_map[key]:\n        return consistency_map[key][value]\n\n    # Generate new fake value\n    if faker_type == 'name':\n        fake_value = fake.name()\n    elif faker_type == 'email':\n        fake_value = fake.email()\n    elif faker_type == 'phone_number':\n        fake_value = fake.phone_number()\n    elif faker_type == 'address':\n        fake_value = fake.address()\n    else:\n        fake_value = fake.text(max_nb_chars=20)\n\n    # Store in consistency map\n    consistency_map[key][value] = fake_value\n\n    return fake_value\n```\n\n#### Strategy 3: Tokenization (reversible if needed)\n\n```python\ndef anonymize_tokenize(value, token_map):\n    \"\"\"\n    Replace with token. Optionally reversible.\n    \"\"\"\n    if value not in token_map:\n        token_map[value] = f\"TOKEN_{len(token_map):08d}\"\n    return token_map[value]\n```\n\n#### Strategy 4: Nullification (for highly sensitive data)\n\n```python\ndef anonymize_nullify(value):\n    \"\"\"\n    Remove data entirely by replacing with NULL.\n    \"\"\"\n    return None\n```\n\n### 5. Process All Tables\n\n```python\ndef anonymize_district_data(district_id, input_dir, output_dir, rules):\n    \"\"\"\n    Anonymize all extracted data for a district.\n    \"\"\"\n    consistency_map = initialize_consistency_map()\n    stores = ['ids', 'hcp1', 'hcp2', 'adb', 'sp']\n\n    for store in stores:\n        store_input = f\"{input_dir}/{store}\"\n        store_output = f\"{output_dir}/{store}\"\n\n        # Get all parquet files\n        files = glob(f\"{store_input}/*.parquet\")\n\n        for file_path in files:\n            table_name = os.path.basename(file_path).replace('.parquet', '')\n\n            # Load data\n            df = pd.read_parquet(file_path)\n\n            # Detect PII fields\n            pii_fields = detect_pii_fields(table_name, df.columns, rules)\n\n            # Anonymize each PII field\n            for pii_field in pii_fields:\n                column = pii_field['column']\n                strategy = pii_field['strategy']\n                config = pii_field['config']\n\n                if strategy == 'hash':\n                    df[column] = df[column].apply(\n                        lambda x: anonymize_hash(x, config['salt']) if pd.notna(x) else None\n                    )\n                elif strategy == 'faker':\n                    df[column] = df[column].apply(\n                        lambda x: anonymize_faker(x, config['faker_type'], consistency_map, column) if pd.notna(x) else None\n                    )\n                elif strategy == 'tokenize':\n                    df[column] = df[column].apply(\n                        lambda x: anonymize_tokenize(x, consistency_map.get('tokens', {})) if pd.notna(x) else None\n                    )\n                elif strategy == 'nullify':\n                    df[column] = None\n                elif strategy == 'preserve':\n                    pass  # Don't anonymize (FK fields, etc.)\n\n            # Write anonymized data\n            df.to_parquet(f\"{store_output}/{table_name}.parquet\", compression='snappy')\n\n            log_anonymization(store, table_name, len(pii_fields), len(df))\n\n    return consistency_map\n```\n\n### 6. Validate Anonymization\n\nAfter anonymization, verify no PII leaks:\n\n```python\ndef validate_anonymization(output_dir):\n    \"\"\"\n    Scan anonymized data for potential PII leaks.\n    \"\"\"\n    pii_patterns = [\n        r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # SSN pattern\n        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email\n        r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',  # Phone\n    ]\n\n    leaks = []\n\n    for file_path in glob(f\"{output_dir}/**/*.parquet\", recursive=True):\n        df = pd.read_parquet(file_path)\n\n        for column in df.columns:\n            for value in df[column].dropna().astype(str).unique()[:1000]:  # Sample\n                for pattern in pii_patterns:\n                    if re.search(pattern, str(value)):\n                        leaks.append({\n                            'file': file_path,\n                            'column': column,\n                            'pattern': pattern,\n                            'sample': value[:20] + '...'\n                        })\n\n    return leaks\n```\n\n### 7. Generate Anonymization Report\n\nCreate `data/anonymized/{district_id}/anonymization-report.json`:\n\n```json\n{\n  \"district_id\": \"district-001\",\n  \"anonymized_at\": \"2025-11-06T18:30:00Z\",\n  \"duration_minutes\": 28,\n  \"status\": \"SUCCESS\",\n  \"rules_applied\": 12,\n  \"stores_processed\": {\n    \"ids\": {\n      \"tables\": 45,\n      \"pii_fields_found\": 87,\n      \"records_anonymized\": 250000\n    },\n    \"hcp1\": {\n      \"tables\": 32,\n      \"pii_fields_found\": 65,\n      \"records_anonymized\": 180000\n    },\n    \"hcp2\": {\n      \"tables\": 28,\n      \"pii_fields_found\": 54,\n      \"records_anonymized\": 120000\n    },\n    \"adb\": {\n      \"tables\": 52,\n      \"pii_fields_found\": 102,\n      \"records_anonymized\": 200000\n    },\n    \"sp\": {\n      \"nodes\": 100000,\n      \"pii_properties_found\": 45,\n      \"properties_anonymized\": 150000\n    }\n  },\n  \"totals\": {\n    \"tables_processed\": 157,\n    \"pii_fields_anonymized\": 353,\n    \"records_processed\": 750000,\n    \"unique_values_mapped\": 42500\n  },\n  \"consistency\": {\n    \"first_names_mapped\": 12500,\n    \"last_names_mapped\": 15000,\n    \"emails_mapped\": 8000,\n    \"phones_mapped\": 7000\n  },\n  \"validation\": {\n    \"pii_leak_scan\": \"PASSED\",\n    \"leaks_found\": 0,\n    \"samples_scanned\": 150000\n  },\n  \"consistency_map_location\": \"data/anonymized/district-001/consistency-map.encrypted\"\n}\n```\n\n### 8. Encrypt and Save Consistency Map\n\n```python\ndef save_consistency_map(consistency_map, output_file):\n    \"\"\"\n    Encrypt and save consistency map for audit/reversal if needed.\n    \"\"\"\n    import json\n    from cryptography.fernet import Fernet\n\n    # Generate key (or load from secure storage)\n    key = Fernet.generate_key()\n    cipher = Fernet(key)\n\n    # Serialize map\n    map_json = json.dumps(consistency_map)\n\n    # Encrypt\n    encrypted = cipher.encrypt(map_json.encode())\n\n    # Save encrypted map\n    with open(output_file, 'wb') as f:\n        f.write(encrypted)\n\n    # Save key separately (secure location!)\n    with open(f\"{output_file}.key\", 'wb') as f:\n        f.write(key)\n```\n\n### 9. Report Completion\n\n```\n✓ PII Anonymization Complete\n\nDistrict: district-001\nDuration: 28 minutes\n\nAnonymization Summary:\n- Total PII Fields: 353\n- Records Processed: 750,000\n- Unique Values Mapped: 42,500\n- Strategies Used: hash (125 fields), faker (215 fields), preserve (13 fields)\n\nPII Detection:\n- Names: 187 fields\n- Emails: 45 fields\n- Phones: 38 fields\n- Addresses: 52 fields\n- SSNs: 18 fields\n- Other: 13 fields\n\nValidation: PASSED\n- PII Leak Scan: ✓ 0 leaks found\n- FK Integrity: ✓ All foreign keys preserved\n- Uniqueness: ✓ Unique constraints maintained\n\nConsistency Map:\n- 42,500 unique values mapped\n- Saved to: data/anonymized/district-001/consistency-map.encrypted\n\nOutput: data/anonymized/district-001/\n\nNext step: Validate data integrity\n```\n\n## Tools Available\n\n- **Python Libraries**: pandas, faker, cryptography, pyyaml\n- **ETL MCP**: `anonymize_dataset` tool\n- **Configuration**: `config/anonymization-rules.yaml`\n\n## Success Criteria\n\n- ✓ 100% of PII fields anonymized according to rules\n- ✓ FK relationships preserved (same value → same anonymized value)\n- ✓ Uniqueness constraints maintained\n- ✓ 0 PII leaks detected in validation scan\n- ✓ Anonymization report generated\n- ✓ Consistency map encrypted and saved\n\n## Error Handling\n\n- **Missing rules**: Log warning, use default faker strategy\n- **Invalid values**: Handle nulls gracefully, preserve data types\n- **Consistency conflicts**: Use first-seen mapping, log warning\n- **Encryption failures**: Save plain consistency map with warning\n\nExecute autonomously. Report progress every 10 tables.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "pauljbernard",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/324344?v=4",
      "lastCommit": "2025-11-06T19:39:36Z",
      "createdAt": "2025-11-06T11:54:27Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:23.675Z"
  },
  {
    "id": "machine-manage",
    "source": {
      "type": "github-search",
      "repo": "hsudatalks/univers-machine",
      "path": ".claude/skills/machine-manage/skill.md",
      "url": "https://github.com/hsudatalks/univers-machine/blob/main/.claude/skills/machine-manage/skill.md"
    },
    "frontmatter": {},
    "content": "# Machine Management Skill\n\nThis skill provides comprehensive machine-level management for physical servers, including:\n- **Tmux Session Aggregation**: Manage machine-level desktop and mobile view sessions\n- **Container/VM Lifecycle**: Start, stop, clone, and delete containers/VMs\n- **Resource Monitoring**: Check disk, memory, and resource usage\n- **Cross-Platform Support**: Works on both Linux (LXD) and macOS (OrbStack)\n\n## Core Capabilities\n\n### 1. Machine-Level Tmux Session Management (PRIMARY SKILL)\n- Create and manage aggregated tmux sessions that combine all container/VM sessions\n- `machine-desktop-view` - Full information display for development\n- `machine-mobile-view` - Simplified display for focused work\n- `univers-machine-manage` - Physical machine management session\n- Commands: `mm start`, `mm stop`, `mm status`, `mm attach desktop|mobile`, `mm restart`\n\n### 2. Container/VM Lifecycle Management\nYou are an expert at managing containers and virtual machines:\n- **Linux**: Use LXD containers (`lxc` commands)\n- **macOS**: Use OrbStack VMs (`orb` commands)\n- Automatically detects your OS and uses appropriate commands\n\n### 3. Cross-Platform Abstraction\nThe skill includes a container abstraction library (`lib/container-helper.sh`) that:\n- Detects OS (Linux vs macOS)\n- Detects container system (LXD vs OrbStack)\n- Provides unified commands that work on both platforms\n\n## Available Operations\n\n### 1. Tmux Session Management\n**Start all sessions:**\n- `mm start` - Create desktop-view, mobile-view, and machine-manage sessions\n- Automatically starts containers/VMs if needed\n\n**Manage sessions:**\n- `mm status` - Show status of all machine view sessions\n- `mm attach desktop` - Connect to desktop view\n- `mm attach mobile` - Connect to mobile view\n- `mm stop` - Stop all sessions\n- `mm restart` - Restart all sessions\n\n**Navigation in tmux:**\n- Ctrl+B then 0-4 - Switch between windows\n- Ctrl+B then w - List all windows\n- Ctrl+B then n/p - Next/previous window\n- Ctrl+B then D - Detach\n\n### 2. Container/VM Lifecycle\n**List containers:**\n- Linux: `lxc list`\n- macOS: `orb list`\n- Skill automatically uses correct command\n\n**Lifecycle operations:**\n- **Start**: `container_start <name>` (automatic OS detection)\n- **Stop**: `container_stop <name>`\n- **Restart**: Container operations\n- **Delete**: `container_delete <name>`\n\n### 3. Clone Containers/VMs\n- Use `scripts/clone-vm.sh <source> <dest> [--cleanup]`\n- Optional cleanup of source before cloning\n- Works on both Linux (LXD) and macOS (OrbStack)\n\n### 4. Cleanup Operations\nRun `scripts/cleanup-dev.sh <container>` to:\n- Remove build artifacts (target/ directories)\n- Clear sccache and Cargo caches\n- Clean apt package caches\n- Remove temporary files\n\n### 5. Resource Monitoring\n- Use `scripts/list-resources.sh` to show disk, memory, and large directories\n- Works on both LXD containers and OrbStack VMs\n- Displays resource usage for all running containers/VMs\n\n### 6. Batch Operations\n- Use `scripts/batch-execute.sh '<command>' [--all|<container1> <container2>...]`\n- Execute commands across multiple containers\n- Works on both Linux and macOS\n\n### 7. Direct Container/VM Access via mm shell (RECOMMENDED)\n**IMPORTANT: Always prioritize using `mm shell` for container operations!**\n\nThe `mm shell` command provides a unified interface for accessing containers/VMs with automatic:\n- OS detection (Linux vs macOS)\n- Correct user account handling (ubuntu for LXD, davidxu for OrbStack)\n- Proper parameter quoting and escaping\n- Support for both interactive shells and command execution\n\n**Usage:**\n```bash\n# Interactive shell\nmm shell hvac-dev\n\n# Execute single command\nmm shell hvac-dev \"tmux list-sessions\"\n\n# Execute command with arguments\nmm shell hvac-dev ls -la /home/ubuntu\n\n# Execute complex commands\nmm shell hvac-dev \"tmux kill-session -t univers-developer && sleep 1 && echo 'Done'\"\n```\n\n**Why mm shell is better than direct lxc/orb commands:**\n1. ✅ Automatic user account selection (ubuntu vs davidxu)\n2. ✅ Consistent across Linux and macOS\n3. ✅ Proper shell escaping for special characters\n4. ✅ Support for interactive and non-interactive modes\n5. ✅ Error handling built-in\n\n**Direct container access (legacy - use mm shell instead):**\n- Linux: `lxc exec <name> -- su ubuntu -c '<command>'`\n- macOS: `orb run --machine <name> bash -c '<command>'`\n- ⚠️ These should only be used when mm shell is unavailable\n\n## Scripts Available\n\nAll scripts auto-detect the OS and container system, working seamlessly on both Linux (LXD) and macOS (OrbStack):\n\n- `scripts/machine-view-manager.sh` - **PRIMARY**: Manage machine-level tmux sessions\n  - Start/stop/restart desktop-view, mobile-view, and machine-manage sessions\n  - Used via `mm` alias for convenience\n\n- `scripts/cleanup-dev.sh <container>` - Clean development artifacts\n  - Remove build artifacts, caches, temporary files\n  - Works on both LXD and OrbStack\n\n- `scripts/clone-vm.sh <source> <dest> [--cleanup]` - Clone containers/VMs\n  - Optional cleanup before cloning\n  - Efficient cloning on both platforms\n\n- `scripts/batch-execute.sh '<cmd>' [--all|<containers>]` - Execute commands across multiple containers\n  - Run on all running containers, or specific ones\n  - Cross-platform compatible\n\n- `scripts/list-resources.sh` - Show disk, memory, and resource usage\n  - Works on both LXD containers and OrbStack VMs\n\n## Support Library\n\n- `lib/container-helper.sh` - Cross-platform abstraction layer\n  - Detects OS and container system automatically\n  - Provides unified container management functions\n  - Used by all scripts for platform-agnostic operations\n\n## Usage\n\nWhen the user asks to manage containers, VMs, or tmux sessions:\n\n1. **Detect the context**: Is this about tmux sessions, container lifecycle, resources, or container commands?\n\n2. **Tmux sessions** (primary skill):\n   - Use `mm` command for desktop-view, mobile-view, and machine-manage sessions\n   - Ensure containers/VMs are running before starting sessions\n\n3. **Container/VM command execution** (IMPORTANT - use mm shell):\n   - ✅ **ALWAYS use `mm shell <container> '<command>'` for running commands in containers**\n   - This provides:\n     - Automatic OS detection (Linux vs macOS)\n     - Correct user account handling (ubuntu for LXD, davidxu for OrbStack)\n     - Proper parameter escaping and quoting\n   - Examples:\n     - `mm shell hvac-dev \"tmux list-sessions\"`\n     - `mm shell hvac-dev \"rm -rf /tmp/something\"`\n     - `mm shell hvac-dev \"/path/to/script.sh arg1 arg2\"`\n\n4. **Complex container operations**:\n   - Use scripts for lifecycle operations (clone, cleanup, batch-execute, list-resources)\n   - Rely on `mm shell` for direct command execution\n   - These scripts internally use the container-helper abstraction\n\n5. **Provide clear feedback** about what was done\n\n6. **Suggest related operations** if relevant (e.g., \"Would you like me to clean up before cloning?\")\n\n## Examples\n\n### Tmux Session Management\n- \"Start machine view sessions\" → `mm start`\n- \"Show machine view status\" → `mm status`\n- \"Attach to desktop view\" → `mm attach desktop`\n- \"Attach to mobile view\" → `mm attach mobile`\n- \"Restart all sessions\" → `mm restart`\n\n### Container Command Execution (use mm shell)\n- \"List tmux sessions in hvac-dev\" → `mm shell hvac-dev \"tmux list-sessions\"`\n- \"Kill a tmux session\" → `mm shell hvac-dev \"tmux kill-session -t univers-developer\"`\n- \"Run a script in container\" → `mm shell hvac-dev \"/path/to/script.sh arg1 arg2\"`\n- \"Execute multiple commands\" → `mm shell hvac-dev \"cmd1 && cmd2 && cmd3\"`\n- \"Check file status\" → `mm shell hvac-dev \"ls -la /path/to/file\"`\n\n### Container Lifecycle\n- \"List all containers\" → Use appropriate command (lxc list on Linux, orb list on macOS)\n- \"Clone ubuntu to web-dev\" → `scripts/clone-vm.sh ubuntu web-dev` (with optional --cleanup)\n- \"Clean up ubuntu\" → `scripts/cleanup-dev.sh ubuntu`\n- \"Delete a container\" → Confirm, then execute with appropriate command\n- \"Start all stopped containers\" → Parse list and start each using correct command\n\n### Resource & Batch Operations\n- \"Show resource usage\" → `scripts/list-resources.sh`\n- \"Run command on all containers\" → `scripts/batch-execute.sh '<command>' --all`\n- \"Check container status\" → Automatically detects OS and uses lxc or orb\n\n## Best Practices\n\n1. **ALWAYS use `mm shell` for container commands** - This is the unified interface\n   - ✅ `mm shell hvac-dev \"tmux list-sessions\"`\n   - ❌ Don't use `lxc exec hvac-dev -- su ubuntu -c \"tmux list-sessions\"`\n   - Exception: Only use direct lxc/orb commands if mm shell is temporarily unavailable\n\n2. Always confirm destructive operations (delete, cleanup)\n\n3. Check container/VM status before operations\n\n4. Suggest cleanup before cloning large containers\n\n5. Auto-detect the operating system and use appropriate commands (mm shell does this automatically)\n\n6. Provide clear status updates\n\n7. Handle errors gracefully and suggest alternatives\n\n8. When managing machine views, ensure containers are running first\n\n9. For complex container tasks, use specialized scripts (clone-vm.sh, cleanup-dev.sh, batch-execute.sh)\n\n## Implementation Notes for Claude Code\n\nWhen working with containers in hvac-dev or other LXD/OrbStack containers:\n- **PRIORITY 1**: Use `mm shell <container> '<command>'`\n- **PRIORITY 2**: Use dedicated scripts for specific tasks (clone, cleanup, etc.)\n- **PRIORITY 3**: Use direct container system commands only when mm shell is unavailable\n- Never mix direct `lxc` and `orb` commands without checking the OS first\n\nWhen invoked, help the user manage their containers and machine-level tmux sessions efficiently and safely on both Linux (LXD) and macOS (OrbStack).\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "hsudatalks",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/1204758?v=4",
      "lastCommit": "2026-01-04T07:35:32Z",
      "createdAt": "2025-10-24T06:11:16Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:24.355Z"
  },
  {
    "id": "analyze-schema",
    "source": {
      "type": "github-search",
      "repo": "pauljbernard/datamig",
      "path": ".claude/skills/analyze-schema/skill.md",
      "url": "https://github.com/pauljbernard/datamig/blob/main/.claude/skills/analyze-schema/skill.md"
    },
    "frontmatter": {},
    "content": "# Analyze Schema Skill\n\nYou are a database schema analysis specialist operating autonomously. Your mission: Analyze all 5 data stores (IDS, HCP1, HCP2, ADB, SP) and map their schemas, relationships, and dependencies.\n\n## Autonomous Execution Plan\n\nExecute the following workflow without human intervention:\n\n### 1. Connect to All Data Stores\n\nUse available MCP servers to connect to each data store:\n- **IDS** (PostgreSQL): Use `postgres-ids-prod` MCP server\n- **HCP1** (PostgreSQL): Use `postgres-hcp1-prod` MCP server\n- **HCP2** (PostgreSQL): Use `postgres-hcp2-prod` MCP server\n- **ADB** (PostgreSQL): Use `postgres-adb-prod` MCP server\n- **SP** (Neo4j Graph): Use `neo4j-sp-prod` MCP server\n\n### 2. Extract Schema Information\n\nFor each PostgreSQL database, query the information_schema:\n\n```sql\n-- Get all tables\nSELECT table_schema, table_name, table_type\nFROM information_schema.tables\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema')\nORDER BY table_schema, table_name;\n\n-- Get all columns\nSELECT table_schema, table_name, column_name, data_type,\n       is_nullable, column_default, character_maximum_length\nFROM information_schema.columns\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema')\nORDER BY table_schema, table_name, ordinal_position;\n\n-- Get primary keys\nSELECT tc.table_schema, tc.table_name, kcu.column_name\nFROM information_schema.table_constraints tc\nJOIN information_schema.key_column_usage kcu\n  ON tc.constraint_name = kcu.constraint_name\n  AND tc.table_schema = kcu.table_schema\nWHERE tc.constraint_type = 'PRIMARY KEY';\n\n-- Get foreign keys\nSELECT\n  tc.table_schema,\n  tc.table_name,\n  kcu.column_name,\n  ccu.table_schema AS foreign_table_schema,\n  ccu.table_name AS foreign_table_name,\n  ccu.column_name AS foreign_column_name,\n  tc.constraint_name\nFROM information_schema.table_constraints AS tc\nJOIN information_schema.key_column_usage AS kcu\n  ON tc.constraint_name = kcu.constraint_name\n  AND tc.table_schema = kcu.table_schema\nJOIN information_schema.constraint_column_usage AS ccu\n  ON ccu.constraint_name = tc.constraint_name\n  AND ccu.table_schema = tc.table_schema\nWHERE tc.constraint_type = 'FOREIGN KEY';\n\n-- Get indexes\nSELECT schemaname, tablename, indexname, indexdef\nFROM pg_indexes\nWHERE schemaname NOT IN ('pg_catalog', 'information_schema');\n```\n\nFor Neo4j, use the `get_schema` tool from the neo4j-mcp-server to get:\n- Node labels\n- Relationship types\n- Property keys\n- Constraints\n- Indexes\n\n### 3. Build Dependency Graph\n\nProcess the extracted foreign key relationships to build a dependency graph:\n\n1. Create a directed graph where:\n   - **Nodes** = Tables/Collections\n   - **Edges** = Foreign key relationships (child → parent)\n\n2. Identify:\n   - **Root tables**: Tables with no incoming FKs (no parent tables)\n   - **Leaf tables**: Tables with no outgoing FKs (no child tables)\n   - **Circular dependencies**: Cycles in the graph\n\n3. Document cross-store relationships:\n   - IDS may reference HCP1 via external_id fields\n   - ADB may reference IDS via district/school IDs\n   - SP (Graph) may reference all stores via node properties\n\n### 4. Perform Topological Sort\n\nGenerate extraction order using topological sort algorithm:\n\n1. Calculate in-degree for each table (number of incoming FK edges)\n2. Start with tables having in-degree = 0 (no dependencies)\n3. Process queue:\n   - Remove table from queue\n   - Add to extraction order\n   - Decrement in-degree of child tables\n   - Add child tables with in-degree = 0 to queue\n4. If circular dependencies detected:\n   - Identify cycle participants\n   - Determine optimal break point (least critical FK)\n   - Document manual intervention needed\n\n### 5. Analyze Data Volumes\n\nFor each table, estimate data volumes:\n\n```sql\nSELECT schemaname, tablename, n_live_tup as estimated_rows\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n```\n\nCalculate:\n- Total rows per table\n- Estimated rows per district (if district_id column exists)\n- Data size estimation (rows * avg_row_size)\n\n### 6. Generate Artifacts\n\nCreate the following files in `data/analysis/`:\n\n#### `schema-analysis.json`\nComplete schema documentation for all 5 data stores:\n\n```json\n{\n  \"generated_at\": \"2025-11-06T10:00:00Z\",\n  \"data_stores\": {\n    \"ids\": {\n      \"type\": \"postgresql\",\n      \"connection\": \"postgres-ids-prod\",\n      \"tables\": [\n        {\n          \"schema\": \"public\",\n          \"name\": \"districts\",\n          \"columns\": [\n            {\n              \"name\": \"id\",\n              \"type\": \"bigint\",\n              \"nullable\": false,\n              \"primary_key\": true\n            },\n            {\n              \"name\": \"name\",\n              \"type\": \"varchar\",\n              \"length\": 255,\n              \"nullable\": false\n            }\n          ],\n          \"foreign_keys\": [],\n          \"indexes\": [\"idx_districts_name\"],\n          \"estimated_rows\": 15000\n        }\n      ]\n    },\n    \"sp\": {\n      \"type\": \"neo4j\",\n      \"connection\": \"neo4j-sp-prod\",\n      \"labels\": [\"District\", \"School\", \"Student\", \"Staff\"],\n      \"relationship_types\": [\"BELONGS_TO\", \"ENROLLED_IN\", \"TEACHES\"],\n      \"property_keys\": [\"id\", \"name\", \"district_id\", \"created_at\"],\n      \"constraints\": [],\n      \"indexes\": []\n    }\n  }\n}\n```\n\n#### `dependency-graph.dot`\nGraphViz DOT format visualization of table dependencies:\n\n```dot\ndigraph dependencies {\n  rankdir=LR;\n\n  // Nodes\n  districts [shape=box];\n  schools [shape=box];\n  students [shape=box];\n  enrollments [shape=box];\n\n  // Edges (FK relationships)\n  schools -> districts [label=\"district_id\"];\n  students -> districts [label=\"district_id\"];\n  enrollments -> students [label=\"student_id\"];\n  enrollments -> schools [label=\"school_id\"];\n}\n```\n\n#### `extraction-order.json`\nTopologically sorted extraction order:\n\n```json\n{\n  \"generated_at\": \"2025-11-06T10:00:00Z\",\n  \"stores\": {\n    \"ids\": {\n      \"extraction_order\": [\"districts\", \"schools\", \"students\", \"enrollments\"],\n      \"circular_dependencies\": [],\n      \"total_tables\": 45\n    },\n    \"hcp1\": {\n      \"extraction_order\": [\"...\", \"...\"],\n      \"circular_dependencies\": [\n        {\n          \"tables\": [\"table_a\", \"table_b\"],\n          \"break_point\": \"table_a.fk_to_b\",\n          \"strategy\": \"extract_table_b_first_without_fk_validation\"\n        }\n      ],\n      \"total_tables\": 32\n    }\n  }\n}\n```\n\n#### `data/analysis/README.md`\nHuman-readable summary report:\n\n```markdown\n# Schema Analysis Report\n\nGenerated: 2025-11-06 10:00:00\n\n## Summary\n\n- **Total Data Stores**: 5 (IDS, HCP1, HCP2, ADB, SP)\n- **Total Tables**: 202\n- **Total Foreign Keys**: 487\n- **Circular Dependencies**: 3\n\n## Data Store Breakdown\n\n### IDS (Identity Service)\n- **Type**: PostgreSQL\n- **Tables**: 45\n- **Foreign Keys**: 98\n- **Estimated Rows**: 12.5M\n\n[... detailed breakdown for each store ...]\n\n## Extraction Strategy\n\nExtraction will proceed in topological order within each store.\nCross-store extractions will follow this sequence:\n1. IDS (foundation - districts, schools, users)\n2. HCP1, HCP2 (depend on IDS entities)\n3. ADB (depends on IDS and HCP)\n4. SP (graph - references all stores)\n\n## Circular Dependencies\n\n3 circular dependencies detected:\n1. **IDS**: users ↔ user_sessions (break at user_sessions.user_id)\n2. **HCP1**: ...\n3. **ADB**: ...\n\n[... details and resolution strategies ...]\n```\n\n### 7. Report Completion\n\nGenerate a summary and report back:\n\n```\n✓ Schema Analysis Complete\n\nSummary:\n- Analyzed 5 data stores (IDS, HCP1, HCP2, ADB, SP)\n- Discovered 202 tables with 1,245 columns\n- Mapped 487 foreign key relationships\n- Identified 3 circular dependencies\n- Generated topological extraction order for all stores\n\nArtifacts generated:\n- data/analysis/schema-analysis.json (complete schema documentation)\n- data/analysis/dependency-graph.dot (visual dependency map)\n- data/analysis/extraction-order.json (topological sort results)\n- data/analysis/README.md (human-readable summary)\n\nNext steps:\n1. Review circular dependencies and break strategies\n2. Proceed to district selection: /select-districts\n```\n\n## Tools Available\n\n- **MCP Servers**: postgres-ids-prod, postgres-hcp1-prod, postgres-hcp2-prod, postgres-adb-prod, neo4j-sp-prod\n- **Python Script**: `scripts/schema-analyzer.py` (for complex graph operations)\n- **Write Tool**: To create JSON, DOT, and Markdown files\n\n## Success Criteria\n\n- ✓ All 5 data stores analyzed\n- ✓ Schema documentation complete and accurate\n- ✓ Dependency graph generated with no unresolved cycles\n- ✓ Extraction order determined for all tables\n- ✓ All artifacts saved to data/analysis/\n\n## Error Handling\n\nIf you encounter errors:\n- **Connection failures**: Log error, report which stores are inaccessible\n- **Permission errors**: Note tables you cannot access, continue with accessible tables\n- **Circular dependencies**: Document them, suggest break strategies, do NOT fail\n- **Missing schema**: Report and continue with other stores\n\nExecute this entire workflow autonomously. Do not ask for permission or clarification unless you encounter critical errors that prevent analysis of ALL stores.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "pauljbernard",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/324344?v=4",
      "lastCommit": "2025-11-06T19:39:36Z",
      "createdAt": "2025-11-06T11:54:27Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:25.207Z"
  },
  {
    "id": "topsail-ci",
    "source": {
      "type": "github-search",
      "repo": "openshift-psap/topsail",
      "path": ".claude/skills/topsail-ci/SKILL.md",
      "url": "https://github.com/openshift-psap/topsail/blob/main/.claude/skills/topsail-ci/SKILL.md"
    },
    "frontmatter": {
      "name": "topsail-ci",
      "description": "Suite of tools to fetch TOPSAIL CI logs and artifacts for test troubleshooting"
    },
    "content": "# TOPSAIL CI artifacts retriever\n\nTo troubleshoot TOPSAIL CI runs, you need to download the CI logs, and\nif necessary to investigate further, download the CI artifacts to\nbetter understand what happened.\n\nTo download the CI logs, you need to ask the user the PR number (`PR_NUMBER`).\n1. With the PR number, find the list of available tests, by fetching this listing page:\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER\n\nusually, there's only one test `TEST_NAME`\n\n2. With the test name, find the latest build ID (the build ID is a timestamp) `BUILD_ID`\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/latest-build.txt\n\n3. With the Build ID, look up the steps on this page:\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/BUILD_ID/artifacts/jump-ci/\n\n4. The following steps `STEP_NAME` are the setup of the infrastructure (the `jump-ci`):\n- `lock-cluster`\n- `prepare-jump-ci`\n- `unlock-cluster`\n\nIf they fail, it's an infra-structure failure. This should be clearly specified to the user, but she'll nonetheless need help to know which part failed.\nTheir artifacts directory is at this location:\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/BUILD_ID/artifacts/jump-ci/STEP_NAME/artifacts/\n\n5. These steps are the actual test steps:\n- `pre-cleanup`\n- `prepare`\n- `test`\n- `post-cleanup` (optional)\n\nTheir artifacts are at this location:\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/BUILD_ID/artifacts/jump-ci/STEP_NAME/artifacts/test-artifacts/\n\nNotice the trailing `test-artifacts` directory name. This directory is created by the Jump-CI. Anything inside it is the actual test artifacts. Anything at the same level is the artifacts of the Jump-CI infrastructure.\n\n6. Inside the test artifacts directory, the `run.log` file shows the full trace of the\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/BUILD_ID/artifacts/jump-ci/STEP_NAME/artifacts/test-artifacts/run.log\n\nthis file might be long and hard to grasp (for the user).\n\n7. Inside the test artifacts directory, the `FAILURES` file, if present, gives an indication of what failed:\n\n> https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/BUILD_ID/artifacts/jump-ci/STEP_NAME/artifacts/test-artifacts/FAILURES\n\n8. This command helps downloading locally a full directory `DIRECTORY` of artifacts:\n\n> gsutil -m cp -r gs://test-platform-results/pr-logs/pull/openshift-psap_topsail/PR_NUMBER/TEST_NAME/BUILD_ID/artifacts/jump-ci/STEP_NAME/artifacts/test-artifacts/DIRECTORY .\n\n(the command is also shown at the bottom of the directory listing).\n",
    "github": {
      "stars": 16,
      "forks": 20,
      "owner": "openshift-psap",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/55542927?v=4",
      "license": "Apache-2.0",
      "lastCommit": "2026-01-01T00:42:13Z",
      "createdAt": "2023-07-20T15:57:51Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:25.862Z"
  },
  {
    "id": "cleanup",
    "source": {
      "type": "github-search",
      "repo": "Brownbull/taskflow",
      "path": ".claude/skills/utility/cleanup/skill.md",
      "url": "https://github.com/Brownbull/taskflow/blob/main/.claude/skills/utility/cleanup/skill.md"
    },
    "frontmatter": {
      "name": "cleanup",
      "description": "Project organization expert - analyzes project state, detects redundancy, consolidates files, removes obsolete content, maintains structure integrity",
      "tier": "1-2",
      "phase": "[\"mvp\", \"growth\", \"scale\"]",
      "version": "3.0.0"
    },
    "content": "# Cleanup & Organization Expert - V3\r\n\r\n## Purpose\r\n\r\n**Phase-aware project organization skill** that analyzes and maintains clean, well-structured codebases. Understands project conventions, identifies redundancy, consolidates files, and removes obsolete content while respecting your current development phase.\r\n\r\n## Key Capabilities\r\n\r\n- **Analyze project state** - Understand current folder structure\r\n- **Detect redundancy** - Find duplicate and overlapping content\r\n- **Consolidate files** - Merge redundant documentation\r\n- **Remove obsolete content** - Identify and delete deprecated files\r\n- **Phase-appropriate cleanup** - Different standards per phase\r\n- **Generate reports** - Detailed operation logs\r\n\r\n## When to Invoke\r\n\r\nUse this skill when:\r\n- Project has accumulated duplicate/scattered files\r\n- Multiple files contain redundant content\r\n- Files need reorganization\r\n- Obsolete files need removal\r\n- After major refactoring\r\n- Before shipping to next phase\r\n\r\n**Command**: `/cleanup` or invoke via Skill tool\r\n\r\n## Phase-Appropriate Cleanup\r\n\r\n### Prototype Phase\r\n- **Minimal cleanup** - Keep it loose\r\n- Remove only obvious duplicates\r\n- Don't enforce strict organization\r\n- Focus: Keep it shipping\r\n\r\n### MVP Phase\r\n- **Basic organization** - Some structure\r\n- Consolidate major duplicates\r\n- Remove clearly obsolete files\r\n- Focus: Maintain clarity\r\n\r\n### Growth Phase\r\n- **Structured cleanup** - Enforce conventions\r\n- Detect and merge redundant docs\r\n- Organize by standards\r\n- Focus: Maintainability\r\n\r\n### Scale Phase\r\n- **Complete organization** - Full enforcement\r\n- Comprehensive redundancy detection\r\n- Strict folder structure\r\n- Focus: Enterprise quality\r\n\r\n## Workflow\r\n\r\n### 1. Analyze Current State\r\n\r\n**Understand project phase**:\r\n```bash\r\n# Check current phase\r\ncat .khujta/phase.json\r\n```\r\n\r\n**Scan project structure**:\r\n- Read project documentation (CLAUDE.md, README.md)\r\n- Identify key folders (ai-state/, docs/, src/, tests/)\r\n- Map current file locations\r\n- Detect patterns and conventions\r\n\r\n**Generate state report**:\r\n- Total files by type/category\r\n- Misplaced files (wrong location)\r\n- Duplicate/redundant content\r\n- Obsolete file candidates\r\n- Phase compliance status\r\n\r\n### 2. Detect Redundancy\r\n\r\n**Content Similarity Analysis**:\r\n- Compare file contents pairwise\r\n- Calculate similarity scores (0-100%)\r\n- Identify duplicate/near-duplicate clusters\r\n- Group files by content similarity\r\n\r\n**Pattern Recognition**:\r\n- Versioned files (file_v1.md, file_v2.md, file_final.md)\r\n- Dated files (notes_2025_10.md, notes_2025_11.md)\r\n- Topic variants (auth.md, authentication.md, login.md)\r\n\r\n**Redundancy Scoring**:\r\n- HIGH (9-10): Immediate consolidation recommended\r\n- MEDIUM (6-8): User review suggested\r\n- LOW (3-5): Optional optimization\r\n\r\n**Phase-Appropriate Thresholds**:\r\n- Prototype: Only flag 95%+ similarity (near-identical)\r\n- MVP: Flag 80%+ similarity\r\n- Growth: Flag 70%+ similarity\r\n- Scale: Flag 60%+ similarity\r\n\r\n### 3. Categorize Files\r\n\r\n**Decision tree per phase**:\r\n\r\n```\r\nFILE → Check phase requirements\r\n       ├─ Prototype → Skip unless obvious duplicate\r\n       ├─ MVP → Basic categorization\r\n       ├─ Growth → Structured organization\r\n       └─ Scale → Strict enforcement\r\n\r\n     → Is it redundant?\r\n       ├─ YES → Flag for merge/consolidate\r\n       └─ NO → Is it in correct location?\r\n               ├─ YES → Skip\r\n               └─ NO → Flag for move\r\n\r\n     → Is it obsolete?\r\n       ├─ YES → Flag for deletion\r\n       └─ NO → Keep\r\n```\r\n\r\n### 4. Plan Operations\r\n\r\n**Group operations by type**:\r\n1. **MERGE** - Consolidate redundant files\r\n2. **MOVE** - Relocate misplaced files\r\n3. **DELETE** - Remove obsolete files\r\n4. **RENAME** - Standardize naming\r\n\r\n**Phase-Appropriate Planning**:\r\n- **Prototype**: Only merges and deletes\r\n- **MVP**: Add basic moves\r\n- **Growth**: Full organization\r\n- **Scale**: Strict enforcement + renames\r\n\r\n**Safety checks**:\r\n- Verify destinations exist\r\n- Check for overwrite conflicts\r\n- Confirm deletion safety\r\n- Preserve unique content\r\n\r\n### 5. Execute with Approval\r\n\r\n**Present operation plan**:\r\n```markdown\r\n## Cleanup Plan for [Phase] Phase\r\n\r\n**Current State**:\r\n- 156 files total\r\n- 8 redundant files detected\r\n- 5 misplaced files\r\n- 3 obsolete files\r\n\r\n**Proposed Operations**:\r\n\r\n### MERGE (3 operations)\r\n1. setup_v1.md + setup_v2.md → setup.md\r\n   Similarity: 85% | Recommendation: Keep latest\r\n\r\n### MOVE (5 operations)\r\n1. old_spec.md → ai-state/archive/\r\n2. test_results.md → ai-state/regressions/\r\n\r\n### DELETE (3 operations)\r\n1. BACKUP_file.md (obsolete, 92 days old)\r\n2. temp_notes.txt (temporary file)\r\n\r\n**Approve? (yes/no/skip-deletes)**\r\n```\r\n\r\n**Execute approved operations**:\r\n- Perform in safe order: MERGE → MOVE → DELETE\r\n- Log each operation with timestamp\r\n- Handle errors gracefully\r\n- Update operations log\r\n\r\n### 6. Generate Report\r\n\r\n**Create cleanup report**:\r\n```markdown\r\n# Cleanup Report - [Date]\r\n\r\n## Summary\r\n- Phase: [prototype/mvp/growth/scale]\r\n- Operations: 11 total (3 merge, 5 move, 3 delete)\r\n- Files reduced: 156 → 148\r\n- Duration: 2 minutes\r\n\r\n## Detailed Operations\r\n\r\n### Merges (3)\r\n✅ Merged setup_v1.md + setup_v2.md → setup.md\r\n✅ Consolidated auth docs → authentication.md\r\n✅ Merged test results → TEST_RESULTS.md\r\n\r\n### Moves (5)\r\n✅ Moved old_spec.md → ai-state/archive/\r\n...\r\n\r\n### Deletions (3)\r\n✅ Deleted BACKUP_file.md\r\n...\r\n\r\n## Impact\r\n- Clarity: Improved documentation findability\r\n- Maintenance: Fewer files to update\r\n- Storage: 45KB saved\r\n\r\n## Recommendations\r\n- Consider archiving files older than 90 days\r\n- Set up automated cleanup hook\r\n```\r\n\r\n**Save report**: `ai-state/cleanup/cleanup_report_YYYYMMDD.md`\r\n\r\n## File Organization Standards\r\n\r\n**Standard locations for Khujta Sphere V3**:\r\n\r\n| Content Type | Location | Phase |\r\n|-------------|----------|-------|\r\n| Phase config | `.khujta/phase.json` | All |\r\n| Operations log | `ai-state/operations.log` | All |\r\n| Task files | `ai-state/active/tasks.yaml` | All |\r\n| Test debt | `ai-state/debt/` | MVP+ |\r\n| Regressions | `ai-state/regressions/` | MVP+ |\r\n| Knowledge | `ai-state/knowledge/` | Growth+ |\r\n| Standards | `ai-state/standards/` | Growth+ |\r\n| Human docs | `ai-state/human-docs/` | Scale |\r\n\r\n## Obsolescence Detection\r\n\r\n**Files are likely obsolete if**:\r\n- Contains \"OLD\", \"BACKUP\", \"DEPRECATED\", \"TEMP\" in name\r\n- Creation date > 90 days + no modifications\r\n- Duplicates content in primary files\r\n- References outdated architecture\r\n- Not referenced by any other file\r\n\r\n**Phase-Appropriate Deletion**:\r\n- **Prototype**: Delete only TEMP and BACKUP files\r\n- **MVP**: Add files >90 days old\r\n- **Growth**: Add unreferenced files\r\n- **Scale**: Comprehensive cleanup\r\n\r\n**Manual review required**:\r\n- Configuration files\r\n- Data files (CSVs, JSONs)\r\n- Files > 100 lines\r\n- Files with \"IMPORTANT\" markers\r\n\r\n## Safety Guidelines\r\n\r\n**CRITICAL RULES**:\r\n1. **ALWAYS get user approval** before executing\r\n2. **NEVER delete without showing** what will be removed\r\n3. **ALWAYS preserve unique content** during merges\r\n4. **ALWAYS generate operation report** for audit\r\n5. **Respect phase requirements** - don't over-organize in prototype\r\n\r\n**Dry-run mode**: Present plan WITHOUT executing (default)\r\n\r\n**Execution mode**: Only after explicit user approval\r\n\r\n## Example Usage\r\n\r\n### Prototype Phase Example\r\n\r\n**User**: `/cleanup`\r\n\r\n**Response**:\r\n```\r\n## Cleanup Analysis (Prototype Phase)\r\n\r\n**Current State**: 45 files\r\n**Phase Policy**: Minimal cleanup only\r\n\r\n**Redundancy Detection**:\r\n- 2 obvious duplicates found (98% similarity)\r\n- Keeping relaxed - prototype phase allows disorder\r\n\r\n**Proposed Operations**:\r\n\r\nMERGE (2 operations):\r\n1. notes.md + notes_copy.md → notes.md (identical)\r\n\r\nDELETE (1 operation):\r\n1. TEMP_scratch.txt (temporary file)\r\n\r\n**Phase Note**: Skipping organization - you're in prototype mode!\r\nShip fast, clean later.\r\n\r\nApprove? (yes/no)\r\n```\r\n\r\n### Scale Phase Example\r\n\r\n**User**: `/cleanup`\r\n\r\n**Response**:\r\n```\r\n## Cleanup Analysis (Scale Phase)\r\n\r\n**Current State**: 156 files\r\n**Phase Policy**: Comprehensive cleanup + strict organization\r\n\r\n**Redundancy Detection**:\r\nHIGH (3 groups): 85% avg similarity\r\nMEDIUM (2 groups): 70% avg similarity\r\n\r\n**Organization Issues**:\r\n- 12 files in wrong locations\r\n- 3 files missing standard naming\r\n- 5 files exceed age threshold (>90 days)\r\n\r\n**Proposed Operations**:\r\n\r\nMERGE (5 operations):\r\n[... detailed list ...]\r\n\r\nMOVE (12 operations):\r\n[... relocations to proper folders ...]\r\n\r\nRENAME (3 operations):\r\n[... standardize naming ...]\r\n\r\nDELETE (5 operations):\r\n[... obsolete files ...]\r\n\r\n**Impact**:\r\n156 → 131 files (16% reduction)\r\nFull standards compliance achieved\r\n\r\nApprove? (yes/no/skip-deletes/modify)\r\n```\r\n\r\n## Integration with V3 Framework\r\n\r\n**Works with**:\r\n- `/test-cleanup` command (test-specific cleanup)\r\n- `operations-logger` skill (logs all operations)\r\n- Phase validator hook (respects phase limits)\r\n- Quality gate hook (maintains standards)\r\n\r\n**Complementary commands**:\r\n- `/test-cleanup` - Clean test files specifically\r\n- `/test-phase [phase]` - Switch phase before cleanup\r\n- `/regression-quick` - Verify after cleanup\r\n\r\n## Escape Hatches\r\n\r\nIf cleanup is blocking you:\r\n- `/skip-cleanup` - Defer organization\r\n- `/simple-cleanup` - Only obvious duplicates\r\n- `/prototype-mode` - Switch to minimal requirements\r\n\r\n## Quality Standards\r\n\r\nCleanup operations must meet (phase-appropriate):\r\n\r\n**Prototype (6.0/10)**:\r\n- Remove obvious duplicates only\r\n- Don't enforce strict organization\r\n- Preserve flexibility\r\n\r\n**MVP (7.0/10)**:\r\n- Basic redundancy consolidation\r\n- Simple folder organization\r\n- Keep it straightforward\r\n\r\n**Growth (7.5/10)**:\r\n- Structured organization\r\n- Comprehensive redundancy detection\r\n- Standards enforcement\r\n\r\n**Scale (8.0/10)**:\r\n- Full standards compliance\r\n- Complete redundancy elimination\r\n- Enterprise-grade organization\r\n\r\n## Output Example\r\n\r\n```markdown\r\n# Cleanup Operation Report\r\n\r\n**Date**: 2025-11-06T14:30:00Z\r\n**Phase**: MVP\r\n**Duration**: 3 minutes\r\n\r\n## Summary\r\n- Files analyzed: 156\r\n- Operations executed: 11\r\n- Files after cleanup: 148\r\n- Reduction: 5.1%\r\n\r\n## Operations\r\n\r\n### Merged (3)\r\n✅ setup_v1.md + setup_v2.md → setup.md\r\n   Similarity: 85% | Unique content preserved\r\n\r\n### Moved (5)\r\n✅ old_spec.md → ai-state/archive/specs/\r\n✅ test_results_20251105.md → ai-state/regressions/\r\n\r\n### Deleted (3)\r\n✅ BACKUP_config.md (obsolete backup)\r\n✅ TEMP_notes.txt (temporary, 92 days old)\r\n✅ old_implementation.md (replaced by current version)\r\n\r\n## Impact\r\n- ✅ Documentation clarity improved\r\n- ✅ Easier to find authoritative sources\r\n- ✅ Reduced maintenance overhead\r\n\r\n## Next Cleanup Suggested\r\n- In 30 days (or after next major refactoring)\r\n- Consider setting up automated cleanup hook\r\n\r\n**Status**: ✅ COMPLETE\r\n```\r\n\r\n---\r\n\r\n**Version**: 3.0.0\r\n**Tier**: 1-2 (MVP, Growth, Scale)\r\n**Skill Type**: Project Organization & Maintenance\r\n**Last Updated**: 2025-11-06",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "Brownbull",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/16549632?v=4",
      "lastCommit": "2025-11-10T00:22:56Z",
      "createdAt": "2025-11-07T00:56:34Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:26.530Z"
  },
  {
    "id": "brainstorm",
    "source": {
      "type": "github-search",
      "repo": "Brownbull/taskflow",
      "path": ".claude/skills/core/brainstorm/skill.md",
      "url": "https://github.com/Brownbull/taskflow/blob/main/.claude/skills/core/brainstorm/skill.md"
    },
    "frontmatter": {},
    "content": "# Brainstorm Skill\r\n\r\n## Purpose\r\nRefines rough ideas and requirements into fully-formed designs through collaborative questioning, alternative exploration, and incremental validation using the Socratic method.\r\n\r\n## Tier\r\n**Tier 0** - Pre-Launch Foundation\r\n\r\n## When to Use\r\n- Starting new features or projects\r\n- Converting vague requirements into concrete tasks\r\n- Exploring design alternatives\r\n- Identifying potential issues early\r\n- Clarifying ambiguous requirements\r\n\r\n## Process\r\n\r\n### 1. Requirement Analysis\r\n- Parse the initial request\r\n- Identify key stakeholders\r\n- Extract functional and non-functional requirements\r\n- Determine success criteria\r\n\r\n### 2. Socratic Questioning\r\nApply iterative questioning to refine:\r\n- **What**: What exactly needs to be built?\r\n- **Why**: Why is this needed? What problem does it solve?\r\n- **Who**: Who are the users? What are their needs?\r\n- **How**: How should it work? What are the alternatives?\r\n- **When**: When is it needed? What are the milestones?\r\n- **Where**: Where will it run? What are the constraints?\r\n\r\n### 3. Alternative Exploration\r\n- Generate 3-5 alternative approaches\r\n- Evaluate pros and cons of each\r\n- Consider technical debt implications\r\n- Assess scalability requirements\r\n\r\n### 4. Validation\r\n- Check against existing patterns\r\n- Verify technical feasibility\r\n- Ensure alignment with quality standards\r\n- Confirm budget and timeline constraints\r\n\r\n### 5. Output Generation\r\nProduces refined requirements including:\r\n- Clear user stories\r\n- Technical constraints\r\n- Quality criteria\r\n- Risk assessment\r\n- Recommended approach\r\n\r\n## Input Format\r\n```yaml\r\nrequest: \"Natural language description of the idea\"\r\ncontext: \"Optional background information\"\r\nconstraints: \"Any known limitations\"\r\n```\r\n\r\n## Output Format\r\n```yaml\r\nrefined_requirements:\r\n  user_stories:\r\n    - as_a: \"User role\"\r\n      i_want: \"Feature/capability\"\r\n      so_that: \"Business value\"\r\n\r\n  technical_requirements:\r\n    - category: \"Frontend/Backend/Data\"\r\n      requirement: \"Specific technical need\"\r\n      priority: \"Must/Should/Could\"\r\n\r\n  alternatives_considered:\r\n    - approach: \"Description\"\r\n      pros: [\"List of advantages\"]\r\n      cons: [\"List of disadvantages\"]\r\n      recommendation: \"Yes/No with reasoning\"\r\n\r\n  quality_criteria:\r\n    - metric: \"Measurable criteria\"\r\n      threshold: \"Minimum acceptable value\"\r\n\r\n  risks:\r\n    - risk: \"Description\"\r\n      impact: \"High/Medium/Low\"\r\n      mitigation: \"Strategy to address\"\r\n\r\n  recommended_approach:\r\n    summary: \"Recommended solution\"\r\n    justification: \"Why this approach\"\r\n    next_steps: [\"Ordered list of actions\"]\r\n```\r\n\r\n## Integration Points\r\n- **Output feeds into**: write-plan skill\r\n- **Receives context from**: User input, existing documentation\r\n- **Updates**: ai-state/knowledge/requirements.md\r\n\r\n## Quality Standards\r\n- All requirements must be testable\r\n- User stories must follow standard format\r\n- Technical requirements must specify measurability\r\n- At least 3 alternatives must be considered\r\n- Risk assessment is mandatory\r\n\r\n## Example Usage\r\n```bash\r\n/brainstorm \"Create a payment processing system for our fintech app\"\r\n```\r\n\r\nThis will:\r\n1. Analyze payment processing requirements\r\n2. Question assumptions about payment types, currencies, compliance\r\n3. Explore alternatives (Stripe, PayPal, custom solution)\r\n4. Validate against fintech regulations\r\n5. Output refined requirements ready for planning\r\n\r\n## Common Patterns\r\n- **MVP First**: Always identify the minimum viable version\r\n- **Progressive Enhancement**: Plan for future scaling\r\n- **Fail Fast**: Identify deal-breakers early\r\n- **User-Centric**: Focus on user value, not technical elegance\r\n\r\n## Anti-Patterns to Avoid\r\n- Skipping alternatives analysis\r\n- Accepting vague requirements\r\n- Ignoring non-functional requirements\r\n- Not considering technical debt\r\n- Missing risk assessment\r\n\r\n## Metrics\r\n- Time from idea to refined requirements: < 30 minutes\r\n- Requirement clarity score: > 8/10\r\n- Alternative approaches generated: >= 3\r\n- Risk identification rate: 100%",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "Brownbull",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/16549632?v=4",
      "lastCommit": "2025-11-10T00:22:56Z",
      "createdAt": "2025-11-07T00:56:34Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:27.202Z"
  },
  {
    "id": "generate-report",
    "source": {
      "type": "github-search",
      "repo": "pauljbernard/datamig",
      "path": ".claude/skills/generate-report/skill.md",
      "url": "https://github.com/pauljbernard/datamig/blob/main/.claude/skills/generate-report/skill.md"
    },
    "frontmatter": {},
    "content": "# Generate Report Skill\n\nYou are a reporting specialist operating autonomously. Your mission: Generate comprehensive migration reports by aggregating artifacts from all migration phases.\n\n## Input Parameters\n\n- `run_id`: Migration run identifier\n- `district_id`: District that was migrated\n- `output_formats`: Report formats to generate (default: [\"json\", \"markdown\", \"html\"])\n\n## Autonomous Execution Plan\n\n### 1. Collect All Artifacts\n\nGather outputs from each migration phase:\n\n```python\ndef collect_artifacts(district_id, run_id):\n    \"\"\"\n    Collect all artifacts from migration run.\n    \"\"\"\n    artifacts = {\n        'extraction': read_json(f'data/staging/{district_id}/extraction-manifest.json'),\n        'anonymization': read_json(f'data/anonymized/{district_id}/anonymization-report.json'),\n        'validation': read_json(f'data/anonymized/{district_id}/validation-report.json'),\n        'loading': read_json(f'data/loads/{district_id}/load-report.json'),\n        'district_info': get_district_from_manifest(district_id),\n        'run_metadata': {\n            'run_id': run_id,\n            'district_id': district_id,\n            'generated_at': datetime.utcnow().isoformat() + 'Z'\n        }\n    }\n\n    return artifacts\n```\n\n### 2. Calculate Summary Metrics\n\n```python\ndef calculate_summary(artifacts):\n    \"\"\"\n    Calculate overall migration metrics.\n    \"\"\"\n    extraction = artifacts['extraction']\n    anonymization = artifacts['anonymization']\n    validation = artifacts['validation']\n    loading = artifacts['loading']\n\n    # Timing\n    start_time = parse_iso(extraction['started_at'])\n    end_time = parse_iso(loading['completed_at'])\n    total_duration = (end_time - start_time).total_seconds() / 60  # minutes\n\n    # Phase durations\n    phase_durations = {\n        'extraction': extraction['duration_minutes'],\n        'anonymization': anonymization['duration_minutes'],\n        'validation': validation['duration_minutes'],\n        'loading': loading['duration_minutes']\n    }\n\n    # Records\n    total_records_extracted = extraction['totals']['records']\n    total_records_anonymized = anonymization['totals']['records_processed']\n    total_records_loaded = loading['totals']['records_loaded']\n\n    # Status\n    overall_status = determine_overall_status(artifacts)\n\n    return {\n        'start_time': start_time.isoformat(),\n        'end_time': end_time.isoformat(),\n        'total_duration_minutes': round(total_duration, 1),\n        'total_duration_hours': round(total_duration / 60, 2),\n        'phase_durations': phase_durations,\n        'total_records_extracted': total_records_extracted,\n        'total_records_anonymized': total_records_anonymized,\n        'total_records_loaded': total_records_loaded,\n        'data_loss_pct': ((total_records_extracted - total_records_loaded) / total_records_extracted * 100),\n        'overall_status': overall_status\n    }\n```\n\n### 3. Analyze Phase Performance\n\n```python\ndef analyze_phases(artifacts):\n    \"\"\"\n    Detailed analysis of each migration phase.\n    \"\"\"\n    phases = {\n        'extraction': {\n            'status': artifacts['extraction']['status'],\n            'duration_minutes': artifacts['extraction']['duration_minutes'],\n            'records_extracted': artifacts['extraction']['totals']['records'],\n            'tables_extracted': artifacts['extraction']['totals']['tables'],\n            'size_mb': artifacts['extraction']['totals']['size_mb'],\n            'throughput_records_per_minute': artifacts['extraction']['totals']['records'] / artifacts['extraction']['duration_minutes'],\n            'stores': artifacts['extraction']['stores']\n        },\n        'anonymization': {\n            'status': artifacts['anonymization']['status'],\n            'duration_minutes': artifacts['anonymization']['duration_minutes'],\n            'pii_fields_anonymized': artifacts['anonymization']['totals']['pii_fields_anonymized'],\n            'records_processed': artifacts['anonymization']['totals']['records_processed'],\n            'unique_values_mapped': artifacts['anonymization']['totals']['unique_values_mapped'],\n            'throughput_records_per_minute': artifacts['anonymization']['totals']['records_processed'] / artifacts['anonymization']['duration_minutes'],\n            'pii_leak_status': artifacts['anonymization']['validation']['pii_leak_scan']\n        },\n        'validation': {\n            'status': artifacts['validation']['overall_status'],\n            'duration_minutes': artifacts['validation']['duration_minutes'],\n            'checks_run': artifacts['validation']['checks_run'],\n            'errors': artifacts['validation']['summary']['total_errors'],\n            'warnings': artifacts['validation']['summary']['total_warnings'],\n            'validation_results': artifacts['validation']['validation_results']\n        },\n        'loading': {\n            'status': artifacts['loading']['overall_status'],\n            'duration_minutes': artifacts['loading']['duration_minutes'],\n            'records_loaded': artifacts['loading']['totals']['records_loaded'],\n            'tables_loaded': artifacts['loading']['totals']['tables_loaded'],\n            'throughput_records_per_minute': artifacts['loading']['totals']['records_loaded'] / artifacts['loading']['duration_minutes'],\n            'stores': artifacts['loading']['stores']\n        }\n    }\n\n    return phases\n```\n\n### 4. Generate JSON Report\n\nCreate `data/reports/{run_id}.json`:\n\n```json\n{\n  \"migration_report\": {\n    \"run_id\": \"mig-20251106-001\",\n    \"district_id\": \"district-001\",\n    \"district_name\": \"Large Urban District A\",\n    \"generated_at\": \"2025-11-06T22:00:00Z\",\n    \"report_version\": \"1.0\",\n    \"summary\": {\n      \"overall_status\": \"SUCCESS\",\n      \"start_time\": \"2025-11-06T14:00:00Z\",\n      \"end_time\": \"2025-11-06T21:45:00Z\",\n      \"total_duration_hours\": 7.75,\n      \"phase_durations_minutes\": {\n        \"extraction\": 135,\n        \"anonymization\": 28,\n        \"validation\": 12,\n        \"loading\": 105\n      },\n      \"records\": {\n        \"extracted\": 750000,\n        \"anonymized\": 750000,\n        \"loaded\": 750000,\n        \"data_loss_pct\": 0.0\n      }\n    },\n    \"phases\": { /* detailed phase analysis */ },\n    \"stores\": {\n      \"ids\": { /* IDS metrics */ },\n      \"hcp1\": { /* HCP1 metrics */ },\n      \"hcp2\": { /* HCP2 metrics */ },\n      \"adb\": { /* ADB metrics */ },\n      \"sp\": { /* SP metrics */ }\n    },\n    \"validation_summary\": {\n      \"overall_status\": \"PASSED_WITH_WARNINGS\",\n      \"total_checks\": 487,\n      \"errors\": 0,\n      \"warnings\": 3\n    },\n    \"recommendations\": [\n      \"Migration completed successfully\",\n      \"Data is ready for QE testing on CERT\",\n      \"3 business rule warnings found - review recommended\",\n      \"Monitor CERT for any rostering issues during testing\"\n    ]\n  }\n}\n```\n\n### 5. Generate Markdown Report\n\nCreate `data/reports/{run_id}.md`:\n\n```markdown\n# Migration Report: district-001\n\n**Migration Run**: mig-20251106-001\n**District**: Large Urban District A (district-001)\n**Status**: ✅ SUCCESS\n**Date**: 2025-11-06\n**Duration**: 7.75 hours\n\n---\n\n## Executive Summary\n\nSuccessfully migrated **750,000 records** across **5 data stores** from PROD to CERT environment.\n\n### Key Metrics\n\n| Metric | Value |\n|--------|-------|\n| Total Records | 750,000 |\n| Data Stores | 5 (IDS, HCP1, HCP2, ADB, SP) |\n| Tables Migrated | 157 |\n| PII Fields Anonymized | 353 |\n| Validation Checks | 487 (PASSED) |\n| Data Loss | 0.0% |\n| Total Duration | 7.75 hours |\n\n---\n\n## Phase Breakdown\n\n### 1. Extraction Phase ✅\n**Duration**: 2.25 hours (135 minutes)\n**Status**: SUCCESS\n\n- **Records Extracted**: 750,000\n- **Tables**: 157\n- **Data Size**: 4.5 GB\n- **Throughput**: 5,555 records/minute\n\n#### By Data Store\n\n| Store | Tables | Records | Size |\n|-------|--------|---------|------|\n| IDS | 45 | 250,000 | 1.2 GB |\n| HCP1 | 32 | 180,000 | 850 MB |\n| HCP2 | 28 | 120,000 | 600 MB |\n| ADB | 52 | 200,000 | 1.5 GB |\n| SP (Graph) | 100K nodes | 250K rels | 400 MB |\n\n### 2. Anonymization Phase ✅\n**Duration**: 28 minutes\n**Status**: SUCCESS\n\n- **PII Fields Detected**: 353\n- **Records Anonymized**: 750,000\n- **Unique Values Mapped**: 42,500\n- **Throughput**: 26,785 records/minute\n- **PII Leak Scan**: ✅ PASSED (0 leaks)\n\n#### PII Categories\n\n- Names: 187 fields\n- Emails: 45 fields\n- Phones: 38 fields\n- Addresses: 52 fields\n- SSNs: 18 fields\n\n### 3. Validation Phase ✅\n**Duration**: 12 minutes\n**Status**: PASSED WITH WARNINGS\n\n- **Checks Run**: 487\n- **Errors**: 0 ❌\n- **Warnings**: 3 ⚠️\n\n#### Validation Results\n\n| Check | Status | Details |\n|-------|--------|---------|\n| Schema Validation | ✅ PASSED | 785 checks |\n| Referential Integrity | ✅ PASSED | 312 FK relationships |\n| Uniqueness | ✅ PASSED | 157 checks |\n| Business Rules | ⚠️ WARNINGS | 3 warnings |\n| Completeness | ✅ PASSED | 157 tables |\n\n#### Warnings\n\n1. **student_age_range**: 3 students with age > 22 (likely adult learners)\n2. [Additional warnings...]\n\n### 4. Loading Phase ✅\n**Duration**: 1.75 hours (105 minutes)\n**Status**: SUCCESS\n\n- **Records Loaded**: 750,000\n- **Tables Loaded**: 157\n- **Throughput**: 7,142 records/minute\n- **Transactions**: All COMMITTED ✅\n\n#### By Data Store\n\n| Store | Status | Records | Duration |\n|-------|--------|---------|----------|\n| IDS | ✅ COMMITTED | 250,000 | 32 min |\n| HCP1 | ✅ COMMITTED | 180,000 | 24 min |\n| HCP2 | ✅ COMMITTED | 120,000 | 18 min |\n| ADB | ✅ COMMITTED | 200,000 | 28 min |\n| SP | ✅ COMMITTED | 350,000 | 3 min |\n\n---\n\n## Performance Analysis\n\n### Throughput by Phase\n\n| Phase | Records/Minute | Efficiency |\n|-------|----------------|------------|\n| Extraction | 5,555 | Baseline |\n| Anonymization | 26,785 | 4.8x faster |\n| Validation | 62,500 | 11.2x faster |\n| Loading | 7,142 | 1.3x |\n\n**Bottleneck**: Loading phase (slowest at 7,142 records/min)\n\n### Timeline\n\n```\n14:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Extraction (135 min)\n16:15 ━━━━━━ Anonymization (28 min)\n16:43 ━━ Validation (12 min)\n16:55 ━━━━━━━━━━━━━━━━━━━━ Loading (105 min)\n18:40 ✅ Complete\n```\n\n---\n\n## Recommendations\n\n1. ✅ **Migration Successful** - Data is ready for QE testing on CERT\n2. 📋 **Review Warnings** - 3 business rule warnings found (non-blocking)\n3. 🧪 **Begin Testing** - Validate rostering flows with migrated data\n4. 📊 **Monitor CERT** - Watch for any issues during initial testing\n5. 🔄 **Schedule Refreshes** - Plan monthly data refresh cycle\n\n---\n\n## Next Steps\n\n1. Notify QE team that CERT is ready with district-001 data\n2. Begin rostering flow testing\n3. Monitor for any data quality issues\n4. Proceed with next district migration if successful\n\n---\n\n## Artifacts\n\n- **Extraction Manifest**: `data/staging/district-001/extraction-manifest.json`\n- **Anonymization Report**: `data/anonymized/district-001/anonymization-report.json`\n- **Validation Report**: `data/anonymized/district-001/validation-report.json`\n- **Load Report**: `data/loads/district-001/load-report.json`\n- **This Report**: `data/reports/mig-20251106-001.md`\n\n---\n\n**Generated**: 2025-11-06 22:00:00 UTC\n**Report Version**: 1.0\n**Claude Code Migration Framework**: v1.0\n```\n\n### 6. Generate HTML Dashboard (Optional)\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Migration Report: district-001</title>\n    <style>\n        /* Responsive dashboard styling */\n    </style>\n</head>\n<body>\n    <div class=\"dashboard\">\n        <h1>Migration Report</h1>\n        <div class=\"status success\">✅ SUCCESS</div>\n\n        <!-- Summary cards -->\n        <div class=\"metrics\">\n            <div class=\"card\">\n                <h3>750,000</h3>\n                <p>Records Migrated</p>\n            </div>\n            <!-- More cards -->\n        </div>\n\n        <!-- Phase timeline -->\n        <div class=\"timeline\">\n            <!-- Visual timeline -->\n        </div>\n\n        <!-- Detailed tables -->\n        <!-- Charts (if visualization library available) -->\n    </div>\n</body>\n</html>\n```\n\n### 7. Send Notifications (Optional)\n\n```python\ndef send_notifications(report, config):\n    \"\"\"\n    Send migration completion notifications.\n    \"\"\"\n    if config.get('email_enabled'):\n        send_email(\n            to=config['stakeholder_emails'],\n            subject=f\"Migration Complete: {report['district_name']}\",\n            body=generate_email_body(report)\n        )\n\n    if config.get('slack_enabled'):\n        post_to_slack(\n            webhook=config['slack_webhook'],\n            message=generate_slack_message(report)\n        )\n```\n\n### 8. Archive Artifacts\n\n```python\ndef archive_artifacts(run_id, district_id):\n    \"\"\"\n    Archive all migration artifacts for audit trail.\n    \"\"\"\n    archive_dir = f\"data/archive/{run_id}\"\n\n    # Copy all artifacts\n    shutil.copytree(f\"data/staging/{district_id}\", f\"{archive_dir}/staging\")\n    shutil.copytree(f\"data/anonymized/{district_id}\", f\"{archive_dir}/anonymized\")\n    shutil.copytree(f\"data/loads/{district_id}\", f\"{archive_dir}/loads\")\n\n    # Create archive tarball\n    shutil.make_archive(f\"{archive_dir}\", 'gztar', archive_dir)\n\n    log_info(f\"Artifacts archived to: {archive_dir}.tar.gz\")\n```\n\n### 9. Report Completion\n\n```\n✓ Migration Report Generated\n\nRun ID: mig-20251106-001\nDistrict: district-001 (Large Urban District A)\nStatus: SUCCESS\n\nReports Generated:\n- JSON: data/reports/mig-20251106-001.json\n- Markdown: data/reports/mig-20251106-001.md\n- HTML: data/reports/mig-20251106-001.html\n\nSummary:\n- Duration: 7.75 hours\n- Records: 750,000 (100% success)\n- Status: ✅ ALL PHASES SUCCESSFUL\n- Warnings: 3 (non-blocking)\n\nRecommendations:\n1. Data ready for QE testing on CERT\n2. Review 3 business rule warnings\n3. Monitor CERT during initial testing\n\nArtifacts archived to: data/archive/mig-20251106-001.tar.gz\n\nMigration Complete! 🎉\n```\n\n## Tools Available\n\n- **Python Libraries**: pandas, json, jinja2 (for templates)\n- **Report Templates**: `templates/report-template.md`, `templates/report-template.html`\n\n## Success Criteria\n\n- ✓ All artifacts collected successfully\n- ✓ Reports generated in requested formats\n- ✓ Summary metrics calculated\n- ✓ Recommendations provided\n- ✓ Artifacts archived\n\n## Error Handling\n\n- **Missing artifacts**: Note in report, continue with available data\n- **Calculation errors**: Use safe defaults, document assumptions\n\nAlways generate report even if migration had failures.\nReport should clearly indicate status and provide troubleshooting guidance.\n\nExecute autonomously.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "pauljbernard",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/324344?v=4",
      "lastCommit": "2025-11-06T19:39:36Z",
      "createdAt": "2025-11-06T11:54:27Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:28.057Z"
  },
  {
    "id": "write-plan",
    "source": {
      "type": "github-search",
      "repo": "Brownbull/taskflow",
      "path": ".claude/skills/core/write-plan/skill.md",
      "url": "https://github.com/Brownbull/taskflow/blob/main/.claude/skills/core/write-plan/skill.md"
    },
    "frontmatter": {},
    "content": "# Write-Plan Skill V3 - Pragmatic Planning\r\n\r\n## Purpose\r\nCreates adaptive implementation plans with phase-appropriate testing requirements. Plans scale with project maturity from prototype to enterprise.\r\n\r\n## Core Change from V2\r\n**Instead of 8 mandatory test types per task, we now use phase-appropriate testing:**\r\n- Prototype: 2 tests (smoke + happy path)\r\n- MVP: 4 tests (+ critical errors + auth)\r\n- Growth: 5 tests (+ edge cases + performance)\r\n- Scale: 6-8 tests (as actually needed)\r\n\r\n## Phase Detection\r\n```yaml\r\nphase_detection:\r\n  prototype: \"0-100 users, exploring\"\r\n  mvp: \"100-1K users, launching\"\r\n  growth: \"1K-10K users, scaling\"\r\n  scale: \"10K+ users, enterprise\"\r\n```\r\n\r\n## Task Generation V3\r\n\r\n### Adaptive Task Structure\r\n```yaml\r\ntasks:\r\n  - id: \"task-001\"\r\n    epic: \"epic-001\"\r\n    sprint: \"sprint-001\"\r\n    phase: \"prototype\"  # NEW: Current project phase\r\n\r\n    # Core task info\r\n    what: \"Implement login\"\r\n    who: \"backend-orchestrator\"\r\n    where: \"apps/auth/views.py\"\r\n\r\n    # Phase-appropriate testing\r\n    check:\r\n      required:  # Based on phase\r\n        smoke: \"API responds\"\r\n        happy_path: \"User can login\"\r\n      optional:  # Can add if time permits\r\n        error: \"Handle wrong password\"\r\n      deferred:  # Explicitly not doing yet\r\n        edge_cases: \"After MVP\"\r\n        performance: \"When we have users\"\r\n\r\n    # Escape hatches\r\n    overrides:\r\n      skip_tests: false\r\n      defer_quality: false\r\n      prototype_mode: true  # Minimal requirements\r\n```\r\n\r\n## Sprint Planning V3\r\n\r\n### Phase-Aware Sprint Composition\r\n```yaml\r\nprototype_sprint:\r\n  features: 70%  # Ship fast\r\n  testing: 20%   # Minimal\r\n  docs: 5%       # Basic\r\n  buffer: 5%\r\n\r\nmvp_sprint:\r\n  features: 60%\r\n  testing: 25%   # Core tests\r\n  docs: 10%\r\n  buffer: 5%\r\n\r\ngrowth_sprint:\r\n  features: 50%\r\n  testing: 30%   # More comprehensive\r\n  docs: 15%\r\n  buffer: 5%\r\n\r\nscale_sprint:\r\n  features: 40%\r\n  testing: 35%   # Full coverage\r\n  docs: 20%\r\n  buffer: 5%\r\n```\r\n\r\n## Test Requirements by Phase\r\n\r\n### Prototype Tasks\r\n```yaml\r\ncheck:\r\n  smoke: \"Does it run?\"\r\n  happy_path: \"Basic flow works?\"\r\n  # That's it! Just 2 tests\r\n```\r\n\r\n### MVP Tasks\r\n```yaml\r\ncheck:\r\n  smoke: \"Health check\"\r\n  happy_path: \"Main user flow\"\r\n  critical_errors: \"Database down handled\"\r\n  auth: \"Login required works\"\r\n  # Just 4 tests\r\n```\r\n\r\n### Growth Tasks\r\n```yaml\r\ncheck:\r\n  happy_path: \"All user flows\"\r\n  errors: \"Graceful failures\"\r\n  edge_cases: \"Boundary conditions\"\r\n  performance: \"Fast enough\"\r\n  security: \"No obvious vulnerabilities\"\r\n  # 5 tests typical\r\n```\r\n\r\n### Scale Tasks\r\n```yaml\r\ncheck:\r\n  # Add only what's actually needed\r\n  # Not all 8 automatically\r\n```\r\n\r\n## Quality Standards V3\r\n\r\n### Adaptive Quality Gates\r\n```yaml\r\nquality_gates:\r\n  prototype:\r\n    minimum_score: 6.0\r\n    coverage_target: 40%\r\n    test_types_required: 2\r\n\r\n  mvp:\r\n    minimum_score: 7.0\r\n    coverage_target: 60%\r\n    test_types_required: 4\r\n\r\n  growth:\r\n    minimum_score: 7.5\r\n    coverage_target: 70%\r\n    test_types_required: 5\r\n\r\n  scale:\r\n    minimum_score: 8.0\r\n    coverage_target: 80%\r\n    test_types_required: \"as_needed\"\r\n```\r\n\r\n## Escape Hatches in Planning\r\n\r\n### Quick Overrides\r\n```yaml\r\ntask_overrides:\r\n  hotfix_mode:\r\n    skip_all_tests: true\r\n    skip_quality_gates: true\r\n    reason: \"Production emergency\"\r\n\r\n  prototype_mode:\r\n    minimal_tests: true\r\n    quality_gate: 5.0\r\n    reason: \"Exploring idea\"\r\n\r\n  deadline_mode:\r\n    defer_tests: true\r\n    technical_debt_ticket: \"TECH-123\"\r\n    reason: \"Launch deadline\"\r\n```\r\n\r\n## ROI-Based Task Priority\r\n\r\n### Task Value Calculation\r\n```yaml\r\ntask_value:\r\n  critical_path: 10  # Login, payment, data\r\n  user_facing: 7    # Direct user impact\r\n  technical_debt: 3  # Internal cleanup\r\n  nice_to_have: 1   # Polish\r\n\r\ntask_cost:\r\n  complex_logic: 8\r\n  integration: 6\r\n  simple_crud: 2\r\n  ui_only: 1\r\n\r\npriority: value / cost  # Higher = do first\r\n```\r\n\r\n## Example Output - Prototype Phase\r\n\r\n```yaml\r\nproject:\r\n  name: \"Payment System\"\r\n  phase: \"prototype\"  # Key addition\r\n\r\ntasks:\r\n  - id: \"task-001\"\r\n    what: \"Create payment endpoint\"\r\n    phase: \"prototype\"\r\n\r\n    # Only 2 tests required!\r\n    check:\r\n      smoke: \"Endpoint responds 200\"\r\n      happy_path: \"Payment processes\"\r\n      # No other tests required\r\n\r\n    quality_target: 6.0  # Lower bar\r\n    time_estimate: 4  # Hours, not days\r\n```\r\n\r\n## Migration Helper\r\n\r\n### Converting Old Tasks\r\n```python\r\ndef migrate_task_to_v3(old_task, current_phase):\r\n    \"\"\"Convert 8-test task to phase-appropriate\"\"\"\r\n\r\n    new_task = old_task.copy()\r\n\r\n    if current_phase == 'prototype':\r\n        # Keep only critical tests\r\n        new_task['check'] = {\r\n            'smoke': old_task['check'].get('valid'),\r\n            'happy_path': old_task['check'].get('functional')\r\n        }\r\n    elif current_phase == 'mvp':\r\n        # Keep 4 most important\r\n        new_task['check'] = {\r\n            'smoke': old_task['check'].get('valid'),\r\n            'happy_path': old_task['check'].get('functional'),\r\n            'critical_errors': old_task['check'].get('error'),\r\n            'auth': old_task['check'].get('security')\r\n        }\r\n\r\n    return new_task\r\n```\r\n\r\n## Key Benefits\r\n\r\n1. **Faster Planning**: Less time defining 8 tests\r\n2. **Realistic Goals**: Phase-appropriate requirements\r\n3. **Escape Hatches**: Can override when needed\r\n4. **ROI Focus**: High-value tasks first\r\n5. **Progressive Enhancement**: Grow complexity gradually\r\n\r\n## Summary\r\n\r\nWrite-Plan V3 creates plans that:\r\n- Match project maturity\r\n- Require only necessary tests\r\n- Include escape hatches\r\n- Focus on shipping value\r\n- Can be adjusted on the fly\r\n\r\nRemember: **A shipped feature with 2 tests beats a planned feature with 8 tests.**",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "Brownbull",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/16549632?v=4",
      "lastCommit": "2025-11-10T00:22:56Z",
      "createdAt": "2025-11-07T00:56:34Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:28.688Z"
  },
  {
    "id": "narwhals",
    "source": {
      "type": "github-search",
      "repo": "anam-org/metaxy",
      "path": ".claude/skills/narwhals/SKILL.md",
      "url": "https://github.com/anam-org/metaxy/blob/main/.claude/skills/narwhals/SKILL.md"
    },
    "frontmatter": {
      "name": "narwhals",
      "description": "Effectively use Narwhals to write dataframe-agnostic code that works seamlessly across multiple Python dataframe libraries. Write correct type annotations for code using Narwhals."
    },
    "content": "# Narwhals - DataFrame Agnostic API\n\nNarwhals is a lightweight, zero-dependency compatibility layer for dataframe libraries in Python that provides a unified interface across different backends.\n\nDocs: https://narwhals-dev.github.io/narwhals/\n\n## What is Narwhals?\n\nNarwhals enables writing dataframe-agnostic code that works seamlessly across multiple Python dataframe libraries:\n\n**Full API Support:**\n\n- cuDF\n- Modin\n- pandas\n- Polars\n- PyArrow\n\n**Lazy-Only Support:**\n\n- Dask\n- DuckDB\n- Ibis\n- PySpark\n- SQLFrame\n\n## Core Philosophy\n\n**Why Narwhals?**\n\n- Resolves subtle differences between libraries (e.g., pandas checking index vs Polars checking values)\n- Provides unified, simple, and predictable API\n- Handles backwards compatibility internally\n- Tests against nightly builds of supported libraries\n- Maintains negligible performance overhead\n- Full static typing support\n- Zero dependencies\n\n**Target Use Case:**\nAnyone building libraries, applications, or services that consume dataframes and need complete backend independence.\n\n## Key Features\n\n1. **Backend Agnostic**: Write once, run on any supported dataframe library\n2. **Polars-Like API**: Uses a subset of the Polars API for consistency\n3. **Lazy & Eager Execution**: Separate APIs for both execution modes\n4. **Expression Support**: Full expression API for complex operations\n5. **Type Safety**: Perfect static typing support\n6. **100% Branch Coverage**: Thoroughly tested\n\n## Basic Usage Pattern\n\n### Three-Step Workflow\n\n```python\nimport narwhals as nw\n\n# 1. Convert to Narwhals\ndf_nw = nw.from_native(df)  # Works with pandas, Polars, PyArrow, etc.\n\n# 2. Perform operations using Polars-like API\nresult = df_nw.select(\n    a_sum=nw.col(\"a\").sum(), a_mean=nw.col(\"a\").mean(), b_std=nw.col(\"b\").std()\n)\n\n# 3. Convert back to original library\nresult_native = result.to_native()\n```\n\n### Using the @narwhalify Decorator\n\nSimplifies function definitions for automatic conversion:\n\n```python\n@nw.narwhalify\ndef my_func(df: IntoDataFrameT):\n    return df.select(nw.col(\"a\").sum(), nw.col(\"b\").mean()).filter(nw.col(\"a\") > 0)\n\n\n# Automatically handles conversion to/from Narwhals\nresult = my_func(pandas_df)  # Works!\nresult = my_func(polars_df)  # Also works!\n```\n\n## Top-Level Functions\n\n### Conversion Functions\n\n- `from_native(df, ...)`: Convert native DataFrame/Series to Narwhals object\n  - Parameters: `pass_through`, `backend`, `eager_only`, `allow_series`\n- `to_native(nw_obj)`: Convert Narwhals object back to native library type\n- `narwhalify()`: Decorator for automatic dataframe-agnostic functions\n\n### Data Creation\n\n- `new_series(name, values, dtype)`: Create a new Series\n- `from_dict(data)`: Create DataFrame from dictionary\n- `from_dicts(data)`: Create DataFrame from sequence of dictionaries\n\n### File I/O\n\n**Eager Loading:**\n\n- `read_csv(source, **kwargs)`: Read CSV file into DataFrame\n- `read_parquet(source, **kwargs)`: Read Parquet file into DataFrame\n\n**Lazy Loading:**\n\n- `scan_csv(source, **kwargs)`: Lazily scan CSV file\n- `scan_parquet(source, **kwargs)`: Lazily scan Parquet file\n\n### Aggregation Functions\n\n- `sum()`, `mean()`, `min()`, `max()`, `median()`\n- `sum_horizontal()`, `mean_horizontal()`, etc.\n\n### Expression Creation\n\n- `col(name)`: Reference column by name\n- `lit(value)`: Create literal expression\n- `when(condition)`: Create conditional expression\n- `format(template, *args)`: Format expression as string\n\n### Utilities\n\n- `generate_temporary_column_name()`: Generate unique column names\n- `get_native_namespace(obj)`: Get the native library of an object\n- `show_versions()`: Print debugging information\n\n## DataFrame Methods\n\n### Properties\n\n- `columns`: List of column names\n- `schema`: Ordered mapping of column names to dtypes\n- `shape`: Tuple of (rows, columns)\n- `implementation`: Name of native implementation\n\n### Column Operations\n\n- `select(*exprs)`: Select columns using expressions\n- `with_columns(*exprs)`: Add or modify columns\n- `drop(*columns)`: Remove specified columns\n- `rename(mapping)`: Rename columns\n\n### Row Operations\n\n- `filter(predicate)`: Filter rows based on conditions\n- `head(n)`: Get first n rows\n- `tail(n)`: Get last n rows\n- `sample(n)`: Randomly sample n rows\n- `drop_nulls()`: Drop rows with null values\n- `unique()`: Remove duplicate rows\n\n### Inspection\n\n- `is_empty()`: Check if DataFrame has no rows\n- `is_duplicated()`: Identify duplicated rows\n- `is_unique()`: Identify unique rows\n- `null_count()`: Count null values per column\n- `estimated_size()`: Estimate memory usage\n\n### Transformations\n\n- `sort(*by)`: Sort by one or more columns\n- `group_by(*by)`: Group by columns for aggregation\n- `join(other, on, how)`: Perform SQL-style joins\n- `pivot(on, index, values)`: Create pivot table\n- `explode(*columns)`: Expand list columns to long format\n- `lazy()`: Convert to LazyFrame\n\n### Export\n\n- `to_native()`: Convert to original library type\n- `to_numpy()`: Convert to NumPy array\n- `to_pandas()`: Convert to pandas DataFrame\n- `to_polars()`: Convert to Polars DataFrame\n- `clone()`: Create a copy\n\n## LazyFrame Methods\n\nLazyFrame provides the same API as DataFrame but with lazy evaluation:\n\n### Key Differences\n\n- Operations build an execution plan without computing\n- `collect()`: Materialize the LazyFrame into a DataFrame\n- `collect_schema()`: Get schema without collecting data\n- `sink_parquet(path)`: Write results directly to Parquet\n\n### Common Methods\n\nAll DataFrame methods are available on LazyFrame:\n\n- `select()`, `filter()`, `with_columns()`, `drop()`\n- `group_by()`, `join()`, `sort()`, `unique()`\n- `head()`, `tail()`, `top_k()`\n- `gather_every()`: Select rows at regular intervals\n- `unpivot()`: Convert from wide to long format\n- `with_row_index()`: Add row index column\n- `pipe()`: Apply function to LazyFrame\n\n## Expression (Expr) API\n\nExpressions are the building blocks for column operations.\n\n### Creation\n\n```python\nnw.col(\"column_name\")  # Reference column\nnw.lit(42)  # Literal value\n```\n\n### Filtering\n\n- `filter(predicate)`: Filter elements\n- `is_in(values)`: Check membership\n- `is_between(lower, upper)`: Check range\n- `drop_nulls()`: Remove nulls\n\n### Aggregations\n\n- `count()`: Count non-null elements\n- `null_count()`: Count null values\n- `n_unique()`: Count unique values\n- `sum()`, `mean()`, `median()`: Statistical aggregations\n- `min()`, `max()`: Extremes\n- `std()`, `var()`: Spread measures\n- `quantile(q)`: Quantile values\n\n### Transformations\n\n**Mathematical:**\n\n- `abs()`: Absolute value\n- `round()`, `floor()`, `ceil()`: Rounding\n- `sqrt()`, `log()`, `exp()`: Mathematical functions\n\n**Type/Value Operations:**\n\n- `cast(dtype)`: Change data type\n- `fill_null(value)`: Replace null values\n- `replace_strict(old, new)`: Replace specific values\n\n**Window Operations:**\n\n- `rolling_mean(window_size)`: Moving average\n- `rolling_sum(window_size)`: Moving sum\n- `rolling_std(window_size)`: Moving standard deviation\n- `shift(n)`: Shift values by n positions\n- `over(*by)`: Compute expression over groups\n\n**Ranking/Uniqueness:**\n\n- `rank()`: Assign ranks\n- `unique()`: Get unique values\n- `is_duplicated()`: Identify duplicates\n- `is_first_distinct()`: Mark first distinct occurrences\n\n### Namespace Methods\n\nExpressions have specialized namespaces for specific data types:\n\n**String Operations (`Expr.str`)**\n\n- String manipulation methods\n\n**DateTime Operations (`Expr.dt`)**\n\n- Date/time manipulation methods\n\n**List Operations (`Expr.list`)**\n\n- List column operations\n\n**Categorical Operations (`Expr.cat`)**\n\n- Categorical data methods\n\n**Struct Operations (`Expr.struct`)**\n\n- Struct/nested data methods\n\n**Name Operations (`Expr.name`)**\n\n- Column name operations\n\n## Series API\n\nSeries represents a single column:\n\n### Properties\n\n- Same as DataFrame: `shape`, `dtype`, `name`\n\n### Methods\n\n- Similar to DataFrame but for single column operations\n- Has specialized namespaces: `str`, `dt`, `list`, `cat`, `struct`\n\n## Type Hints\n\nFull docs: [narwhals.typing](https://narwhals-dev.github.io/narwhals/api-reference/typing/)\n\nTLDR:\n\nDataFrameT module-attribute\n\nDataFrameT = TypeVar('DataFrameT', bound='DataFrame[Any]')\nTypeVar bound to Narwhals DataFrame.\n\nUse this if your function can accept a Narwhals DataFrame and returns a Narwhals DataFrame backed by the same backend.\n\nExamples:\n\n```py\n>>> import narwhals as nw\n>>> from narwhals.typing import DataFrameT\n>>> @nw.narwhalify\n>>> def func(df: DataFrameT) -> DataFrameT:\n...     return df.with_columns(c=df[\"a\"] + 1)\nFrame module-attribute\n```\n\nFrame: TypeAlias = Union[\"DataFrame[Any]\", \"LazyFrame[Any]\"]\nNarwhals DataFrame or Narwhals LazyFrame.\n\nUse this if your function can work with either and your function doesn't care about its backend.\n\nExamples:\n\n```py\n>>> import narwhals as nw\n>>> from narwhals.typing import Frame\n>>> @nw.narwhalify\n... def agnostic_columns(df: Frame) -> list[str]:\n...     return df.columns\nFrameT module-attribute\n```\n\nFrameT = TypeVar(\n\"FrameT\", \"DataFrame[Any]\", \"LazyFrame[Any]\"\n)\nTypeVar bound to Narwhals DataFrame or Narwhals LazyFrame.\n\nUse this if your function accepts either nw.DataFrame or nw.LazyFrame and returns an object of the same kind.\n\nExamples:\n\n```py\n>>> import narwhals as nw\n>>> from narwhals.typing import FrameT\n>>> @nw.narwhalify\n... def agnostic_func(df: FrameT) -> FrameT:\n...     return df.with_columns(c=nw.col(\"a\") + 1)\n```\n\nIntoDataFrame module-attribute\n\nIntoDataFrame: TypeAlias = NativeDataFrame\nAnything which can be converted to a Narwhals DataFrame.\n\nUse this if your function accepts a narwhalifiable object but doesn't care about its backend.\n\nExamples:\n\n```py\n>>> import narwhals as nw\n>>> from narwhals.typing import IntoDataFrame\n>>> def agnostic_shape(df_native: IntoDataFrame) -> tuple[int, int]:\n...     df = nw.from_native(df_native, eager_only=True)\n...     return df.shape\n```\n\nIntoDataFrameT module-attribute\n\nIntoDataFrameT = TypeVar(\n\"IntoDataFrameT\", bound=IntoDataFrame\n)\nTypeVar bound to object convertible to Narwhals DataFrame.\n\nUse this if your function accepts an object which can be converted to nw.DataFrame and returns an object of the same class.\n\nExamples:\n\n```py\n>>> import narwhals as nw\n>>> from narwhals.typing import IntoDataFrameT\n>>> def agnostic_func(df_native: IntoDataFrameT) -> IntoDataFrameT:\n...     df = nw.from_native(df_native, eager_only=True)\n...     return df.with_columns(c=df[\"a\"] + 1).to_native()\n```\n\n## Common Patterns\n\n### Group By and Aggregate\n\n```python\nresult = df.group_by(\"category\").agg(\n    count=nw.col(\"id\").count(),\n    total=nw.col(\"amount\").sum(),\n    average=nw.col(\"amount\").mean(),\n)\n```\n\n### Conditional Operations\n\n```python\nresult = df.with_columns(\n    category=nw.when(nw.col(\"value\") > 100)\n    .then(nw.lit(\"high\"))\n    .otherwise(nw.lit(\"low\"))\n)\n```\n\n### Joins\n\n```python\nresult = df1.join(\n    df2,\n    on=\"key_column\",\n    how=\"left\",  # inner, left, outer, cross\n)\n```\n\n### Chain Operations\n\n```python\nresult = (\n    df.filter(nw.col(\"status\") == \"active\")\n    .select(\"user_id\", \"amount\")\n    .group_by(\"user_id\")\n    .agg(total=nw.col(\"amount\").sum())\n    .sort(\"total\", descending=True)\n    .head(10)\n)\n```\n\n## Best Practices\n\n1. **Use `@nw.narwhalify` for library functions**: Simplifies API and handles conversion automatically\n\n2. **Prefer expressions over method chaining**: More flexible and composable\n   ```python\n   # Good\n   df.select(nw.col(\"a\").sum(), nw.col(\"b\").mean())\n\n   # Also fine, but less composable\n   df.select(\"a\", \"b\")\n   ```\n\n3. **Use lazy evaluation when possible**: Better performance for complex pipelines\n   ```python\n   result = df.lazy().select(...).filter(...).collect()\n   ```\n\n4. **Always convert back to native**: Remember to call `.to_native()` when returning from library functions (unless using `@narwhalify`)\n\n5. **Type hint your functions**: Use `IntoDataFrame` and `FrameT` for better IDE support\n\n6. **Check supported backends**: Some operations may not be available on all backends\n\n## Important Constraints\n\n1. **Zero Dependencies**: Narwhals has no dependencies, keeping it lightweight\n\n2. **Polars API Subset**: Uses Polars-style API but may not support all Polars features\n\n3. **Backend Limitations**: Some backends (lazy-only) have restricted functionality\n\n## Tips\n\nChecking whether a Narwhals frame is a Polars frame:\n\n```py\nimport polars as pl\nimport narwhals as nw\n\ndf_native = pl.DataFrame({\"a\": [1, 2, 3]})\ndf = nw.from_native(df_native)\ndf.implementation.is_polars()\n```\n\nAsserting series equality:\n\n```py\nimport pandas as pd\nimport narwhals as nw\nfrom narwhals.testing import assert_series_equal\ns1 = nw.from_native(pd.Series([1, 2, 3]), series_only=True)\ns2 = nw.from_native(pd.Series([1, 5, 3]), series_only=True)\nassert_series_equal(s1, s2)\nTraceback (most recent call last):\n...\nAssertionError: Series are different (exact value mismatch)\n[left]:\n┌───────────────┐\n|Narwhals Series|\n|---------------|\n| 0    1        |\n| 1    2        |\n| 2    3        |\n| dtype: int64  |\n└───────────────┘\n[right]:\n┌───────────────┐\n|Narwhals Series|\n|---------------|\n| 0    1        |\n| 1    5        |\n| 2    3        |\n| dtype: int64  |\n└───────────────┘\n```\n",
    "github": {
      "stars": 18,
      "forks": 2,
      "owner": "anam-org",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/156597016?v=4",
      "license": "Apache-2.0",
      "lastCommit": "2026-01-03T08:09:56Z",
      "createdAt": "2025-10-13T07:45:16Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:29.417Z"
  },
  {
    "id": "niri",
    "source": {
      "type": "github-search",
      "repo": "tobi/dotnix",
      "path": ".claude/skills/niri/SKILL.md",
      "url": "https://github.com/tobi/dotnix/blob/main/.claude/skills/niri/SKILL.md"
    },
    "frontmatter": {
      "name": "Niri",
      "description": "Always include when making changes to niri"
    },
    "content": "We are using niri with flake settings. \n\nFull flake docs are at [./niri-flake-settings.md](./niri-flake-settings.md) or to get just the listing of valid settings read [./valid-settings.md](./valid-settings.md). \n",
    "github": {
      "stars": 40,
      "forks": 2,
      "owner": "tobi",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/347?v=4",
      "lastCommit": "2025-12-22T19:34:16Z",
      "createdAt": "2025-06-08T17:21:30Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:30.027Z"
  },
  {
    "id": "select-districts",
    "source": {
      "type": "github-search",
      "repo": "pauljbernard/datamig",
      "path": ".claude/skills/select-districts/skill.md",
      "url": "https://github.com/pauljbernard/datamig/blob/main/.claude/skills/select-districts/skill.md"
    },
    "frontmatter": {},
    "content": "# Select Districts Skill\n\nYou are a district selection specialist operating autonomously. Your mission: Identify and prioritize districts for migration based on size, activity, and business importance.\n\n## Autonomous Execution Plan\n\nExecute the following workflow without human intervention:\n\n### 1. Query District Metadata from PROD\n\nConnect to all PROD data stores and gather district information:\n\n#### Query IDS (Primary District Data)\n\n```sql\n-- Get all districts with basic metrics\nSELECT\n  d.id,\n  d.name,\n  d.state,\n  d.created_at,\n  d.updated_at,\n  COUNT(DISTINCT s.id) as school_count,\n  COUNT(DISTINCT st.id) as student_count,\n  COUNT(DISTINCT staff.id) as staff_count\nFROM districts d\nLEFT JOIN schools s ON s.district_id = d.id\nLEFT JOIN students st ON st.district_id = d.id\nLEFT JOIN staff ON staff.district_id = d.id\nGROUP BY d.id, d.name, d.state, d.created_at, d.updated_at\nORDER BY student_count DESC;\n```\n\n#### Query HCP1, HCP2, ADB for Record Counts\n\nFor each data store, count records associated with each district:\n\n```sql\n-- Example for HCP1\nSELECT\n  district_id,\n  COUNT(*) as record_count,\n  SUM(pg_column_size(t.*)) as estimated_size_bytes\nFROM <table> t\nGROUP BY district_id;\n```\n\nAggregate across all tables in each store to get total footprint.\n\n#### Query Neo4j (SP) for Graph Metrics\n\n```cypher\n// Get district metrics from graph\nMATCH (d:District)\nOPTIONAL MATCH (d)<-[:BELONGS_TO]-(s:School)\nOPTIONAL MATCH (s)<-[:ENROLLED_IN]-(student:Student)\nOPTIONAL MATCH (d)<-[:WORKS_FOR]-(staff:Staff)\nRETURN\n  d.id as district_id,\n  d.name as district_name,\n  count(DISTINCT s) as school_count,\n  count(DISTINCT student) as student_count,\n  count(DISTINCT staff) as staff_count,\n  count(DISTINCT *) as total_nodes\nORDER BY total_nodes DESC;\n```\n\n### 2. Calculate District Footprint\n\nFor each district, calculate total data footprint across all 5 stores:\n\n```python\ndistrict_footprint = {\n    'district_id': 'district-001',\n    'name': 'Large Urban District',\n    'metrics': {\n        'students': 75000,\n        'staff': 8000,\n        'schools': 150,\n        'total_records': 0  # Sum below\n    },\n    'footprint_by_store': {\n        'ids': {\n            'tables': 45,\n            'records': 250000,\n            'estimated_size_mb': 1200\n        },\n        'hcp1': {\n            'tables': 32,\n            'records': 180000,\n            'estimated_size_mb': 850\n        },\n        'hcp2': {\n            'tables': 28,\n            'records': 120000,\n            'estimated_size_mb': 600\n        },\n        'adb': {\n            'tables': 52,\n            'records': 200000,\n            'estimated_size_mb': 1500\n        },\n        'sp': {\n            'nodes': 100000,\n            'relationships': 250000,\n            'estimated_size_mb': 400\n        }\n    },\n    'total_records': 850000,\n    'total_size_mb': 4550,\n    'estimated_migration_hours': 4.2\n}\n```\n\n### 3. Calculate Activity Scores\n\nMeasure recent activity to prioritize active districts:\n\n```sql\n-- Count recent updates (last 30 days)\nSELECT\n  district_id,\n  COUNT(*) as recent_updates\nFROM (\n  SELECT district_id, updated_at FROM students WHERE updated_at > NOW() - INTERVAL '30 days'\n  UNION ALL\n  SELECT district_id, updated_at FROM staff WHERE updated_at > NOW() - INTERVAL '30 days'\n  UNION ALL\n  SELECT district_id, updated_at FROM enrollments WHERE updated_at > NOW() - INTERVAL '30 days'\n) recent\nGROUP BY district_id\nORDER BY recent_updates DESC;\n```\n\n### 4. Apply Ranking Algorithm\n\nRank districts using weighted scoring:\n\n```python\ndef calculate_priority_score(district):\n    \"\"\"\n    Calculate priority score for district migration.\n\n    Factors:\n    - Size (40%): Larger districts = higher priority\n    - Activity (30%): More active = higher priority\n    - Completeness (20%): More complete data = higher priority\n    - Business Priority (10%): Manual override if provided\n    \"\"\"\n    # Normalize to 0-100 scale\n    size_score = min(100, (district['total_records'] / 1000000) * 100)\n    activity_score = min(100, (district['recent_updates'] / 10000) * 100)\n    completeness_score = (district['data_completeness_pct'])\n    business_score = district.get('business_priority', 50)\n\n    weighted_score = (\n        size_score * 0.40 +\n        activity_score * 0.30 +\n        completeness_score * 0.20 +\n        business_score * 0.10\n    )\n\n    return weighted_score\n```\n\nSort districts by priority score descending.\n\n### 5. Estimate Migration Time\n\nFor each district, estimate migration duration:\n\n```python\ndef estimate_migration_time(total_records):\n    \"\"\"\n    Estimate migration time based on historical performance.\n\n    Assumptions:\n    - Extraction: 50,000 records/minute\n    - Anonymization: 100,000 records/minute\n    - Validation: 200,000 records/minute\n    - Loading: 30,000 records/minute\n\n    Bottleneck: Loading (slowest step)\n    \"\"\"\n    extraction_min = total_records / 50000\n    anonymization_min = total_records / 100000\n    validation_min = total_records / 200000\n    loading_min = total_records / 30000\n\n    # Add overhead (10% for setup, monitoring, reporting)\n    total_min = (extraction_min + anonymization_min + validation_min + loading_min) * 1.1\n\n    return round(total_min / 60, 1)  # Convert to hours\n```\n\n### 6. Generate District Manifest\n\nCreate `data/manifests/district-manifest.json`:\n\n```json\n{\n  \"generated_at\": \"2025-11-06T12:00:00Z\",\n  \"total_districts_analyzed\": 1247,\n  \"recommended_districts\": 15,\n  \"selection_criteria\": {\n    \"min_students\": 5000,\n    \"min_schools\": 10,\n    \"min_total_records\": 50000,\n    \"max_total_records\": 2000000\n  },\n  \"districts\": [\n    {\n      \"id\": \"district-001\",\n      \"name\": \"Large Urban District A\",\n      \"priority\": 1,\n      \"priority_score\": 92.5,\n      \"state\": \"CA\",\n      \"metrics\": {\n        \"students\": 75000,\n        \"staff\": 8000,\n        \"schools\": 150,\n        \"total_records\": 850000,\n        \"recent_updates_30d\": 45000,\n        \"data_completeness_pct\": 95\n      },\n      \"footprint_by_store\": {\n        \"ids\": 250000,\n        \"hcp1\": 180000,\n        \"hcp2\": 120000,\n        \"adb\": 200000,\n        \"sp\": 100000\n      },\n      \"estimated_migration_hours\": 4.2,\n      \"recommended_for_pilot\": true,\n      \"notes\": \"Large, active district with complete data. Ideal for pilot.\"\n    },\n    {\n      \"id\": \"district-002\",\n      \"name\": \"Mid-Size Suburban District B\",\n      \"priority\": 2,\n      \"priority_score\": 87.3,\n      \"state\": \"TX\",\n      \"metrics\": {\n        \"students\": 45000,\n        \"staff\": 5000,\n        \"schools\": 85,\n        \"total_records\": 520000,\n        \"recent_updates_30d\": 28000,\n        \"data_completeness_pct\": 92\n      },\n      \"footprint_by_store\": {\n        \"ids\": 150000,\n        \"hcp1\": 110000,\n        \"hcp2\": 75000,\n        \"adb\": 125000,\n        \"sp\": 60000\n      },\n      \"estimated_migration_hours\": 2.8,\n      \"recommended_for_pilot\": true,\n      \"notes\": \"Good mid-size representation.\"\n    }\n  ],\n  \"summary\": {\n    \"total_students\": 675000,\n    \"total_staff\": 72000,\n    \"total_schools\": 1340,\n    \"total_records\": 8500000,\n    \"estimated_total_migration_hours\": 48,\n    \"districts_by_size\": {\n      \"large\": 5,\n      \"medium\": 7,\n      \"small\": 3\n    },\n    \"pilot_recommended\": [\n      \"district-001\",\n      \"district-002\",\n      \"district-005\"\n    ]\n  }\n}\n```\n\n### 7. Generate Human-Readable Report\n\nCreate `data/manifests/district-selection-report.md`:\n\n```markdown\n# District Selection Report\n\n**Generated**: 2025-11-06 12:00:00\n**Total Districts Analyzed**: 1,247\n**Recommended for Migration**: 15\n\n## Selection Criteria\n\n- Minimum 5,000 students\n- Minimum 10 schools\n- Minimum 50,000 total records\n- Maximum 2,000,000 records (to avoid overly long migrations)\n- Active within last 30 days\n- Data completeness > 85%\n\n## Top 15 Districts\n\n| Priority | District ID | Name | State | Students | Records | Est. Hours |\n|----------|-------------|------|-------|----------|---------|------------|\n| 1 | district-001 | Large Urban District A | CA | 75,000 | 850,000 | 4.2 |\n| 2 | district-002 | Mid-Size Suburban District B | TX | 45,000 | 520,000 | 2.8 |\n| 3 | district-003 | ... | ... | ... | ... | ... |\n\n## Pilot Recommendations\n\nFor initial pilot testing, we recommend starting with these 3 districts:\n\n### 1. District-001: Large Urban District A\n- **Why**: Largest, most complex, most active\n- **Size**: 850K records\n- **Risk**: Medium (large but well-structured)\n- **Value**: High confidence if this succeeds\n\n### 2. District-002: Mid-Size Suburban District B\n- **Why**: Representative mid-size district\n- **Size**: 520K records\n- **Risk**: Low (typical structure)\n- **Value**: Validates process works for majority\n\n### 3. District-005: Small Rural District\n- **Why**: Smallest recommended size\n- **Size**: 180K records\n- **Risk**: Low (simple structure)\n- **Value**: Fast validation, edge case testing\n\n## Migration Schedule Estimate\n\n- **Pilot Phase (3 districts)**: ~12 hours runtime\n- **Full Migration (15 districts)**: ~48 hours total runtime\n- **Recommended approach**: Run 2-3 migrations per day\n- **Total calendar time**: 1 week (with monitoring and validation)\n\n## Coverage Analysis\n\nThese 15 districts represent:\n- **54%** of total PROD students\n- **48%** of total PROD staff\n- **51%** of total PROD schools\n- **47%** of total PROD data volume\n\nThis provides excellent coverage for realistic CERT testing while being achievable within timeline.\n\n## Next Steps\n\n1. Review and approve district list\n2. Notify stakeholders for selected districts\n3. Schedule migration windows\n4. Begin with pilot districts: `/migrate district-001`\n```\n\n### 8. Report Completion\n\nGenerate summary and report back to human:\n\n```\n✓ District Selection Complete\n\nSummary:\n- Analyzed 1,247 districts across PROD\n- Applied selection criteria (size, activity, completeness)\n- Ranked districts by priority score\n- Recommended top 15 districts for migration\n- Identified 3 districts for pilot phase\n\nSelected Districts:\n- Total Students: 675,000 (54% of PROD)\n- Total Records: 8.5M (47% of PROD)\n- Estimated Migration Time: 48 hours\n\nPilot Recommendations:\n1. district-001 (Large Urban, 850K records, 4.2 hours)\n2. district-002 (Mid-Size Suburban, 520K records, 2.8 hours)\n3. district-005 (Small Rural, 180K records, 1.5 hours)\n\nArtifacts generated:\n- data/manifests/district-manifest.json\n- data/manifests/district-selection-report.md\n\nNext steps:\n1. Review selected districts\n2. Approve for migration\n3. Start pilot: /migrate district-001\n```\n\n## Tools Available\n\n- **MCP Servers**: All PROD database connections (postgres-ids-prod, postgres-hcp1-prod, postgres-hcp2-prod, postgres-adb-prod, neo4j-sp-prod)\n- **Python Script**: `scripts/district-analyzer.py` (for complex aggregations)\n- **Write Tool**: To create JSON and Markdown files\n\n## Success Criteria\n\n- ✓ All PROD data stores queried successfully\n- ✓ District metadata collected for all districts\n- ✓ Footprint calculated across all 5 stores\n- ✓ Priority ranking algorithm applied\n- ✓ Top 15-20 districts identified\n- ✓ Pilot recommendations provided\n- ✓ Manifest and report generated\n\n## Error Handling\n\nIf you encounter errors:\n- **Connection failures**: Try alternative stores, report incomplete data\n- **Missing data**: Note gaps, continue with available data\n- **Permission errors**: Log tables you cannot access\n- **Calculation errors**: Use estimates, document assumptions\n\nDo not fail the entire selection due to partial data issues. Generate the best recommendations possible with available data and document any limitations.\n\nExecute this entire workflow autonomously. Only report back when complete or if critical errors prevent ANY district selection.\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "pauljbernard",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/324344?v=4",
      "lastCommit": "2025-11-06T19:39:36Z",
      "createdAt": "2025-11-06T11:54:27Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:30.806Z"
  },
  {
    "id": "sync-gruvbox-theme",
    "source": {
      "type": "github-search",
      "repo": "bfowle/dotfiles",
      "path": ".claude/skills/sync-gruvbox-theme/Skill.md",
      "url": "https://github.com/bfowle/dotfiles/blob/main/.claude/skills/sync-gruvbox-theme/Skill.md"
    },
    "frontmatter": {
      "name": "sync-gruvbox-theme",
      "description": "Verify and sync gruvbox theme consistency across vim, neovim, tmux, and terminal"
    },
    "content": "# Sync Gruvbox Theme\n\nThis skill ensures the gruvbox theme is consistently configured across all tools.\n\n## Task\n\nYou should:\n\n1. Check gruvbox configuration in:\n   - Neovim (`nvim/lua/plugins/gruvbox.lua`)\n   - Vim (`vim/vimrc.ln`)\n   - Tmux (`tmux/tmux.conf.ln`)\n   - Bash/terminal settings (`bash/bashrc.ln`)\n\n2. Verify all use the same variant (dark/light)\n\n3. Check color settings:\n   - 256 color support\n   - True color (RGB) support\n   - Terminal overrides\n\n4. Ensure consistent appearance of:\n   - Background colors\n   - Foreground text\n   - Syntax highlighting\n   - Status lines\n   - Borders/separators\n\n5. Suggest improvements for better consistency\n\n## Files to Check\n\n### Neovim\n- `nvim/lua/plugins/gruvbox.lua` - should use ellisonleao/gruvbox.nvim\n- Check contrast setting (hard/soft/default)\n- Verify colorscheme is set\n\n### Vim\n- `vim/vimrc.ln` - should have gruvbox colorscheme\n- Background setting (dark/light)\n\n### Tmux\n- `tmux/tmux.conf.ln` - should use egel/tmux-gruvbox plugin\n- Verify @tmux-gruvbox setting matches vim/neovim\n- Check terminal-overrides for true color\n\n### Bash\n- `bash/bashrc.ln` - check TERM variable\n- Verify LSCOLORS or LS_COLORS use gruvbox palette\n\n## Validation\n\nTest by:\n1. Opening neovim and checking `:colorscheme`\n2. Running tmux and verifying status bar colors\n3. Listing files with `ls` and checking colors\n4. Opening vim and verifying theme\n\n## Common Issues\n\n- Terminal doesn't support 256 colors\n- True color not enabled in tmux\n- Mismatched dark/light variants\n- Missing gruvbox plugins\n- Wrong TERM environment variable\n\n## Output\n\nReport:\n- Current theme variant in each tool\n- Any inconsistencies found\n- Color support status (256/true color)\n- Recommended fixes\n- Commands to apply fixes\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "bfowle",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/240658?v=4",
      "lastCommit": "2025-10-31T15:31:06Z",
      "createdAt": "2013-02-06T00:11:17Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:31.599Z"
  },
  {
    "id": "content-discovery",
    "source": {
      "type": "github-search",
      "repo": "longkeyy/claude-discover",
      "path": ".claude/skills/content-discovery/skill.md",
      "url": "https://github.com/longkeyy/claude-discover/blob/main/.claude/skills/content-discovery/skill.md"
    },
    "frontmatter": {
      "name": "content-discovery",
      "description": "|",
      "Use this skill when": ""
    },
    "content": "# Content Discovery Skill\n\n自动化内容发现工作流：扫描数据源 → 智能分析 → 发布内容 → 优化关键词\n\n## 输入配置\n\n每个发现任务需要以下配置：\n\n1. **任务ID**: {task_id}\n2. **任务配置文件**: config/tasks/{task_id}.md\n3. **关键词文件**: config/keywords/{task_id}.json\n4. **会话目录**: temp/sessions/{task_id}/{session_id}/\n5. **输出目录**: posts/{task_id}/\n\n## 工作流程（5个阶段）\n\n### 阶段 1: 数据源检测和选择\n\n1. **读取任务配置** (config/tasks/{task_id}.md)\n   - 提取已启用的数据源\n   - 读取过滤规则 (YAML 格式)\n   - 读取发布配置\n\n2. **检测可用工具**\n   - 优先检测 MCP 服务 (mcp__hf-mcp-server__*)\n   - Fallback 到 WebSearch\n   - 记录选择的工具到会话日志\n\n3. **读取关键词** (config/keywords/{task_id}.json)\n   - 提取所有类别的关键词\n   - 用于构建搜索查询\n\n### 阶段 2: 内容搜索和过滤\n\n1. **执行搜索**\n   - 使用选定的工具 (MCP 或 WebSearch)\n   - 应用关键词构建查询\n\n2. **应用过滤规则**\n   - 根据task_type应用相应过滤规则\n   - GitHub: minimum_stars, has_readme, time_range\n   - ArXiv: time_range, categories, author_email_domains\n   - HuggingFace: min_downloads, min_likes, tags\n\n3. **AI 语义去重**\n   - 列出已发布文件: posts/{task_id}/*.json\n   - 提取文件名中的语义信息\n   - 使用 AI 判断新内容与已有内容的语义相似度\n   - 如果相似度 > 90%，判定为重复，跳过\n\n### 阶段 3: 内容分析和评估\n\n1. **提取元数据**\n   - 标题、作者/创建者、发布日期、摘要等\n   - 相关 URL（github_url, official_url, demo_url）\n\n2. **数据来源追踪（重要）**\n   必须添加 `metadata` 字段用于数据溯源和二次更新：\n   ```json\n   {\n     \"metadata\": {\n       \"source_url\": \"<数据采集的原始URL>\",\n       \"source_type\": \"<自动检测: github/huggingface/arxiv/web>\",\n       \"collected_at\": \"<当前时间 ISO 8601>\",\n       \"updated_at\": \"<当前时间 ISO 8601>\",\n       \"task_id\": \"<当前任务ID>\",\n       \"source_details\": {\n         // 根据 source_type 添加特定信息\n         // GitHub: {\"repo\": \"...\", \"branch\": \"main\"}\n         // HuggingFace: {\"repo_type\": \"model\", \"repo_id\": \"...\"}\n         // ArXiv: {\"arxiv_id\": \"...\", \"version\": \"v1\"}\n       }\n     }\n   }\n   ```\n\n   **自动类型检测规则**：\n   - URL 包含 `github.com` → source_type: \"github\"\n   - URL 包含 `huggingface.co` → source_type: \"huggingface\"\n   - URL 包含 `arxiv.org` → source_type: \"arxiv\"\n   - 其他 → source_type: \"web\"\n\n3. **生成内容摘要与观点提炼**\n   - **风格要求**: 高质量公众号/小红书/Twitter文章风格\n   - **信息密度**: 高密度，无废话，直击要点\n   - **内容结构**:\n     * 核心观点：提炼1-2句精华观点（独特见解）\n     * 技术亮点：3-5个关键技术点（带数据支撑）\n     * 实用价值：实际应用场景和价值分析\n   - **写作原则**:\n     * 有态度：明确的技术判断和评价\n     * 有深度：不仅介绍\"是什么\"，更分析\"为什么\"和\"意味着什么\"\n     * 有对比：与同类方案对比，突出差异和优势\n     * 有洞察：提供超越论文本身的思考\n   - **语言风格**: 专业但不枯燥，准确但不晦涩\n   - **长度控制**: 800-1500字（不是字符），信息密度优先\n   - **必须包含**: 原文链接（arXiv/GitHub/官网）\n   - **重要**: 所有内容必须为中文\n\n4. **质量标准检查**（根据任务类型）\n\n   **Foundation Models**:\n   - ✅ 提取技术报告链接（优先 arxiv.org）\n   - ✅ 确保 release_date 使用实际发布日期（非采集日期）\n   - ✅ 内容长度 ≥ 1500 字符\n   - ✅ 全部中文翻译\n   - ✅ 保留 source_url\n\n   **MCP Servers**:\n   - ✅ 提取所有工具列表（tools_resources.tools）\n   - ✅ 每个工具必须包含: name, description, parameters\n   - ✅ **如果无法获取工具文档，直接跳过该内容**（不发布）\n   - ✅ 提供配置示例（config_example）\n   - ✅ 内容长度 ≥ 1500 字符\n   - ✅ 全部中文翻译\n\n   **Prompt Papers**:\n   - ✅ 确保包含 arxiv_url\n   - ✅ 内容长度 ≥ 1000 字符\n   - ✅ 全部中文翻译\n\n5. **评分与过滤**\n   - 根据配置的 scoring_weights 评估\n   - 过滤低于 minimum_score 的内容\n   - **如果不符合质量标准，直接跳过**\n\n**注意**：详见 `docs/DATA_SOURCE_TRACKING.md` 和 `docs/CONTENT_QUALITY_STANDARDS.md`\n\n### 阶段 3.5: 图片提取和下载（AI Agent 主导）\n\n**由 AI Agent 直接处理，无需独立脚本**\n\n1. **图片发现（AI 分析）**\n   - 使用 WebFetch/Read 工具获取内容页面（GitHub README、HF Model Card）\n   - AI 分析页面内容，提取所有图片 URL\n   - AI 智能识别图片类型：\n     * Cover: 宽高比 16:9，尺寸较大，位于顶部，包含 \"cover/banner/hero\" 关键词\n     * Screenshot: 包含 \"screenshot/demo/preview\" 关键词或上下文\n     * Diagram: 包含 \"arch/diagram/flow/structure\" 关键词\n     * Badge/Icon: 自动排除（尺寸 < 50x50px 或包含 badge/shield 关键词）\n\n2. **智能选择（AI 决策）**\n   - Cover: 选择 1 张最合适的（优先 16:9，尺寸 ≥ 800px）\n   - Screenshots: 最多 3 张最重要的\n   - Diagrams: 最多 3 张架构图/流程图\n   - AI 评估图片的内容价值和相关性\n\n3. **下载到本地（Bash 工具）**\n   ```bash\n   # Agent 调用 Bash 工具\n   source .claude-plugin/scripts/discover/image_utils.sh\n\n   # 创建目录\n   IMAGE_DIR=$(create_image_dir \"{task_id}\" \"{slug}\")\n\n   # 下载封面\n   download_image \"https://example.com/cover.png\" \\\n                  \"$IMAGE_DIR/cover.png\"\n\n   # 下载截图\n   download_image \"https://example.com/screenshot-1.png\" \\\n                  \"$IMAGE_DIR/screenshot-1.png\"\n\n   # 下载架构图\n   download_image \"https://example.com/architecture.svg\" \\\n                  \"$IMAGE_DIR/diagram-1.svg\"\n\n   # 获取图片尺寸\n   SIZE=$(get_image_size \"$IMAGE_DIR/cover.png\")\n   ```\n\n4. **更新 JSON 数据（AI Agent）**\n   - Agent 使用 Write 工具更新 JSON 文件\n   - 添加完整的图片信息：\n   ```json\n   {\n     \"images\": {\n       \"cover\": {\n         \"original_url\": \"https://...\",\n         \"local_path\": \"images/{task_id}/{slug}/cover.png\",\n         \"alt\": \"AI生成的描述性文本\",\n         \"width\": 1200,\n         \"height\": 630,\n         \"downloaded\": true\n       },\n       \"screenshots\": [\n         {\n           \"original_url\": \"https://...\",\n           \"local_path\": \"images/{task_id}/{slug}/screenshot-1.png\",\n           \"alt\": \"...\",\n           \"caption\": \"AI生成的说明\",\n           \"downloaded\": true\n         }\n       ],\n       \"diagrams\": [...]\n     },\n     \"featured_image\": \"images/{task_id}/{slug}/cover.png\"\n   }\n   ```\n\n5. **错误处理**\n   - 下载失败: 标记 `downloaded: false`，保留原始 URL\n   - 超大文件（>5MB）: curl 自动拒绝\n   - 网络超时: Agent 可选择重试\n   - 图片无效: 使用 validate_image 检查，清理失败文件\n\n**工具集**:\n- `.claude-plugin/scripts/discover/image_utils.sh` - Bash 辅助函数\n- Agent 的 WebFetch/Read - 页面内容获取\n- Agent 的 Bash - 执行下载命令\n- Agent 的 Write - 更新 JSON\n\n**注意**: 详见 `docs/IMAGE_MANAGEMENT.md`\n\n### 阶段 4: 发布\n\n1. **保存原始数据**\n   - 保存为 JSON 格式到 posts/{task_id}/\n   - 文件命名: {date}_{topic}_{title-slug}.json\n   - **必须包含 metadata 字段**（用于数据追溯和二次更新）\n   - 确保 metadata.source_url 指向原始数据页面\n\n2. **读取发布配置**\n   - 从任务配置文件读取 hexo, telegram, discord 配置\n   - 检查 enabled 字段\n\n3. **发布到启用的渠道**\n\n   **Hexo博客** (如果 hexo.enabled: true):\n   - 读取 hexo.template 模板\n   - 使用AI将JSON数据转换为高质量markdown\n   - 理解并处理模板中的条件逻辑 ({{#if}})\n   - 填充所有变量 ({{variable}})\n   - **处理图片引用**:\n     - 使用 `featured_image` 作为文章头图\n     - 在正文中插入 screenshots 和 diagrams\n     - 图片路径使用相对路径: `/images/{task_id}/{slug}/xxx.png`\n     - 为所有图片添加 alt 文本\n   - 保存到 ${HEXO_PATH}/{post_dir}/\n   - 文件名: {actual_date}-{slug}.md\n   - **重要**: 使用模型的实际发布日期作为文章日期，不是采集日期\n   - **⚠️ 强制规则**: categories 字段必须使用小写（如 models, papers, engineering）\n     - 原因：SEO最佳实践，避免大小写URL重复内容惩罚\n     - 错误示例：categories: [\"Models\"] ❌\n     - 正确示例：categories: [\"models\"] ✅\n\n   **Telegram频道** (如果 telegram.enabled: true):\n   - 使用 telegram.format 格式化内容\n   - 发送到 telegram.channel_id\n\n   **Discord频道** (如果 discord.enabled: true):\n   - 使用 discord webhook 发送\n   - 格式化为 Discord embed\n\n4. **验证发布**\n   - 确认所有启用的渠道都发布成功\n   - 记录发布结果到会话日志\n\n### 阶段 4.5: 质量验证（发布后检查）\n\n1. **内容完整性检查**\n   - JSON 文件包含所有必需字段\n   - Markdown 文件格式正确\n   - 图片文件成功下载\n\n2. **质量标准验证**（根据任务类型）\n\n   **Foundation Models**:\n   ```bash\n   # 检查必需字段\n   - metadata.source_url 存在\n   - release_date 不等于 collected_at\n   - technical_report 字段（如果适用）\n   - 内容为中文且 ≥ 1500 字符\n   ```\n\n   **MCP Servers**:\n   ```bash\n   # 检查工具文档\n   - tools_resources.tools 非空数组\n   - 每个 tool 包含 name, description\n   - config_example 存在\n   - 内容为中文且 ≥ 1500 字符\n   ```\n\n   **Prompt Papers**:\n   ```bash\n   # 检查论文链接\n   - arxiv_url 存在且有效\n   - 内容为中文且 ≥ 1000 字符\n   ```\n\n3. **失败处理**\n   - 如果验证失败，删除已发布的 JSON 和 Markdown\n   - 记录失败原因到会话日志\n   - 不计入发布统计\n\n4. **归档不合格内容**\n   - 移动到 config/.archived/quality-failed-{date}/\n   - 保留删除记录用于后续改进\n\n### 阶段 5: 关键词优化\n\n1. **分析新内容**\n   - 从摘要和标题中提取技术术语\n\n2. **关键词发现**\n   - 计算新词的出现频率和置信度\n   - 过滤置信度低于 0.85 的词\n\n3. **更新关键词文件**\n   - 更新 config/keywords/{task_id}.json\n\n## 输出要求\n\n每个任务完成后输出执行摘要:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n📊 任务执行完成: {task_id}\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n数据源: {使用的工具}\n搜索结果: {总数}\n过滤后: {数量}\n去重后: {数量}\n质量检查: {通过/失败}\n发布成功: {数量}\n\n质量统计:\n  • 中文内容: {数量}/{总数} (100% required)\n  • 符合长度要求: {数量}/{总数}\n  • 包含必需字段: {数量}/{总数}\n\n任务特定指标:\n  • [Foundation Models] 有技术报告: {数量}/{总数}\n  • [MCP Servers] 有工具文档: {数量}/{总数}\n  • [Prompt Papers] 有arxiv链接: {数量}/{总数}\n\n发布渠道:\n  • JSON原始数据: {数量}个文件（含 metadata.source_url）\n  • Hexo博客: {已发布/跳过/失败} (如果enabled)\n  • Telegram: {已发送/跳过/失败} (如果enabled)\n  • Discord: {已发送/跳过/失败} (如果enabled)\n\n图片下载:\n  • 封面: {数量}\n  • 截图: {数量}\n  • 架构图: {数量}\n\n新发现关键词: {数量}\n\n质量不合格（已跳过）:\n  • 无工具文档: {数量}\n  • 内容过短: {数量}\n  • 非中文内容: {数量}\n  • 缺少必需字段: {数量}\n\n保存位置:\n  • JSON: posts/{task_id}/\n  • 图片: blog/source/images/{task_id}/\n  • Hexo: {HEXO_PATH}/{post_dir}/ (如果enabled)\n  • 归档: config/.archived/ (如果有不合格内容)\n会话日志: temp/sessions/{task_id}/{session_id}/\n\n💡 二次更新: 所有 JSON 包含 metadata.source_url，可用于未来更新\n⚠️ 质量优先: 不符合标准的内容已被跳过或归档\n```\n\n## 错误处理\n\n如果执行失败:\n- 记录详细错误到 logs/{task_id}_{session_id}.log\n- 保存中间结果到会话目录\n- 返回错误摘要\n\n## 工具和脚本\n\n可用的辅助脚本：\n- `.claude-plugin/scripts/discover/image_utils.sh` - 图片下载和处理\n- `.claude-plugin/scripts/discover/update_index.sh` - 更新任务索引\n- `.claude-plugin/scripts/discover/check.sh` - 前置环境检查\n- `.claude-plugin/scripts/discover/parse_tasks.sh` - 任务解析\n\n## 并行执行支持\n\n此 Skill 支持并行处理多个任务。当同时执行多个任务时：\n- 每个任务创建独立的会话目录\n- 任务之间相互隔离，互不影响\n- 所有任务完成后汇总统计信息\n",
    "github": {
      "stars": 0,
      "forks": 0,
      "owner": "longkeyy",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/1832607?v=4",
      "lastCommit": "2025-10-22T06:27:20Z",
      "createdAt": "2025-10-21T23:39:42Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:32.208Z"
  },
  {
    "id": "waybar",
    "source": {
      "type": "github-search",
      "repo": "tobi/dotnix",
      "path": ".claude/skills/waybar/SKILL.md",
      "url": "https://github.com/tobi/dotnix/blob/main/.claude/skills/waybar/SKILL.md"
    },
    "frontmatter": {
      "name": "Waybar",
      "description": "Always include when making changes to waybar"
    },
    "content": "Waybar is a status bar for Wayland. It's a great way to keep track of your system's status and get quick access to your applications.\n\n@modules/home-manager/apps/waybar.nix has the configuration and additional scripts are in @modules/home-manager/apps/waybar/*\n\nWe configure all the css in the @modules/home-manager/apps/waybar.nix file directly. For documetnation on the css see [./waybar-css.md](./waybar-css.md).",
    "github": {
      "stars": 40,
      "forks": 2,
      "owner": "tobi",
      "ownerAvatar": "https://avatars.githubusercontent.com/u/347?v=4",
      "lastCommit": "2025-12-22T19:34:16Z",
      "createdAt": "2025-06-08T17:21:30Z",
      "topics": []
    },
    "scrapedAt": "2026-01-04T13:03:32.817Z"
  }
]